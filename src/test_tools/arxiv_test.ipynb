{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "819d76c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arxiv\n",
      "  Downloading arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting feedparser~=6.0.10 (from arxiv)\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: requests~=2.32.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from arxiv) (2.32.3)\n",
      "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests~=2.32.0->arxiv) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests~=2.32.0->arxiv) (2025.4.26)\n",
      "Downloading arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (pyproject.toml): started\n",
      "  Building wheel for sgmllib3k (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6105 sha256=8e63b398b23e2164692b1d8ddd60b659f88f61165fadd76070eba9b07308a717\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\3d\\4d\\ef\\37cdccc18d6fd7e0dd7817dcdf9146d4d6789c32a227a28134\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
      "Successfully installed arxiv-2.2.0 feedparser-6.0.11 sgmllib3k-1.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65a4d52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-5.9.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Downloading pypdf-5.9.0-py3-none-any.whl (313 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026105f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lecture notes:\n",
      "Optimization for Machine Learning\n",
      "version 0.57\n",
      "All rights reserved.\n",
      "Elad Hazan1\n",
      "1www.cs.princeton.edu/~ehazan\n",
      "arXiv:1909.03550v1  [cs.LG]  8 Sep 2019iiPreface\n",
      "This text was written to accompany a series of lectures given at the Machine\n",
      "Learning Summer School Buenos Aires, following a lecture series at the\n",
      "Simons Center for Theoretical Computer Science, Berkeley. It was extended\n",
      "for the course COS 598D - Optimization for Machine Learning, Princeton\n",
      "University, Spring 2019.\n",
      "I am grateful to Paula Gradu for proofreading parts of this manuscript.\n",
      "I’m also thankful for the help of the following students and colleagues for\n",
      "corrections and suggestions to this text: Udaya Ghai, John Hallman, No´ e\n",
      "Pion, Xinyi Chen.\n",
      "iiiiv Preface\n",
      "Figure 1: Professor Arkadi Nemirovski, Pioneer of mathematical optimiza-\n",
      "tionContents\n",
      "Preface iii\n",
      "1 Introduction 3\n",
      "1.1 Examples of optimization problems in machine learning . . . 4\n",
      "1.1.1 Empirical Risk Minimization . . . . . . . . . . . . . . 4\n",
      "1.1.2 Matrix completion and recommender systems . . . . . 6\n",
      "1.1.3 Learning in Linear Dynamical Systems . . . . . . . . 7\n",
      "1.2 Why is mathematical programming hard? . . . . . . . . . . . 8\n",
      "1.2.1 The computational model . . . . . . . . . . . . . . . . 8\n",
      "1.2.2 Hardness of constrained mathematical programming . 9\n",
      "2 Basic concepts in optimization and analysis 11\n",
      "2.1 Basic deﬁnitions and the notion of convexity . . . . . . . . . . 11\n",
      "2.1.1 Projections onto convex sets . . . . . . . . . . . . . . . 13\n",
      "2.1.2 Introduction to optimality conditions . . . . . . . . . . 14\n",
      "2.1.3 Solution concepts for non-convex optimization . . . . 15\n",
      "2.2 Potentials for distance to optimality . . . . . . . . . . . . . . 16\n",
      "2.3 Gradient descent and the Polyak stepsize . . . . . . . . . . . 18\n",
      "2.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n",
      "2.5 Bibliographic remarks . . . . . . . . . . . . . . . . . . . . . . 23\n",
      "3 Stochastic Gradient Descent 25\n",
      "3.1 Training feedforward neural networks . . . . . . . . . . . . . 25\n",
      "3.2 Gradient descent for smooth optimization . . . . . . . . . . . 27\n",
      "3.3 Stochastic gradient descent . . . . . . . . . . . . . . . . . . . 29\n",
      "3.4 Bibliographic remarks . . . . . . . . . . . . . . . . . . . . . . 31\n",
      "4 Generalization and Non-Smooth Optimization 33\n",
      "4.1 A note on non-smooth optimization . . . . . . . . . . . . . . 34\n",
      "4.2 Minimizing Regret . . . . . . . . . . . . . . . . . . . . . . . . 35\n",
      "vvi CONTENTS\n",
      "4.3 Regret implies generalization . . . . . . . . . . . . . . . . . . 35\n",
      "4.4 Online gradient descent . . . . . . . . . . . . . . . . . . . . . 36\n",
      "4.5 Lower bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n",
      "4.6 Online gradient descent for strongly convex functions . . . . . 39\n",
      "4.7 Online Gradient Descent implies SGD . . . . . . . . . . . . . 41\n",
      "4.8 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n",
      "4.9 Bibliographic remarks . . . . . . . . . . . . . . . . . . . . . . 45\n",
      "5 Regularization 47\n",
      "5.1 Motivation: prediction from expert advice . . . . . . . . . . . 47\n",
      "5.1.1 The weighted majority algorithm . . . . . . . . . . . . 49\n",
      "5.1.2 Randomized weighted majority . . . . . . . . . . . . . 51\n",
      "5.1.3 Hedge . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\n",
      "5.2 The Regularization framework . . . . . . . . . . . . . . . . . 53\n",
      "5.2.1 The RFTL algorithm . . . . . . . . . . . . . . . . . . 54\n",
      "5.2.2 Mirrored Descent . . . . . . . . . . . . . . . . . . . . . 55\n",
      "5.2.3 Deriving online gradient descent . . . . . . . . . . . . 56\n",
      "5.2.4 Deriving multiplicative updates . . . . . . . . . . . . . 57\n",
      "5.3 Technical background: regularization functions . . . . . . . . 57\n",
      "5.4 Regret bounds for Mirrored Descent . . . . . . . . . . . . . . 59\n",
      "5.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n",
      "5.6 Bibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . 63\n",
      "6 Adaptive Regularization 65\n",
      "6.1 Adaptive Learning Rates: Intuition . . . . . . . . . . . . . . . 65\n",
      "6.2 A Regularization Viewpoint . . . . . . . . . . . . . . . . . . 66\n",
      "6.3 Tools from Matrix Calculus . . . . . . . . . . . . . . . . . . . 66\n",
      "6.4 The AdaGrad Algorithm and Its Analysis . . . . . . . . . . . 67\n",
      "6.5 Diagonal AdaGrad . . . . . . . . . . . . . . . . . . . . . . . . 71\n",
      "6.6 State-of-the-art: from Adam to Shampoo and beyond . . . . 72\n",
      "6.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n",
      "6.8 Bibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . 74\n",
      "7 Variance Reduction 75\n",
      "7.1 Variance reduction: Intuition . . . . . . . . . . . . . . . . . . 75\n",
      "7.2 Setting and deﬁnitions . . . . . . . . . . . . . . . . . . . . . . 76\n",
      "7.3 The variance reduction advantage . . . . . . . . . . . . . . . . 77\n",
      "7.4 A simple variance-reduced algorithm . . . . . . . . . . . . . . 78\n",
      "7.5 Bibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . 80CONTENTS vii\n",
      "8 Nesterov Acceleration 81\n",
      "8.1 Algorithm and implementation . . . . . . . . . . . . . . . . . 81\n",
      "8.2 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n",
      "8.3 Bibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . 84\n",
      "9 The conditional gradient method 85\n",
      "9.1 Review: relevant concepts from linear algebra . . . . . . . . . 85\n",
      "9.2 Motivation: matrix completion and recommendation systems 86\n",
      "9.3 The Frank-Wolfe method . . . . . . . . . . . . . . . . . . . . 88\n",
      "9.4 Projections vs. linear optimization . . . . . . . . . . . . . . . 90\n",
      "9.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93\n",
      "9.6 Bibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . 94\n",
      "10 Second order methods for machine learning 95\n",
      "10.1 Motivating example: linear regression . . . . . . . . . . . . . 95\n",
      "10.2 Self-Concordant Functions . . . . . . . . . . . . . . . . . . . . 96\n",
      "10.3 Newton’s method for self-concordant functions . . . . . . . . 97\n",
      "10.4 Linear-time second-order methods . . . . . . . . . . . . . . . 100\n",
      "10.4.1 Estimators for the Hessian Inverse . . . . . . . . . . . 100\n",
      "10.4.2 Incorporating the estimator . . . . . . . . . . . . . . . 101\n",
      "10.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\n",
      "10.6 Bibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . 104\n",
      "11 Hyperparameter Optimization 105\n",
      "11.1 Formalizing the problem . . . . . . . . . . . . . . . . . . . . . 105\n",
      "11.2 Hyperparameter optimization algorithms . . . . . . . . . . . . 106\n",
      "11.3 A Spectral Method . . . . . . . . . . . . . . . . . . . . . . . . 107\n",
      "11.3.1 Background: Compressed Sensing . . . . . . . . . . . 108\n",
      "11.3.2 The Spectral Algorithm . . . . . . . . . . . . . . . . . 110\n",
      "11.4 Bibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . 111viii CONTENTSNotation\n",
      "We use the following mathematical notation in this writeup:\n",
      "•d-dimensional Euclidean space is denoted Rd.\n",
      "•Vectors are denoted by boldface lower-case letters such asx ∈Rd. Co-\n",
      "ordinates of vectors are denoted by underscore notation xi or regular\n",
      "brackets x(i).\n",
      "•Matrices are denoted by boldface upper-case letters such asX ∈Rm×n.\n",
      "Their coordinates by X(i,j), or Xij.\n",
      "•Functions are denoted by lower case letters f : Rd ↦→R.\n",
      "•The k-th diﬀerential of function f is denoted by ∇kf ∈Rdk\n",
      ". The\n",
      "gradient is denoted without the superscript, as ∇f.\n",
      "•We use the mathcal macro for sets, such as K⊆ Rd.\n",
      "•We denote the gradient at point xt as ∇xt, or simply ∇t.\n",
      "•We denote the global or local optima of functions by x⋆.\n",
      "•We denote distance to optimality for iterative algorithms by ht =\n",
      "f(xt) −f(x⋆).\n",
      "•Euclidean distance to optimality is denoted dt = ∥xt −x⋆∥.\n",
      "12 CONTENTSChapter 1\n",
      "Introduction\n",
      "The topic of this lecture series is the mathematical optimization approach\n",
      "to machine learning.\n",
      "In standard algorithmic theory, the burden of designing an eﬃcient al-\n",
      "gorithm for solving a problem at hand is on the algorithm designer. In the\n",
      "decades since in the introduction of computer science, elegant algorithms\n",
      "have been designed for tasks ranging from ﬁnding the shortest path in a\n",
      "graph, computing the optimal ﬂow in a network, compressing a computer\n",
      "ﬁle containing an image captured by digital camera, and replacing a string\n",
      "in a text document.\n",
      "The design approach, while useful to many tasks, falls short of more\n",
      "complicated problems, such as identifying a particular person in an image\n",
      "in bitmap format, or translating text from English to Hebrew. There may\n",
      "very well be an elegant algorithm for the above tasks, but the algorithmic\n",
      "design scheme does not scale.\n",
      "As Turing promotes in his paper [83], it is potentially easier to teach a\n",
      "computer to learn how to solve a task, rather than teaching it the solution\n",
      "for the particular tasks. In eﬀect, that’s what we do at school, or in this\n",
      "lecture series...\n",
      "The machine learning approach to solving problems is to have an au-\n",
      "tomated mechanism for learning an algorithm. Consider the problem of\n",
      "classifying images into two categories: those containing cars and those con-\n",
      "taining chairs (assuming there are only two types of images in the world).\n",
      "In ML we train (teach) a machine to achieve the desired functionality. The\n",
      "same machine can potentially solve any algorithmic task, and diﬀers from\n",
      "task to task only by a set of parameters that determine the functionality of\n",
      "the machine. This is much like the wires in a computer chip determine its\n",
      "34 CHAPTER 1. INTRODUCTION\n",
      "functionality. Indeed, one of the most popular machines are artiﬁcial neural\n",
      "networks.\n",
      "The mathematical optimization approach to machine learning is to view\n",
      "the process of machine training as an optimization problem. If we letw∈Rd\n",
      "be the parameters of our machine (a.k.a. model), that are constrained to\n",
      "be in some set K⊆ Rd, and f the function measuring success in mapping\n",
      "examples to their correct label, then the problem we are interested in is\n",
      "described by the mathematical optimization problem of\n",
      "min\n",
      "w∈K\n",
      "f(w) (1.1)\n",
      "This is the problem that the lecture series focuses on, with particular em-\n",
      "phasis on functions that arise in machine learning and have special structure\n",
      "that allows for eﬃcient algorithms.\n",
      "1.1 Examples of optimization problems in machine\n",
      "learning\n",
      "1.1.1 Empirical Risk Minimization\n",
      "Machine learning problems exhibit special structure. For example, one of\n",
      "the most basic optimization problems in supervised learning is that of ﬁtting\n",
      "a model to data, or examples, also known as the optimization problem of\n",
      "Empirical Risk Minimization (ERM). The special structure of the problems\n",
      "arising in such formulations is separability across diﬀerent examples into\n",
      "individual losses.\n",
      "An example of such formulation is the supervised learning paradigm of\n",
      "linear classiﬁcation. In this model, the learner is presented with positive and\n",
      "negative examples of a concept. Each example, denoted by ai, is represented\n",
      "in Euclidean space by a d dimensional feature vector. For example, a com-\n",
      "mon representation for emails in the spam-classiﬁcation problem are binary\n",
      "vectors in Euclidean space, where the dimension of the space is the number of\n",
      "words in the language. The i’th email is a vector ai whose entries are given\n",
      "as ones for coordinates corresponding to words that appear in the email,\n",
      "and zero otherwise 1. In addition, each example has a label bi ∈{−1,+1},\n",
      "corresponding to whether the email has been labeled spam/not spam. The\n",
      "1Such a representation may seem na¨ ıve at ﬁrst as it completely ignores the words’ order\n",
      "of appearance and their context. Extensions to capture these features are indeed studied\n",
      "in the Natural Language Processing literature.1.1. EXAMPLES OF OPTIMIZATION PROBLEMS IN MACHINE LEARNING5\n",
      "goal is to ﬁnd a hyperplane separating the two classes of vectors: those with\n",
      "positive labels and those with negative labels. If such a hyperplane, which\n",
      "completely separates the training set according to the labels, does not ex-\n",
      "ist, then the goal is to ﬁnd a hyperplane that achieves a separation of the\n",
      "training set with the smallest number of mistakes.\n",
      "Mathematically speaking, given a set of mexamples to train on, we seek\n",
      "x ∈Rd that minimizes the number of incorrectly classiﬁed examples, i.e.\n",
      "min\n",
      "x∈Rd\n",
      "1\n",
      "m\n",
      "∑\n",
      "i∈[m]\n",
      "δ(sign(x⊤ai) ̸= bi) (1.2)\n",
      "where sign( x) ∈ {−1,+1}is the sign function, and δ(z) ∈ {0,1}is the\n",
      "indicator function that takes the value 1 if the condition z is satisﬁed and\n",
      "zero otherwise.\n",
      "The mathematical formulation of the linear classiﬁcation above is a spe-\n",
      "cial case of mathematical programming (1.1), in which\n",
      "f(x) = 1\n",
      "m\n",
      "∑\n",
      "i∈[m]\n",
      "δ(sign(x⊤ai) ̸= bi) = E\n",
      "i∼[m]\n",
      "[ℓi(x)],\n",
      "where we make use of the expectation operator for simplicity, and denote\n",
      "ℓi(x) = δ(sign(x⊤ai) ̸= bi) for brevity. Since the program above is non-\n",
      "convex and non-smooth, it is common to take a convex relaxation and replace\n",
      "ℓi with convex loss functions. Typical choices include the means square error\n",
      "function and the hinge loss, given by\n",
      "ℓai,bi(x) = max{0,1 −bi ·x⊤ai}.\n",
      "This latter loss function in the context of binary classiﬁcation gives rise\n",
      "to the popular soft-margin SVM problem.\n",
      "Another important optimization problem is that of training a deep neural\n",
      "network for binary classiﬁcation. For example, consider a dataset of images,\n",
      "represented in bitmap format and denoted by {ai ∈Rd|i ∈[m]}, i.e. m\n",
      "images over n pixels. We would like to ﬁnd a mapping from images to the\n",
      "two categories, {bi ∈{0,1}}of cars and chairs. The mapping is given by a\n",
      "set of parameters of a machine class, such as weights in a neural network,\n",
      "or values of a support vector machine. We thus try to ﬁnd the optimal\n",
      "parameters that match ai to b, i..e\n",
      "min\n",
      "w∈Rd\n",
      "f(w) = E\n",
      "ai,bi\n",
      "[ℓ(fw(ai),bi)] .6 CHAPTER 1. INTRODUCTION\n",
      "1.1.2 Matrix completion and recommender systems\n",
      "Media recommendations have changed signiﬁcantly with the advent of the\n",
      "Internet and rise of online media stores. The large amounts of data collected\n",
      "allow for eﬃcient clustering and accurate prediction of users’ preferences\n",
      "for a variety of media. A well-known example is the so called “Netﬂix\n",
      "challenge”—a competition of automated tools for recommendation from a\n",
      "large dataset of users’ motion picture preferences.\n",
      "One of the most successful approaches for automated recommendation\n",
      "systems, as proven in the Netﬂix competition, is matrix completion. Perhaps\n",
      "the simplest version of the problem can be described as follows.\n",
      "The entire dataset of user-media preference pairs is thought of as a\n",
      "partially-observed matrix. Thus, every person is represented by a row in\n",
      "the matrix, and every column represents a media item (movie). For sim-\n",
      "plicity, let us think of the observations as binary—a person either likes or\n",
      "dislikes a particular movie. Thus, we have a matrix M ∈{0,1,∗}n×m where\n",
      "n is the number of persons considered, m is the number of movies at our\n",
      "library, and 0/1 and ∗signify “dislike”, “like” and “unknown” respectively:\n",
      "Mij =\n",
      "\n",
      "\n",
      "\n",
      "0, person i dislikes movie j\n",
      "1, person i likes movie j\n",
      "∗, preference unknown\n",
      ".\n",
      "The natural goal is to complete the matrix, i.e. correctly assign 0 or 1 to\n",
      "the unknown entries. As deﬁned so far, the problem is ill-posed, since any\n",
      "completion would be equally good (or bad), and no restrictions have been\n",
      "placed on the completions.\n",
      "The common restriction on completions is that the “true” matrix has\n",
      "low rank. Recall that if a matrix X ∈Rn×m has rank k ≤ρ = min{n,m}\n",
      "then it can be written as\n",
      "X = UV , U∈Rn×k,V ∈Rk×m.\n",
      "The intuitive interpretation of this property is that each entry in M\n",
      "can be explained by only k numbers. In matrix completion this means,\n",
      "intuitively, that there are only kfactors that determine a persons preference\n",
      "over movies, such as genre, director, actors and so on.\n",
      "Now the simplistic matrix completion problem can be well-formulated\n",
      "as in the following mathematical program. Denote by ∥·∥OB the Euclidean1.1. EXAMPLES OF OPTIMIZATION PROBLEMS IN MACHINE LEARNING7\n",
      "norm only on the observed (non starred) entries of M, i.e.,\n",
      "∥X∥2\n",
      "OB =\n",
      "∑\n",
      "Mij̸=∗\n",
      "X2\n",
      "ij.\n",
      "The mathematical program for matrix completion is given by\n",
      "min\n",
      "X∈Rn×m\n",
      "1\n",
      "2∥X−M∥2\n",
      "OB\n",
      "s.t. rank( X) ≤k.\n",
      "1.1.3 Learning in Linear Dynamical Systems\n",
      "Many learning problems require memory, or the notion of state. This is\n",
      "captured by the paradigm of reinforcement learning, as well of the special\n",
      "case of control in Linear Dynamical Systems (LDS).\n",
      "LDS model a variety of control and robotics problems in continuous\n",
      "variables. The setting is that of a time series, with following parameters:\n",
      "1. Inputs to the system, also called controls, denoted by u1,..., uT ∈Rn.\n",
      "2. Outputs from the system, also called observations, denotedy1,..., yT ∈\n",
      "Rm.\n",
      "3. The state of the system, which may either be observed or hidden,\n",
      "denoted xt,..., xT ∈Rd.\n",
      "4. The system parameters, which are transformations matricesA,B,C,D\n",
      "in appropriate dimensions.\n",
      "In the online learning problem of LDS, the learner iteratively observes\n",
      "ut,yt, and has to predict ˆyt+1. The actual yt is generated according to the\n",
      "following dynamical equations:\n",
      "xt+1 = Axt + But + εt\n",
      "yt+1 = Cxt+1 + Dut + ζt,\n",
      "where εt,ζt are noise which is distributed as a Normal random variable.\n",
      "Consider an online sequence in which the states are visible. At time t,\n",
      "all system states, inputs and outputs are visible up to this time step. The\n",
      "learner has to predict yt+1, and only afterwards observes ut+1.xt+1,yt+1.8 CHAPTER 1. INTRODUCTION\n",
      "One reasonable way to predict yt+1 based upon past observations is to\n",
      "compute the system, and use the computed transformations to predict. This\n",
      "amounts to solving the following mathematical program:\n",
      "min\n",
      "A,B,ˆC,ˆD\n",
      "{∑\n",
      "τ<t\n",
      "(xτ+1 −Axτ + Buτ)2 + (yτ+1 −ˆCxτ + ˆDuτ)2\n",
      "}\n",
      ",\n",
      "and then predicting ˆyt+1 = ˆC ˆA(xt + But) + ˆDut.\n",
      "1.2 Why is mathematical programming hard?\n",
      "The general formulation (1.1) is NP hard. To be more precise, we have to\n",
      "deﬁne the computational model we are working in as well as and the access\n",
      "model to the function.\n",
      "Before we give a formal proof, the intuition to what makes mathematical\n",
      "optimization hard is simple to state. In one line: it is the fact that global\n",
      "optimality cannot be veriﬁed on the basis of local properties.\n",
      "Most, if not all, eﬃcient optimization algorithms are iterative and based\n",
      "on a local improvement step. By this nature, any optimization algorithm\n",
      "will terminate when the local improvement is no longer possible, giving rise\n",
      "to a proposed solution. However, the quality of this proposed solution may\n",
      "diﬀer signiﬁcantly, in general, from that of the global optimum.\n",
      "This intuition explains the need for a property of objectives for which\n",
      "global optimality is locally veriﬁable. Indeed, this is exactly the notion\n",
      "of convexity, and the reasoning above explains its utmost importance in\n",
      "mathematical optimization.\n",
      "We now to prove that mathematical programming is NP-hard. This\n",
      "requires discussion of the computational model as well as access model to\n",
      "the input.\n",
      "1.2.1 The computational model\n",
      "The computational model we shall adopt throughout this manuscript is that\n",
      "of a RAM machine equipped with oracle access to the objective function\n",
      "f : Rd ↦→R and constraints set K⊆ Rd. The oracle model for the objective\n",
      "function can be one of the following, depending on the speciﬁc scenario:\n",
      "1. Value oracle: given a point x∈Rd, oracle returns f(x) ∈R.\n",
      "2. Gradient (ﬁrst-order) oracle: given a point x∈Rd, oracle returns\n",
      "the gradient ∇f(x) ∈Rd.1.2. WHY IS MATHEMATICAL PROGRAMMING HARD? 9\n",
      "3. k-th order diﬀerential oracle: given a point x∈Rd, oracle returns\n",
      "the tensor ∇kf(x) ∈Rdk\n",
      ".\n",
      "The oracle model for the constraints set is a bit more subtle. We distin-\n",
      "guish between the following oracles:\n",
      "1. Membership oracle: given a point x ∈Rd, oracle returns one if\n",
      "x∈K and zero otherwise.\n",
      "2. Separating hyperplane oracle: given a point x∈Rd, oracle either\n",
      "returns ”Yes” if x∈K, or otherwise returns a hyperplane h∈Rd such\n",
      "that h⊤x> 0 and ∀y∈K , h⊤y≤0.\n",
      "3. Explicit sets: the most common scenario in machine learning is one\n",
      "in which Kis “natural”, such as the Euclidean ball or hypercube, or\n",
      "the entire Euclidean space.\n",
      "1.2.2 Hardness of constrained mathematical programming\n",
      "Under this computational model, we can show:\n",
      "Lemma 1.1. Mathematical programming is NP-hard, even for a convex\n",
      "continuous constraint set Kand quadratic objective functions.\n",
      "Informal sketch. Consider the MAX-CUT problem: given a graph G =\n",
      "(V,E), ﬁnd a subset of the vertices that maximizes the number of edges\n",
      "cut. Let A be the negative adjacency matrix of the graph, i.e.\n",
      "Aij =\n",
      "\n",
      "\n",
      "\n",
      "−1, (i,j) ∈E\n",
      "0, o/w\n",
      "Also suppose that Aii = 0.\n",
      "Next, consider the mathematical program:\n",
      "min\n",
      "{\n",
      "fA(x) = 1\n",
      "4(x⊤Ax −2|E|)\n",
      "}\n",
      "(1.3)\n",
      "∥x∥∞= 1 .\n",
      "Consider the cut deﬁned by the solution of this program, namely\n",
      "Sx = {i∈V|xi = 1},\n",
      "for x = x⋆. Let C(S) denote the size of the cut speciﬁed by the subset of\n",
      "edges S ⊆E. Observe that the expression 1\n",
      "2 x⊤Ax, is exactly equal to the10 CHAPTER 1. INTRODUCTION\n",
      "number of edges that are cut by Sx minus the number of edges that are\n",
      "uncut. Thus, we have\n",
      "1\n",
      "2xAx = C(Sx) −(E−C(Sx)) = 2C(Sx) −E,\n",
      "and hence f(x) = C(Sx). Therefore, maximizing f(x) is equivalent to the\n",
      "MAX-CUT problem, and is thus NP-hard. We proceed to make the con-\n",
      "straint set convex and continuous. Consider the mathematical program\n",
      "min {fA(x)} (1.4)\n",
      "∥x∥∞≤1 .\n",
      "This is very similar to the previous program, but we relaxed the equality\n",
      "to be an inequality, consequently the constraint set is now the hypercube.\n",
      "We now claim that the solution is w.l.o.g. a vertex. To see that, consider\n",
      "y(x) ∈{±1}d a rounding of x to the corners deﬁned by:\n",
      "yi = y(x)i =\n",
      "\n",
      "\n",
      "\n",
      "1, w.p. 1+xi\n",
      "2\n",
      "−1, w.p. 1−xi\n",
      "2\n",
      "Notice that\n",
      "E[y] = x , ∀i̸= j .E[yiyj] = xixj,\n",
      "and therefore E[y(x)⊤Ay(x)] = x⊤Ax. We conclude that the optimum of\n",
      "mathematical program 1.4 is the same as that for 1.3, and both are NP-\n",
      "hard.Chapter 2\n",
      "Basic concepts in\n",
      "optimization and analysis\n",
      "2.1 Basic deﬁnitions and the notion of convexity\n",
      "We consider minimization of a continuous function over a convex subset of\n",
      "Euclidean space. We mostly consider objective functions that are convex. In\n",
      "later chapters we relax this requirement and consider non-convex functions\n",
      "as well.\n",
      "Henceforth, let K⊆ Rd be a bounded convex and compact set in Eu-\n",
      "clidean space. We denote by D an upper bound on the diameter of K:\n",
      "∀x,y ∈K, ∥x −y∥≤ D.\n",
      "A set Kis convex if for any x,y ∈K, all the points on the line segment\n",
      "connecting x and y also belong to K, i.e.,\n",
      "∀α∈[0,1], αx + (1 −α)y ∈K.\n",
      "A function f : K↦→ R is convex if for any x,y ∈K\n",
      "∀α∈[0,1], f(αx + (1 −α)y) ≤αf(x) + (1−α)f(y).\n",
      "Gradients and subgradients. The set of all subgradients of a function\n",
      "f at x, denoted ∂f(x), is the set of all vectors u such that\n",
      "f(y) ≥f(x) + u⊤(y −x).\n",
      "It can be shown that the set of subgradients of a convex function is always\n",
      "non-empty.\n",
      "1112 CHAPTER 2. BASIC CONCEPTS\n",
      "Suppose f is diﬀerentiable, let ∇f(x)[i] = ∂\n",
      "∂xi\n",
      "f(x) be the vector of\n",
      "partial derivatives according to the variables, called the gradient. If the\n",
      "gradient ∇f(x) exists, then ∇f(x) ∈∂f(x) and ∀y ∈K\n",
      "f(y) ≥f(x) + ∇f(x)⊤(y −x).\n",
      "Henceforth we shall denote by∇f(x) the gradient, if it exists, or any member\n",
      "of ∂f(x) otherwise.\n",
      "We denote by G> 0 an upper bound on the norm of the subgradients of\n",
      "f over K, i.e., ∥∇f(x)∥≤ G for all x ∈K. The existence of Such an upper\n",
      "bound implies that the function f is Lipschitz continuous with parameter\n",
      "G, that is, for all x,y ∈K\n",
      "|f(x) −f(y)|≤ G∥x −y∥.\n",
      "Smoothness and strong convexity. The optimization and machine learn-\n",
      "ing literature studies special types of convex functions that admit useful\n",
      "properties, which in turn allow for more eﬃcient optimization. Notably, we\n",
      "say that a function is α-strongly convex if\n",
      "f(y) ≥f(x) + ∇f(x)⊤(y −x) + α\n",
      "2 ∥y −x∥2.\n",
      "A function is β-smooth if\n",
      "f(y) ≤f(x) + ∇f(x)⊤(y −x) + β\n",
      "2 ∥y −x∥2.\n",
      "The latter condition is implied by a slightly stronger Lipschitz condition\n",
      "over the gradients, which is sometimes used to deﬁned smoothness, i.e.,\n",
      "∥∇f(x) −∇f(y)∥≤ β∥x −y∥.\n",
      "If the function is twice diﬀerentiable and admits a second derivative,\n",
      "known as a Hessian for a function of several variables, the above conditions\n",
      "are equivalent to the following condition on the Hessian, denoted ∇2f(x):\n",
      "Smoothness: −βI ≼∇2f(x) ≼ βI\n",
      "Strong-convexity: αI ≼∇2f(x),\n",
      "where A≼ B if the matrix B−A is positive semideﬁnite.\n",
      "When the function f is both α-strongly convex and β-smooth, we say\n",
      "that it is γ-well-conditioned where γ is the ratio between strong convexity\n",
      "and smoothness, also called the condition number of f\n",
      "γ = α\n",
      "β ≤12.1. BASICS 13\n",
      "2.1.1 Projections onto convex sets\n",
      "In the following algorithms we shall make use of a projection operation onto\n",
      "a convex set, which is deﬁned as the closest point inside the convex set to a\n",
      "given point. Formally,\n",
      "Π\n",
      "K\n",
      "(y) ≜ arg min\n",
      "x∈K\n",
      "∥x −y∥.\n",
      "When clear from the context, we shall remove the Ksubscript. It is left as\n",
      "an exercise to the reader to prove that the projection of a given point over\n",
      "a closed non-empty convex set exists and is unique.\n",
      "The computational complexity of projections is a subtle issue that de-\n",
      "pends much on the characterization of Kitself. Most generally, Kcan be\n",
      "represented by a membership oracle—an eﬃcient procedure that is capable\n",
      "of deciding whether a given x belongs to Kor not. In this case, projections\n",
      "can be computed in polynomial time. In certain special cases, projections\n",
      "can be computed very eﬃciently in near-linear time.\n",
      "A crucial property of projections that we shall make extensive use of is\n",
      "the Pythagorean theorem, which we state here for completeness:\n",
      "Figure 2.1: Pythagorean theorem.\n",
      "Theorem 2.1 (Pythagoras, circa 500 BC) . Let K⊆ Rd be a convex set,\n",
      "y ∈Rd and x = ΠK(y). Then for any z ∈K we have\n",
      "∥y −z∥≥∥ x −z∥.14 CHAPTER 2. BASIC CONCEPTS\n",
      "We note that there exists a more general version of the Pythagorean\n",
      "theorem. The above theorem and the deﬁnition of projections are true and\n",
      "valid not only for Euclidean norms, but for projections according to other\n",
      "distances that are not norms. In particular, an analogue of the Pythagorean\n",
      "theorem remains valid with respect to Bregman divergences.\n",
      "2.1.2 Introduction to optimality conditions\n",
      "The standard curriculum of high school mathematics contains the basic facts\n",
      "concerning when a function (usually in one dimension) attains a local opti-\n",
      "mum or saddle point. The KKT (Karush-Kuhn-Tucker) conditions general-\n",
      "ize these facts to more than one dimension, and the reader is referred to the\n",
      "bibliographic material at the end of this chapter for an in-depth rigorous\n",
      "discussion of optimality conditions in general mathematical programming.\n",
      "For our purposes, we describe only brieﬂy and intuitively the main facts\n",
      "that we will require henceforth. We separate the discussion into convex and\n",
      "non-convex programming.\n",
      "Optimality for convex optimization\n",
      "A local minimum of a convex function is also a global minimum (see exercises\n",
      "at the end of this chapter). We say that x⋆ is an ε-approximate optimum if\n",
      "the following holds:\n",
      "∀x ∈K . f(x⋆) ≤f(x) + ε.\n",
      "The generalization of the fact that a minimum of a convex diﬀerentiable\n",
      "function on R is a point in which its derivative is equal to zero, is given by\n",
      "the multi-dimensional analogue that its gradient is zero:\n",
      "∇f(x) = 0 ⇐⇒ x ∈arg min\n",
      "x∈Rn\n",
      "f(x).\n",
      "We will require a slightly more general, but equally intuitive, fact for con-\n",
      "strained optimization: at a minimum point of a constrained convex function,\n",
      "the inner product between the negative gradient and direction towards the\n",
      "interior of Kis non-positive. This is depicted in Figure 2.2, which shows that\n",
      "−∇f(x⋆) deﬁnes a supporting hyperplane to K. The intuition is that if the\n",
      "inner product were positive, one could improve the objective by moving in\n",
      "the direction of the projected negative gradient. This fact is stated formally\n",
      "in the following theorem.2.1. BASICS 15\n",
      "Theorem 2.2 (Karush-Kuhn-Tucker). Let K⊆ Rd be a convex set, x⋆ ∈\n",
      "arg minx∈Kf(x). Then for any y ∈K we have\n",
      "∇f(x⋆)⊤(y −x⋆) ≥0.\n",
      "Figure 2.2: Optimality conditions: negative (sub)gradient pointing out-\n",
      "wards.\n",
      "2.1.3 Solution concepts for non-convex optimization\n",
      "We have seen in the previous chapter that mathematical optimization is NP-\n",
      "hard. This implies that ﬁnding global solutions for non-convex optimization\n",
      "is NP-hard, even for smooth functions over very simple convex domains. We\n",
      "thus consider other trackable concepts of solutions.\n",
      "The most common solution concept is that of ﬁrst-order optimality, a.k.a.\n",
      "saddle-points or stationary points. These are points that satisfy\n",
      "∥∇f(x⋆)∥= 0.\n",
      "Unfortunately, even ﬁnding such stationary points is NP-hard. We thus\n",
      "settle for approximate stationary points, which satisify\n",
      "∥∇f(x⋆)∥≤ ε.16 CHAPTER 2. BASIC CONCEPTS\n",
      "Figure 2.3: First and second-order local optima.\n",
      "A more stringent notion of optimality we may consider is obtained by\n",
      "looking at the second derivatives. We can require they behave as for global\n",
      "minimum, see ﬁgure 2.3. Formally, we say that a point x⋆ is a second-order\n",
      "local minimum if it satisﬁes the two conditions:\n",
      "∥∇f(x⋆)∥≤ ε ,∇2f(x⋆) ⪰−√εI.\n",
      "The diﬀerences in approximation criteria for ﬁrst and second derivatives is\n",
      "natural, as we shall explore in non-convex approximation algorithms hence-\n",
      "forth.\n",
      "We note that it is possible to further deﬁne optimality conditions for\n",
      "higher order derivatives, although this is less useful in the context of machine\n",
      "learning.\n",
      "2.2 Potentials for distance to optimality\n",
      "When analyzing convergence of gradient methods, it is useful to use potential\n",
      "functions in lieu of function distance to optimality, such as gradient norm\n",
      "and/or Euclidean distance to optimality. The following relationships hold\n",
      "between these quantities.\n",
      "Lemma 2.3. The following properties hold for α-strongly-convex functions\n",
      "and/or β-smooth functions over Euclidean space Rd.\n",
      "1. α\n",
      "2 d2\n",
      "t ≤ht\n",
      "2. ht ≤β\n",
      "2 d2\n",
      "t2.2. POTENTIALS FOR DISTANCE TO OPTIMALITY 17\n",
      "3. 1\n",
      "2β∥∇t∥2 ≤ht\n",
      "4. ht ≤ 1\n",
      "2α∥∇t∥2\n",
      "Proof. 1. ht ≥α\n",
      "2 d2\n",
      "t:\n",
      "By strong convexity, we have\n",
      "ht = f(xt) −f(x⋆)\n",
      "≥∇f(x⋆)⊤(xt −x⋆) + α\n",
      "2 ∥xt −x⋆∥2\n",
      "= α\n",
      "2 ∥xt −x⋆∥2\n",
      "where the last inequality follows since the gradient at the global opti-\n",
      "mum is zero.\n",
      "2. ht ≤β\n",
      "2 d2\n",
      "t:\n",
      "By smoothness,\n",
      "ht = f(xt) −f(x⋆)\n",
      "≤∇f(x⋆)⊤(xt −x⋆) + β\n",
      "2 ∥xt −x⋆∥2\n",
      "= β\n",
      "2 ∥xt −x⋆∥2\n",
      "where the last inequality follows since the gradient at the global opti-\n",
      "mum is zero.\n",
      "3. ht ≥ 1\n",
      "2β∥∇t∥2: Using smoothness, and let xt+1 = xt −η∇t for η= 1\n",
      "β,\n",
      "ht = f(xt) −f(x⋆)\n",
      "≥f(xt) −f(xt+1)\n",
      "≥∇f(xt)⊤(xt −xt+1) −β\n",
      "2 ∥xt −xt+1∥2\n",
      "= η∥∇t∥2 −β\n",
      "2 η2∥∇t∥2\n",
      "= 1\n",
      "2β∥∇t∥2.\n",
      "4. ht ≤ 1\n",
      "2α∥∇t∥2:\n",
      "We have for any pair x,y ∈Rd:\n",
      "f(y) ≥f(x) + ∇f(x)⊤(y −x) + α\n",
      "2 ∥x −y∥2\n",
      "≥min\n",
      "z∈Rd\n",
      "{\n",
      "f(x) + ∇f(x)⊤(z −x) + α\n",
      "2 ∥x −z∥2\n",
      "}\n",
      "= f(x) − 1\n",
      "2α∥∇f(x)∥2.\n",
      "by taking z = x −1\n",
      "α∇f(x)18 CHAPTER 2. BASIC CONCEPTS\n",
      "In particular, taking x = xt , y = x⋆, we get\n",
      "ht = f(xt) −f(x⋆) ≤ 1\n",
      "2α∥∇t∥2. (2.1)\n",
      "2.3 Gradient descent and the Polyak stepsize\n",
      "The simplest iterative optimization algorithm is gradient descent, as given\n",
      "in Algorithm 1. We analyze GD with the Polyak stepsize, which has the\n",
      "advantage of not depending on the strong convexity and/or smoothness\n",
      "parameters of the objective function.\n",
      "Algorithm 1 GD with the Polyak stepsize\n",
      "1: Input: time horizon T, x0\n",
      "2: for t= 0,...,T −1 do\n",
      "3: Set ηt = ht\n",
      "∥∇t∥2\n",
      "4: xt+1 = xt −ηt∇t\n",
      "5: end for\n",
      "6: Return ¯x = arg minxt{f(xt)}\n",
      "To prove convergence bounds, assume ∥∇t∥≤ G, and deﬁne:\n",
      "BT = min\n",
      "{\n",
      "Gd0√\n",
      "T\n",
      ",2βd2\n",
      "0\n",
      "T ,3G2\n",
      "αT ,βd2\n",
      "0\n",
      "(\n",
      "1 − α\n",
      "4β\n",
      ")T}\n",
      "Theorem 2.4. (GD with the Polyak Step Size) Algorithm 1 attains the\n",
      "following regret bound after T steps:\n",
      "h(¯x) = min\n",
      "0≤t≤T\n",
      "{ht}≤ BT\n",
      "Theorem 2.4 directly follows from the following lemma. Let 0 ≤γ ≤1,\n",
      "deﬁne RT,γ as follows:\n",
      "RT,γ = min\n",
      "{\n",
      "Gd0√γT,2βd2\n",
      "0\n",
      "γT , 3G2\n",
      "γαT,βd2\n",
      "0\n",
      "(\n",
      "1 −γ α\n",
      "4β\n",
      ")T}\n",
      ".\n",
      "Lemma 2.5. For 0 ≤γ ≤1, suppose that a sequence x0,... xt satisﬁes:\n",
      "d2\n",
      "t+1 ≤d2\n",
      "t −γ h2\n",
      "t\n",
      "∥∇t∥2 (2.2)2.3. GRADIENT DESCENT AND THE POLYAK STEPSIZE 19\n",
      "then for ¯x as deﬁned in the algorithm, we have:\n",
      "h(¯x) ≤RT,γ .\n",
      "Proof. The proof analyzes diﬀerent cases:\n",
      "1. For convex functions with gradient bounded by G,\n",
      "d2\n",
      "t+1 −d2\n",
      "t ≤− γh2\n",
      "t\n",
      "∥∇t∥2 ≤−γh2\n",
      "t\n",
      "G2\n",
      "Summing up over T iterations, and using Cauchy-Schwartz, we have\n",
      "1\n",
      "T\n",
      "∑\n",
      "t\n",
      "ht ≤ 1√\n",
      "T\n",
      "√∑\n",
      "t\n",
      "h2\n",
      "t\n",
      "≤ G√γT\n",
      "√∑\n",
      "t\n",
      "(d2\n",
      "t −d2\n",
      "t+1) ≤ Gd0√γT .\n",
      "2. For smooth functions whose gradient is bounded by G, Lemma 2.3\n",
      "implies:\n",
      "d2\n",
      "t+1 −d2\n",
      "t ≤− γh2\n",
      "t\n",
      "∥∇t∥2 ≤−γht\n",
      "2β .\n",
      "This implies\n",
      "1\n",
      "T\n",
      "∑\n",
      "t\n",
      "ht ≤2βd2\n",
      "0\n",
      "γT .\n",
      "3. For strongly convex functions, Lemma 2.3 implies:\n",
      "d2\n",
      "t+1 −d2\n",
      "t ≤−γ h2\n",
      "t\n",
      "∥∇t∥2 ≤−γh2\n",
      "t\n",
      "G2 ≤−γα2d4\n",
      "t\n",
      "4G2 .\n",
      "In other words, d2\n",
      "t+1 ≤d2\n",
      "t(1 −γα2d2\n",
      "t\n",
      "4G2 ) . Deﬁning at := γα2d2\n",
      "t\n",
      "4G2 , we have:\n",
      "at+1 ≤at(1 −at) .20 CHAPTER 2. BASIC CONCEPTS\n",
      "This implies that at ≤ 1\n",
      "t+1 , which can be seen by induction 1. The\n",
      "proof is completed as follows 2 :\n",
      "1\n",
      "T/2\n",
      "T∑\n",
      "t=T/2\n",
      "h2\n",
      "t ≤ 2G2\n",
      "γT\n",
      "T∑\n",
      "t=T/2\n",
      "(d2\n",
      "t −d2\n",
      "t+1)\n",
      "= 2G2\n",
      "γT (d2\n",
      "T/2 −d2\n",
      "T)\n",
      "= 8G4\n",
      "γ2α2T(aT/2 −aT)\n",
      "≤ 9G4\n",
      "γ2α2T2 .\n",
      "Thus, there exists a t for which h2\n",
      "t ≤ 9G4\n",
      "γ2α2T2 . Taking the square root\n",
      "completes the claim.\n",
      "4. For both strongly convex and smooth functions:\n",
      "d2\n",
      "t+1 −d2\n",
      "t ≤−γ h2\n",
      "t\n",
      "∥∇t∥2 ≤−γht\n",
      "2β ≤−γ α\n",
      "4βd2\n",
      "t\n",
      "Thus,\n",
      "hT ≤βd2\n",
      "T ≤βd2\n",
      "0\n",
      "(\n",
      "1 −γ α\n",
      "4β\n",
      ")T\n",
      ".\n",
      "This completes the proof of all cases.\n",
      "1That a0 ≤1 follows from Lemma 2.3. For t = 1, a1 ≤1\n",
      "2 since a1 ≤a0(1 −a0) and\n",
      "0 ≤a0 ≤1. For the induction step, at ≤at−1(1 −at−1) ≤1\n",
      "t(1 −1\n",
      "t) = t−1\n",
      "t2 = 1\n",
      "t+1 (t2−1\n",
      "t2 ) ≤\n",
      "1\n",
      "t+1 .\n",
      "2This assumes T is even. T odd leads to the same constants.2.4. EXERCISES 21\n",
      "2.4 Exercises\n",
      "1. Write an explicit expression for the gradient and projection operation\n",
      "(if needed) for each of the example optimization problems in the ﬁrst\n",
      "chapter.\n",
      "2. Prove that a diﬀerentiable function f(x) : R →R is convex if and only\n",
      "if for any x,y ∈R it holds that f(x) −f(y) ≤(x−y)f′(x).\n",
      "3. Recall that we say that a function f : Rn →R has a condition number\n",
      "γ = α/β over K ⊆Rd if the following two inequalities hold for all\n",
      "x,y ∈K:\n",
      "(a) f(y) ≥f(x) + (y −x)⊤∇f(x) + α\n",
      "2 ∥x −y∥2\n",
      "(b) f(y) ≤f(x) + (y −x)⊤∇f(x) + β\n",
      "2 ∥x −y∥2\n",
      "For matrices A,B ∈ Rn×n we denote A ≽ B if A−B is positive\n",
      "semideﬁnite. Prove that if f is twice diﬀerentiable and it holds that\n",
      "βI ≽ ∇2f(x) ≽ αI for any x ∈K, then the condition number of f\n",
      "over Kis α/β.\n",
      "4. Prove:\n",
      "(a) The sum of convex functions is convex.\n",
      "(b) Let f be α1-strongly convex and g be α2-strongly convex. Then\n",
      "f + g is (α1 + α2)-strongly convex.\n",
      "(c) Let f be β1-smooth and g be β2-smooth. Then f+g is (β1 +β2)-\n",
      "smooth.\n",
      "5. Let K⊆ Rd be closed, compact, non-empty and bounded. Prove that\n",
      "a necessary and suﬃcient condition for ΠK(x) to be a singleton, that\n",
      "is for |ΠK(x)|= 1, is for K to be convex.\n",
      "6. Prove that for convex functions, ∇f(x) ∈∂f(x), that is, the gradient\n",
      "belongs to the subgradient set.\n",
      "7. Let f(x) : Rn →R be a convex diﬀerentiable function and K⊆ Rn be\n",
      "a convex set. Prove that x⋆ ∈K is a minimizer of f over Kif and only\n",
      "if for any y ∈K it holds that ( y −x⋆)⊤∇f(x⋆) ≥0.\n",
      "8. Consider the n-dimensional simplex\n",
      "∆n = {x ∈Rn|\n",
      "n∑\n",
      "i=1\n",
      "xi = 1, xi ≥0 , ∀i∈[n]}.22 CHAPTER 2. BASIC CONCEPTS\n",
      "Give an algorithm for computing the projection of a point x ∈Rn onto\n",
      "the set ∆n (a near-linear time algorithm exists).2.5. BIBLIOGRAPHIC REMARKS 23\n",
      "2.5 Bibliographic remarks\n",
      "The reader is referred to dedicated books on convex optimization for much\n",
      "more in-depth treatment of the topics surveyed in this background chapter.\n",
      "For background in convex analysis see the texts [11, 68]. The classic text-\n",
      "book [12] gives a broad introduction to convex optimization with numerous\n",
      "applications. For an adaptive analysis of gradient descent with the Polyak\n",
      "stepsize see [33].24 CHAPTER 2. BASIC CONCEPTSChapter 3\n",
      "Stochastic Gradient Descent\n",
      "The most important optimization algorithm in the context of machine learn-\n",
      "ing is stochastic gradient descent (SGD), especially for non-convex optimiza-\n",
      "tion and in the context of deep neural networks. In this chapter we spell\n",
      "out the algorithm and analyze it up to tight ﬁnite-time convergence rates.\n",
      "3.1 Training feedforward neural networks\n",
      "Perhaps the most common optimization problem in machine learning is that\n",
      "of training feedforward neural networks. In this problem, we are given a set\n",
      "of labelled data points, such as labelled images or text. Let {xi,yi}be the\n",
      "set of labelled data points, also called the training data.\n",
      "The goal is to ﬁt the weights of an artiﬁcial neural network in order to\n",
      "minimize the loss over the data. Mathematically, the feedforward network\n",
      "is a given weighted a-cyclic graph G= (V,E,W ). Each node v is assigned\n",
      "an activation function, which we assume is the same function for all nodes,\n",
      "denoted σ : Rd ↦→R. Using a biological analogy, an activation function σ\n",
      "is a function that determines how strongly a neuron (i.e. a node) ‘ﬁres’ for\n",
      "a given input by mapping the result into the desired range, usually [0 ,1] or\n",
      "[−1,1] . Some popular examples include:\n",
      "•Sigmoid: σ(x) = 1\n",
      "1+e−x\n",
      "•Hyperbolic tangent: tanh( x) = ex−e−x\n",
      "ex+e−x\n",
      "•Rectiﬁed linear unit: ReLU(x) = max{0,x}(currently the most widely\n",
      "used of the three)\n",
      "2526 CHAPTER 3. STOCHASTIC GRADIENT DESCENT\n",
      "The inputs to the input layer nodes is a given data point, while the\n",
      "inputs to to all other nodes are the output of the nodes connected to it. We\n",
      "denote by ρ(v) the set of input neighbors to node v. The top node output\n",
      "is the input to the loss function, which takes its “prediction” and the true\n",
      "label to form a loss.\n",
      "For an input node v, its output as a function of the graph weights and\n",
      "input example x (of dimension d), which we denote as\n",
      "v(W,x) = σ\n",
      "(∑\n",
      "i∈d\n",
      "Wv,ixi\n",
      ")\n",
      "The output of an internal node v is a function of its inputs u∈ρ(v) and a\n",
      "given example x, which we denote as\n",
      "v(W,x) = σ\n",
      "\n",
      " ∑\n",
      "u∈ρ(v)\n",
      "Wuvu(W,x)\n",
      "\n",
      "\n",
      "If we denote the top node as v1, then the loss of the network over data point\n",
      "(xi,yi) is given by\n",
      "ℓ(v1(W,xi),yi).\n",
      "The objective function becomes\n",
      "f(W) = E\n",
      "xi,yi\n",
      "[\n",
      "ℓ(v1(W,xi),yi)\n",
      "]\n",
      "For most commonly-used activation and loss functions, the above func-\n",
      "tion is non-convex. However, it admits important computational properties.\n",
      "The most signiﬁcant property is given in the following lemma.\n",
      "Lemma 3.1 (Backpropagation lemma). The gradient of f can be computed\n",
      "in time O(|E|).\n",
      "The proof of this lemma is left as an exercise, but we sketch the main\n",
      "ideas. For every variable Wuv, we have by linearity of expectation that\n",
      "∂\n",
      "∂Wuv\n",
      "f(W) = E\n",
      "xi,yi\n",
      "[ ∂\n",
      "∂Wuv\n",
      "ℓ(v1(W,xi),yi)\n",
      "]\n",
      ".\n",
      "Next, using the chain rule, we claim that it suﬃces to know the partial\n",
      "derivatives of each node w.r.t. its immediate daughters. To see this, let us3.2. GRADIENT DESCENT FOR SMOOTH OPTIMIZATION 27\n",
      "write the derivative w.r.t. Wuv using the chain rule:\n",
      "∂\n",
      "∂Wuv\n",
      "ℓ(v1(W,xi),yi) = ∂ℓ\n",
      "∂v1 · ∂v1\n",
      "∂Wuv\n",
      "= ∂ℓ\n",
      "∂v1 ·\n",
      "∑\n",
      "v2∈ρ(v1)\n",
      "∂v1\n",
      "∂v2 · ∂vj\n",
      "∂Wuv\n",
      "= ...\n",
      "= ∂ℓ\n",
      "∂v1 ·\n",
      "∑\n",
      "v2∈ρ(v1)\n",
      "∂v1\n",
      "∂v2 ·...·\n",
      "∑\n",
      "vk\n",
      "j∈ρ(vk−1)\n",
      "· ∂vk\n",
      "∂Wuv\n",
      "We conclude that we only need to obtain the E partial derivatives along\n",
      "the edges in order to compute all partial derivatives of the function. The\n",
      "actual product at each node can be computed by a dynamic program in\n",
      "linear time.\n",
      "3.2 Gradient descent for smooth optimization\n",
      "Before moving to stochastic gradient descent, we consider its determinis-\n",
      "tic counterpart: gradient descent, in the context of smooth non-convex\n",
      "optimization. Our notion of solution is a point with small gradient, i.e.\n",
      "∥∇f(x)∥≤ ε.\n",
      "As we prove below, this requiresO( 1\n",
      "ε2 ) iterations, each requiring one gra-\n",
      "dient computation. Recall that gradients can be computed eﬃciently, linear\n",
      "in the number of edges, in feed forward neural networks. Thus, the time to\n",
      "obtain a ε-approximate solution becomes O(|E|m\n",
      "ε2 ) for neural networks with\n",
      "E edges and over m examples.\n",
      "Algorithm 2 Gradient descent\n",
      "1: Input: f, T, initial point x1 ∈K, sequence of step sizes {ηt}\n",
      "2: for t= 1 to T do\n",
      "3: Let yt+1 = xt −ηt∇f(xt), xt+1 = ΠK(yt+1)\n",
      "4: end for\n",
      "5: return x T+1\n",
      "Although the choice of ηt can make a diﬀerence in practice, in theory\n",
      "the convergence of the vanilla GD algorithm is well understood and given in\n",
      "the following theorem. Below we assume that the function is bounded such\n",
      "that |f(x)|≤ M.28 CHAPTER 3. STOCHASTIC GRADIENT DESCENT\n",
      "Theorem 3.2. For unconstrained minimization of β-smooth functions and\n",
      "ηt = 1\n",
      "β, GD Algorithm 2 converges as\n",
      "1\n",
      "T\n",
      "∑\n",
      "t\n",
      "∥∇t∥2 ≤4Mβ\n",
      "T .\n",
      "Proof. Denote by ∇t the shorthand for ∇f(xt), and ht = f(xt) −f(x∗).\n",
      "The Descent Lemma is given in the following simple equation,\n",
      "ht+1 −ht = f(xt+1) −f(xt)\n",
      "≤∇⊤\n",
      "t (xt+1 −xt) + β\n",
      "2 ∥xt+1 −xt∥2 β-smoothness\n",
      "= −ηt∥∇t∥2 + β\n",
      "2 η2\n",
      "t∥∇t∥2 algorithm defn.\n",
      "= −1\n",
      "2β∥∇t∥2 choice of ηt = 1\n",
      "β\n",
      "Thus, summing up over T iterations, we have\n",
      "1\n",
      "2β\n",
      "T∑\n",
      "t=1\n",
      "∥∇t∥2 ≤\n",
      "∑\n",
      "t\n",
      "(ht −ht+1) = h1 −hT+1 ≤2M\n",
      "For convex functions, the above theorem implies convergence in function\n",
      "value due to the following lemma,\n",
      "Lemma 3.3. A convex function satisﬁes\n",
      "ht ≤D∥∇t∥,\n",
      "and an α-strongly convex function satisﬁes\n",
      "ht ≤ 1\n",
      "2α∥∇t∥2.\n",
      "Proof. The gradient upper bound for convex functions gives\n",
      "ht ≤∇t(x∗−xt) ≤D∥∇t∥\n",
      "The strongly convex case appears in Lemma 2.3.3.3. STOCHASTIC GRADIENT DESCENT 29\n",
      "3.3 Stochastic gradient descent\n",
      "In the context of training feed forward neural networks, the key idea of\n",
      "Stochastic Gradient Descent is to modify the updates to be:\n",
      "Wt+1 = Wt −η ˜∇t (3.1)\n",
      "where ˜∇t is a random variable with E[ ˜∇t] = ∇f (Wt) and bounded second\n",
      "moment E[∥˜∇t∥2\n",
      "2] ≤σ2.\n",
      "Luckily, getting the desired ˜∇t random variable is easy in the posed\n",
      "problem since the objective function is already in expectation form so:\n",
      "∇f(W) = ∇ E\n",
      "xi,yi\n",
      "[ℓ(v1(W,xi),yi)] = E\n",
      "xi,yi\n",
      "[∇ℓ(v1(W,xi),yi)].\n",
      "Therefore, at iteration t we can take ˜∇t = ∇ℓ(v1(W,xi),yi) where i ∈\n",
      "{1,...,m }is picked uniformly at random. Based on the observation above,\n",
      "choosing ˜∇t this way preserves the desired expectation. So, for each iteration\n",
      "we only compute the gradient w.r.t. to one random example instead of the\n",
      "entire dataset, thereby drastically improving performance for every step. It\n",
      "remains to analyze how this impacts convergence.\n",
      "Algorithm 3 Stochastic gradient descent\n",
      "1: Input: f, T, initial point x1 ∈K, sequence of step sizes {ηt}\n",
      "2: for t= 1 to T do\n",
      "3: Let yt+1 = xt −ηt∇f(xt), xt+1 = ΠK(yt+1)\n",
      "4: end for\n",
      "5: return x T+1\n",
      "Theorem 3.4. For unconstrained minimization of β-smooth functions and\n",
      "ηt = η=\n",
      "√\n",
      "M\n",
      "βσ2T, SGD Algorithm 3 converges as\n",
      "E\n",
      "[\n",
      "1\n",
      "T\n",
      "∑\n",
      "t\n",
      "∥∇t∥2\n",
      "]\n",
      "≤2\n",
      "√\n",
      "Mβσ2\n",
      "T .30 CHAPTER 3. STOCHASTIC GRADIENT DESCENT\n",
      "Proof. Denote by ∇t the shorthand for ∇f(xt), and ht = f(xt) −f(x∗).\n",
      "The stochastic descent lemma is given in the following equation,\n",
      "E[ht+1 −ht] = E[f(xt+1) −f(xt)]\n",
      "≤E[∇⊤\n",
      "t (xt+1 −xt) + β\n",
      "2 ∥xt+1 −xt∥2] β-smoothness\n",
      "= −E[η∇⊤\n",
      "t ˜∇t] + β\n",
      "2 η2 E ∥˜∇t∥2 algorithm defn.\n",
      "= −η∥∇t∥2 + β\n",
      "2 η2σ2 variance bound.\n",
      "Thus, summing up over T iterations, we have for η=\n",
      "√\n",
      "M\n",
      "βσ2T,\n",
      "E\n",
      "[\n",
      "1\n",
      "T\n",
      "T∑\n",
      "t=1\n",
      "∥∇t∥2\n",
      "]\n",
      "≤ 1\n",
      "Tη\n",
      "∑\n",
      "tE [ht −ht+1] + ηβ\n",
      "2 σ2 ≤M\n",
      "Tη + ηβ\n",
      "2 σ2\n",
      "=\n",
      "√\n",
      "Mβσ2\n",
      "T + 1\n",
      "2\n",
      "√\n",
      "Mβσ2\n",
      "T ≤2\n",
      "√\n",
      "Mβσ2\n",
      "T .\n",
      "We thus conclude that O( 1\n",
      "ε4 ) iterations are needed to ﬁnd a point with\n",
      "∥∇f(x)∥≤ ε, as opposed to O( 1\n",
      "ε2 ). However, each iteration takes O(|E|)\n",
      "time, instead of O(|E|m) time for gradient descent.\n",
      "This is why SGD is one of the most useful algorithms in machine learning.3.4. BIBLIOGRAPHIC REMARKS 31\n",
      "3.4 Bibliographic remarks\n",
      "For in depth treatment of backpropagation and the role of deep neural net-\n",
      "works in machine learning the reader is referred to [25].\n",
      "For detailed rigorous convergence proofs of ﬁrst order methods, see lec-\n",
      "ture notes by Nesterov [57] and Nemirovskii [53, 54], as well as the recent\n",
      "text [13].32 CHAPTER 3. STOCHASTIC GRADIENT DESCENTChapter 4\n",
      "Generalization and\n",
      "Non-Smooth Optimization\n",
      "In previous chapter we have introduced the framework of mathematical op-\n",
      "timization within the context of machine learning. We have described the\n",
      "mathematical formulation of several machine learning problems, notably\n",
      "training neural networks, as optimization problems. We then described as\n",
      "well as analyzed the most useful optimization method to solve such formu-\n",
      "lations: stochastic gradient descent.\n",
      "However, several important questions arise:\n",
      "1. SGD was analyzed for smooth functions. Can we minimize non-smooth\n",
      "objectives?\n",
      "2. Given an ERM problem (a.k.a. learning from examples, see ﬁrst chap-\n",
      "ter), what can we say about generalization to unseen examples? How\n",
      "does it aﬀect optimization?\n",
      "3. Are there faster algorithms than SGD in the context of ML?\n",
      "In this chapter we address the ﬁrst two, and devote the rest of this\n",
      "manuscript/course to the last question.\n",
      "How many examples are needed to learn a certain concept? This is a\n",
      "fundamental question of statistical/computational learning theory that has\n",
      "been studied for decades (see end of chapter for bibliographic references).\n",
      "The classical setting of learning from examples is statistical. It assumes\n",
      "examples are drawn i.i.d from a ﬁxed, arbitrary and unknown distribution.\n",
      "The mathematical optimization formulations that we have derived for the\n",
      "ERM problem assume that we have suﬃciently many examples, such that\n",
      "3334 CHAPTER 4. GENERALIZATION\n",
      "optimizing a certain predictor/neural-network/machine on them will result\n",
      "in a solution that is capable of generalizing to unseen examples. The number\n",
      "of examples needed to generalize is called the sample complexity of the prob-\n",
      "lem, and it depends on the concept we are learning as well as the hypothesis\n",
      "class over which we are trying to optimize.\n",
      "There are dimensionality notions in the literature, notably the VC-\n",
      "dimension and related notions, that give precise bounds on the sample com-\n",
      "plexity for various hypothesis classes. In this text we take an algorithmic\n",
      "approach, which is also deterministic. Instead of studying sample complex-\n",
      "ity, which is non-algorithmic, we study algorithms for regret minimization.\n",
      "We will show that they imply generalization for a broad class of machines.\n",
      "4.1 A note on non-smooth optimization\n",
      "Minimization of a function that is both non-convex and non-smooth is in\n",
      "general hopeless, from an information theoretic perspective. The following\n",
      "image explains why. The depicted function on the interval [0,1] has a single\n",
      "local/global minimum, and if the crevasse is narrow enough, it cannot be\n",
      "found by any method other than extensive brute-force search, which can\n",
      "take arbitrarily long.\n",
      "0 1\n",
      "Figure 4.1: Intractability of nonsmooth optimization\n",
      "Since non-convex and non-smooth optimization is hopeless, in the con-\n",
      "text of non-smooth functions we only consider convex optimization.4.2. MINIMIZING REGRET 35\n",
      "4.2 Minimizing Regret\n",
      "The setting we consider for the rest of this chapter is that of online (convex)\n",
      "optimization. In this setting a learner iteratively predicts a point xt ∈K in\n",
      "a convex set K⊆ Rd, and then receives a cost according to an adversarially\n",
      "chosen convex function ft ∈F from family F.\n",
      "The goal of the algorithms introduced in this chapter is to minimize\n",
      "worst-case regret, or diﬀerence between total cost and that of best point in\n",
      "hindsight:\n",
      "regret = sup\n",
      "f1,...,fT∈F\n",
      "{ T∑\n",
      "t=1\n",
      "ft(xt) −min\n",
      "x∈K\n",
      "T∑\n",
      "t=1\n",
      "ft(x)\n",
      "}\n",
      ".\n",
      "In order to compare regret to optimization error it is useful to consider\n",
      "the average regret, or regret/T. Let ¯xT = 1\n",
      "T\n",
      "∑T\n",
      "t=1 xt be the average decision.\n",
      "If the functions ft are all equal to a single function f : K↦→ R, then Jensen’s\n",
      "inequality implies that f(¯xT) converges to f(x⋆) if the average regret is\n",
      "vanishing, since\n",
      "f(¯xT) −f(x⋆) ≤1\n",
      "T\n",
      "T∑\n",
      "t=1\n",
      "[f(xt) −f(x⋆)] = regret\n",
      "T\n",
      "4.3 Regret implies generalization\n",
      "Statistical learning theory for learning from examples postulates that exam-\n",
      "ples from a certain concept are sampled i.i.d. from a ﬁxed and unknown\n",
      "distribution. The learners’ goal is to choose a hypothesis from a certain\n",
      "hypothesis class that can generalize to unseen examples.\n",
      "More formally, let D be a distribution over labelled examples {ai ∈\n",
      "Rd,bi ∈R}∼D . Let H= {x}, x : Rd ↦→R be a hypothsis class over which\n",
      "we are trying to learn (such as linear separators, deep neural networks,\n",
      "etc.). The generalization error of a hypothesis is the expected error of a\n",
      "hypothesis over randomly chosen examples according to a given loss function\n",
      "ℓ: R ×R ↦→R, which is applied to the prediction of the hypothesis and the\n",
      "true label, ℓ(x(ai),bi). Thus,\n",
      "error(x) = E\n",
      "ai,bi∼D\n",
      "[ℓ(x(ai),bi)].\n",
      "An algorithm that attains sublinear regret over the hypothesis class H,\n",
      "w.r.t. loss functions given by ft(x) = fa,b(x) = ℓ(x(a),b), gives rise to a\n",
      "generalizing hypothesis as follows.36 CHAPTER 4. GENERALIZATION\n",
      "Lemma 4.1. Let ¯x = xt for t ∈[T] be chose uniformly at random from\n",
      "{x1,..., xT}.Then, with expectation taken over random choice of ¯x as well\n",
      "as choices of ft ∼D,\n",
      "E[error(¯x)] ≤E[error(x∗)] + regret\n",
      "T\n",
      "Proof. By random choice of ¯x, we have\n",
      "E[f(¯x)] = E\n",
      "[\n",
      "1\n",
      "T\n",
      "∑\n",
      "t\n",
      "f(xt)\n",
      "]\n",
      "Using the fact that ft ∼D, we have\n",
      "E[error(¯x)] = Ef∼D[f(¯x)]\n",
      "= Eft[ 1\n",
      "T\n",
      "∑\n",
      "tft(xt)]\n",
      "≤Eft[ 1\n",
      "T\n",
      "∑\n",
      "tft(x⋆)] + regret\n",
      "T\n",
      "= Ef[f(x⋆)] + regret\n",
      "T\n",
      "= Ef[error(x⋆)] + regret\n",
      "T\n",
      "4.4 Online gradient descent\n",
      "Perhaps the simplest algorithm that applies to the most general setting of\n",
      "online convex optimization is online gradient descent. This algorithm is an\n",
      "online version of standard gradient descent for oﬄine optimization we have\n",
      "seen in the previous chapter. Pseudo-code for the algorithm is given in\n",
      "Algorithm 4, and a conceptual illustration is given in Figure 4.2.\n",
      "In each iteration, the algorithm takes a step from the previous point in\n",
      "the direction of the gradient of the previous cost. This step may result in\n",
      "a point outside of the underlying convex set. In such cases, the algorithm\n",
      "projects the point back to the convex set, i.e. ﬁnds its closest point in the\n",
      "convex set. Despite the fact that the next cost function may be completely\n",
      "diﬀerent than the costs observed thus far, the regret attained by the algo-\n",
      "rithm is sublinear. This is formalized in the following theorem (recall the\n",
      "deﬁnition of G and D from the previous chapter).\n",
      "Theorem 4.2. Online gradient descent with step sizes {ηt = D\n",
      "G\n",
      "√\n",
      "t, t∈[T]}\n",
      "guarantees the following for all T ≥1:\n",
      "regretT =\n",
      "T∑\n",
      "t=1\n",
      "ft(xt) −min\n",
      "x⋆∈K\n",
      "T∑\n",
      "t=1\n",
      "ft(x⋆) ≤3GD\n",
      "√\n",
      "T4.4. ONLINE GRADIENT DESCENT 37\n",
      "Figure 4.2: Online gradient descent: the iterate xt+1 is derived by advancing\n",
      "xt in the direction of the current gradient ∇t, and projecting back into K.\n",
      "Algorithm 4 online gradient descent\n",
      "1: Input: convex set K, T, x1 ∈K, step sizes {ηt}\n",
      "2: for t= 1 to T do\n",
      "3: Play xt and observe cost ft(xt).\n",
      "4: Update and project:\n",
      "yt+1 = xt −ηt∇ft(xt)\n",
      "xt+1 = Π\n",
      "K\n",
      "(yt+1)\n",
      "5: end for\n",
      "Proof. Let x⋆ ∈arg minx∈K\n",
      "∑T\n",
      "t=1 ft(x). Deﬁne ∇t ≜ ∇ft(xt). By convexity\n",
      "ft(xt) −ft(x⋆) ≤∇⊤\n",
      "t (xt −x⋆) (4.1)\n",
      "We ﬁrst upper-bound ∇⊤\n",
      "t (xt−x⋆) using the update rule for xt+1 and The-\n",
      "orem 2.1 (the Pythagorean theorem):\n",
      "∥xt+1 −x⋆∥2 =\n",
      "Π\n",
      "K\n",
      "(xt −ηt∇t) −x⋆\n",
      "\n",
      "2\n",
      "≤∥xt −ηt∇t −x⋆∥2 (4.2)38 CHAPTER 4. GENERALIZATION\n",
      "Hence,\n",
      "∥xt+1 −x⋆∥2 ≤ ∥xt −x⋆∥2 + η2\n",
      "t∥∇t∥2 −2ηt∇⊤\n",
      "t (xt −x⋆)\n",
      "2∇⊤\n",
      "t (xt −x⋆) ≤ ∥xt −x⋆∥2 −∥xt+1 −x⋆∥2\n",
      "ηt\n",
      "+ ηtG2 (4.3)\n",
      "Summing (4.1) and (4.3) from t = 1 to T, and setting ηt = D\n",
      "G\n",
      "√\n",
      "t (with\n",
      "1\n",
      "η0\n",
      "≜ 0):\n",
      "2\n",
      "( T∑\n",
      "t=1\n",
      "ft(xt) −ft(x⋆)\n",
      ")\n",
      "≤2\n",
      "T∑\n",
      "t=1\n",
      "∇⊤\n",
      "t (xt −x⋆)\n",
      "≤\n",
      "T∑\n",
      "t=1\n",
      "∥xt −x⋆∥2 −∥xt+1 −x⋆∥2\n",
      "ηt\n",
      "+ G2\n",
      "T∑\n",
      "t=1\n",
      "ηt\n",
      "≤\n",
      "T∑\n",
      "t=1\n",
      "∥xt −x⋆∥2\n",
      "(1\n",
      "ηt\n",
      "− 1\n",
      "ηt−1\n",
      ")\n",
      "+ G2\n",
      "T∑\n",
      "t=1\n",
      "ηt\n",
      "1\n",
      "η0\n",
      "≜ 0,\n",
      "∥xT+1 −x∗∥2 ≥0\n",
      "≤D2\n",
      "T∑\n",
      "t=1\n",
      "(1\n",
      "ηt\n",
      "− 1\n",
      "ηt−1\n",
      ")\n",
      "+ G2\n",
      "T∑\n",
      "t=1\n",
      "ηt\n",
      "≤D2 1\n",
      "ηT\n",
      "+ G2\n",
      "T∑\n",
      "t=1\n",
      "ηt telescoping series\n",
      "≤3DG\n",
      "√\n",
      "T.\n",
      "The last inequality follows since ηt = D\n",
      "G\n",
      "√\n",
      "t and ∑T\n",
      "t=1\n",
      "1√\n",
      "t ≤2\n",
      "√\n",
      "T.\n",
      "The online gradient descent algorithm is straightforward to implement,\n",
      "and updates take linear time given the gradient. However, there is a projec-\n",
      "tion step which may take signiﬁcantly longer.\n",
      "4.5 Lower bounds\n",
      "Theorem 4.3. Any algorithm for online convex optimization incurs Ω(DG\n",
      "√\n",
      "T)\n",
      "regret in the worst case. This is true even if the cost functions are generated\n",
      "from a ﬁxed stationary distribution.\n",
      "We give a sketch of the proof; ﬁlling in all details is left as an exercise\n",
      "at the end of this chapter.4.6. ONLINE GRADIENT DESCENT FOR STRONGLY CONVEX FUNCTIONS39\n",
      "Consider an instance of OCO where the convex setKis the n-dimensional\n",
      "hypercube, i.e.\n",
      "K= {x ∈Rn , ∥x∥∞≤1}.\n",
      "There are 2 n linear cost functions, one for each vertex v ∈{±1}n, deﬁned\n",
      "as\n",
      "∀v ∈{±1}n , fv(x) = v⊤x.\n",
      "Notice that both the diameter of Kand the bound on the norm of the cost\n",
      "function gradients, denoted G, are bounded by\n",
      "D≤\n",
      "√\n",
      "n∑\n",
      "i=1\n",
      "22 = 2√n, G=\n",
      "√\n",
      "n∑\n",
      "i=1\n",
      "(±1)2 = √n\n",
      "The cost functions in each iteration are chosen at random, with uniform\n",
      "probability, from the set {fv,v ∈{±1}n}. Denote by vt ∈{±1}n the vertex\n",
      "chosen in iteration t, and denote ft = fvt. By uniformity and independence,\n",
      "for any t and xt chosen online, Evt[ft(xt)] = Evt[v⊤\n",
      "t xt] = 0. However,\n",
      "E\n",
      "v1,...,vT\n",
      "[\n",
      "min\n",
      "x∈K\n",
      "T∑\n",
      "t=1\n",
      "ft(x)\n",
      "]\n",
      "= E\n",
      "\n",
      "min\n",
      "x∈K\n",
      "∑\n",
      "i∈[n]\n",
      "T∑\n",
      "t=1\n",
      "vt(i) ·xi\n",
      "\n",
      "\n",
      "= nE\n",
      "[\n",
      "−\n",
      "⏐⏐⏐⏐⏐\n",
      "T∑\n",
      "t=1\n",
      "vt(1)\n",
      "⏐⏐⏐⏐⏐\n",
      "]\n",
      "i.i.d. coordinates\n",
      "= −Ω(n\n",
      "√\n",
      "T).\n",
      "The last equality is left as exercise 3.\n",
      "The facts above nearly complete the proof of Theorem 4.3; see the exer-\n",
      "cises at the end of this chapter.\n",
      "4.6 Online gradient descent for strongly convex\n",
      "functions\n",
      "The ﬁrst algorithm that achieves regret logarithmic in the number of iter-\n",
      "ations is a twist on the online gradient descent algorithm, changing only\n",
      "the step size. The following theorem establishes logarithmic bounds on the\n",
      "regret if the cost functions are strongly convex.40 CHAPTER 4. GENERALIZATION\n",
      "Theorem 4.4. For α-strongly convex loss functions, online gradient descent\n",
      "with step sizes ηt = 1\n",
      "αt achieves the following guarantee for all T ≥1\n",
      "regretT ≤\n",
      "T∑\n",
      "t=1\n",
      "1\n",
      "αt∥∇t∥2 ≤G2\n",
      "2α(1 + logT).\n",
      "Proof. Let x⋆ ∈arg minx∈K\n",
      "∑T\n",
      "t=1 ft(x). Recall the deﬁnition of regret\n",
      "regretT =\n",
      "T∑\n",
      "t=1\n",
      "ft(xt) −\n",
      "T∑\n",
      "t=1\n",
      "ft(x⋆).\n",
      "Deﬁne ∇t ≜ ∇ft(xt). Applying the deﬁnition of α-strong convexity to\n",
      "the pair of points xt,x∗, we have\n",
      "2(ft(xt) −ft(x⋆)) ≤ 2∇⊤\n",
      "t (xt −x⋆) −α∥x⋆ −xt∥2. (4.4)\n",
      "We proceed to upper-bound ∇⊤\n",
      "t (xt −x⋆). Using the update rule for xt+1\n",
      "and the Pythagorean theorem 2.1, we get\n",
      "∥xt+1 −x⋆∥2 = ∥Π\n",
      "K\n",
      "(xt −ηt∇t) −x⋆∥2 ≤∥xt −ηt∇t −x⋆∥2.\n",
      "Hence,\n",
      "∥xt+1 −x⋆∥2 ≤ ∥xt −x⋆∥2 + η2\n",
      "t∥∇t∥2 −2ηt∇⊤\n",
      "t (xt −x⋆)\n",
      "and\n",
      "2∇⊤\n",
      "t (xt −x⋆) ≤ ∥xt −x⋆∥2 −∥xt+1 −x⋆∥2\n",
      "ηt\n",
      "+ ηt∥∇t∥2. (4.5)4.7. ONLINE GRADIENT DESCENT IMPLIES SGD 41\n",
      "Summing (4.5) from t = 1 to T, setting ηt = 1\n",
      "αt (deﬁne 1\n",
      "η0\n",
      "≜ 0), and\n",
      "combining with (4.4), we have:\n",
      "2\n",
      "T∑\n",
      "t=1\n",
      "(ft(xt) −ft(x⋆))\n",
      "≤\n",
      "T∑\n",
      "t=1\n",
      "∥xt −x⋆∥2\n",
      "(1\n",
      "ηt\n",
      "− 1\n",
      "ηt−1\n",
      "−α\n",
      ")\n",
      "+\n",
      "T∑\n",
      "t=1\n",
      "ηt∥∇t∥2\n",
      "since 1\n",
      "η0\n",
      "≜ 0,∥xT+1 −x∗∥2 ≥0\n",
      "= 0 +\n",
      "T∑\n",
      "t=1\n",
      "1\n",
      "αt∥∇t∥2\n",
      "≤ G2\n",
      "α (1 + logT)\n",
      "4.7 Online Gradient Descent implies SGD\n",
      "In this section we notice that OGD and its regret bounds imply the SGD\n",
      "bounds we have studied in the previous chapter. The main advantage are\n",
      "the guarantees for non-smooth stochastic optimization, and constrained op-\n",
      "timization.\n",
      "Recall that in stochastic optimization, the optimizer attempts to mini-\n",
      "mize a convex function over a convex domain as given by the mathematical\n",
      "program:\n",
      "min\n",
      "x∈K\n",
      "f(x).\n",
      "However, unlike standard oﬄine optimization, the optimizer is given access\n",
      "to a noisy gradient oracle, deﬁned by\n",
      "O(x) ≜ ˜∇x s.t. E[ ˜∇x] = ∇f(x) , E[∥˜∇x∥2] ≤G2\n",
      "That is, given a point in the decision set, a noisy gradient oracle returns\n",
      "a random vector whose expectation is the gradient at the point and whose\n",
      "second moment is bounded by G2.\n",
      "We will show that regret bounds for OCO translate to convergence rates\n",
      "for stochastic optimization. As a special case, consider the online gradient42 CHAPTER 4. GENERALIZATION\n",
      "descent algorithm whose regret is bounded by\n",
      "regretT = O(DG\n",
      "√\n",
      "T)\n",
      "Applying the OGD algorithm over a sequence of linear functions that are\n",
      "deﬁned by the noisy gradient oracle at consecutive points, and ﬁnally re-\n",
      "turning the average of all points along the way, we obtain the stochastic\n",
      "gradient descent algorithm, presented in Algorithm 5.\n",
      "Algorithm 5 stochastic gradient descent\n",
      "1: Input: f, K, T, x1 ∈K, step sizes {ηt}\n",
      "2: for t= 1 to T do\n",
      "3: Let ˜∇t = O(xt) and deﬁne: ft(x) ≜ ⟨˜∇t,x⟩\n",
      "4: Update and project:\n",
      "yt+1 = xt −ηt˜∇t\n",
      "xt+1 = Π\n",
      "K\n",
      "(yt+1)\n",
      "5: end for\n",
      "6: return ¯xT ≜ 1\n",
      "T\n",
      "∑T\n",
      "t=1 xt\n",
      "Theorem 4.5. Algorithm 5 with step sizes ηt = D\n",
      "G\n",
      "√\n",
      "t guarantees\n",
      "E[f(¯xT)] ≤min\n",
      "x⋆∈K\n",
      "f(x⋆) + 3GD√\n",
      "T\n",
      "Proof. By the regret guarantee of OGD, we have\n",
      "E[f(¯xT)] −f(x⋆)\n",
      "≤E[ 1\n",
      "T\n",
      "∑\n",
      "t\n",
      "f(xt)] −f(x⋆) convexity of f (Jensen)\n",
      "≤1\n",
      "T E[\n",
      "∑\n",
      "t\n",
      "⟨∇f(xt),xt −x⋆⟩] convexity again\n",
      "= 1\n",
      "T E[\n",
      "∑\n",
      "t\n",
      "⟨˜∇t,xt −x⋆⟩] noisy gradient estimator\n",
      "= 1\n",
      "T E[\n",
      "∑\n",
      "t\n",
      "ft(xt) −ft(x⋆)] Algorithm 5, line (3)\n",
      "≤regretT\n",
      "T deﬁnition\n",
      "≤3GD√\n",
      "T\n",
      "theorem 4.24.7. ONLINE GRADIENT DESCENT IMPLIES SGD 43\n",
      "It is important to note that in the proof above, we have used the fact\n",
      "that the regret bounds of online gradient descent hold against an adaptive\n",
      "adversary. This need arises since the cost functions ft deﬁned in Algorithm\n",
      "5 depend on the choice of decision xt ∈K.\n",
      "In addition, the careful reader may notice that by plugging in diﬀerent\n",
      "step sizes (also called learning rates) and applying SGD to strongly convex\n",
      "functions, one can attain ˜O(1/T) convergence rates. Details of this deriva-\n",
      "tion are left as exercise 1.44 CHAPTER 4. GENERALIZATION\n",
      "4.8 Exercises\n",
      "1. Prove that SGD for a strongly convex function can, with appropriate\n",
      "parameters ηt, converge as ˜O( 1\n",
      "T). You may assume that the gradient\n",
      "estimators have Euclidean norms bounded by the constant G.\n",
      "2. Design an OCO algorithm that attains the same asymptotic regret\n",
      "bound as OGD, up to factors logarithmic inGand D, without knowing\n",
      "the parameters G and D ahead of time.\n",
      "3. In this exercise we prove a tight lower bound on the regret of any\n",
      "algorithm for online convex optimization.\n",
      "(a) For any sequence of T fair coin tosses, let Nh be the number of\n",
      "head outcomes and Nt be the number of tails. Give an asymp-\n",
      "totically tight upper and lower bound on E[|Nh −Nt|] (i.e., order\n",
      "of growth of this random variable as a function of T, up to mul-\n",
      "tiplicative and additive constants).\n",
      "(b) Consider a 2-expert problem, in which the losses are inversely\n",
      "correlated: either expert one incurs a loss of one and the second\n",
      "expert zero, or vice versa. Use the fact above to design a set-\n",
      "ting in which any experts algorithm incurs regret asymptotically\n",
      "matching the upper bound.\n",
      "(c) Consider the general OCO setting over a convex set K. Design\n",
      "a setting in which the cost functions have gradients whose norm\n",
      "is bounded by G, and obtain a lower bound on the regret as\n",
      "a function of G, the diameter of K, and the number of game\n",
      "iterations.4.9. BIBLIOGRAPHIC REMARKS 45\n",
      "4.9 Bibliographic remarks\n",
      "The OCO framework was introduced by Zinkevich in [87], where the OGD\n",
      "algorithm was introduced and analyzed. Precursors to this algorithm, albeit\n",
      "for less general settings, were introduced and analyzed in [47]. Logarithmic\n",
      "regret algorithms for Online Convex Optimization were introduced and an-\n",
      "alyzed in [32]. For more detailed exposition on this prediction framework\n",
      "and its applications see [31].\n",
      "The SGD algorithm dates back to Robbins and Monro [67]. Application\n",
      "of SGD to soft-margin SVM training was explored in [74]. Tight conver-\n",
      "gence rates of SGD for strongly convex and non-smooth functions were only\n",
      "recently obtained in [35],[62],[76].46 CHAPTER 4. GENERALIZATIONChapter 5\n",
      "Regularization\n",
      "In this chapter we consider a generalization of the gradient descent called\n",
      "by diﬀerent names in diﬀerent communities (such as mirrored-descent, or\n",
      "regularized-follow-the-leader). The common theme of this generalization is\n",
      "called Regularization, a concept that is founded in generalization theory.\n",
      "Since this course focuses on optimization rather than generalization, we\n",
      "shall refer the reader to the generalization aspect of regularization, and\n",
      "focus hereby on optimization algorithms.\n",
      "We start by motivating this general family of methods using the funda-\n",
      "mental problem of decision theory.\n",
      "5.1 Motivation: prediction from expert advice\n",
      "Consider the following fundamental iterative decision making problem:\n",
      "At each time step t = 1 ,2,...,T , the decision maker faces a choice\n",
      "between two actions A or B (i.e., buy or sell a certain stock). The decision\n",
      "maker has assistance in the form ofN “experts” that oﬀer their advice. After\n",
      "a choice between the two actions has been made, the decision maker receives\n",
      "feedback in the form of a loss associated with each decision. For simplicity\n",
      "one of the actions receives a loss of zero (i.e., the “correct” decision) and\n",
      "the other a loss of one.\n",
      "We make the following elementary observations:\n",
      "1. A decision maker that chooses an action uniformly at random each\n",
      "iteration, trivially attains a loss of T\n",
      "2 and is “correct” 50% of the time.\n",
      "4748 CHAPTER 5. REGULARIZATION\n",
      "2. In terms of the number of mistakes, no algorithm can do better in the\n",
      "worst case! In a later exercise, we will devise a randomized setting in\n",
      "which the expected number of mistakes of any algorithm is at least T\n",
      "2 .\n",
      "We are thus motivated to consider a relative performance metric : can\n",
      "the decision maker make as few mistakes as the best expert in hindsight?\n",
      "The next theorem shows that the answer in the worst case is negative for a\n",
      "deterministic decision maker.\n",
      "Theorem 5.1. Let L≤T\n",
      "2 denote the number of mistakes made by the best\n",
      "expert in hindsight. Then there does not exist a deterministic algorithm that\n",
      "can guarantee less than 2L mistakes.\n",
      "Proof. Assume that there are only two experts and one always chooses op-\n",
      "tion A while the other always chooses option B. Consider the setting in\n",
      "which an adversary always chooses the opposite of our prediction (she can\n",
      "do so, since our algorithm is deterministic). Then, the total number of mis-\n",
      "takes the algorithm makes is T. However, the best expert makes no more\n",
      "than T\n",
      "2 mistakes (at every iteration exactly one of the two experts is mis-\n",
      "taken). Therefore, there is no algorithm that can always guarantee less than\n",
      "2L mistakes.\n",
      "This observation motivates the design of random decision making algo-\n",
      "rithms, and indeed, the OCO framework gracefully models decisions on a\n",
      "continuous probability space. Henceforth we prove Lemmas 5.3 and 5.4 that\n",
      "show the following:\n",
      "Theorem 5.2. Let ε ∈(0,1\n",
      "2 ). Suppose the best expert makes L mistakes.\n",
      "Then:\n",
      "1. There is an eﬃcient deterministic algorithm that can guarantee less\n",
      "than 2(1 + ε)L+ 2 logN\n",
      "ε mistakes;\n",
      "2. There is an eﬃcient randomized algorithm for which the expected num-\n",
      "ber of mistakes is at most (1 + ε)L+ log N\n",
      "ε .5.1. MOTIVATION: PREDICTION FROM EXPERT ADVICE 49\n",
      "5.1.1 The weighted majority algorithm\n",
      "The weighted majority (WM) algorithm is intuitive to describe: each expert\n",
      "i is assigned a weight Wt(i) at every iteration t. Initially, we set W1(i) = 1\n",
      "for all experts i ∈[N]. For all t ∈[T] let St(A),St(B) ⊆[N] be the set of\n",
      "experts that choose A (and respectively B) at time t. Deﬁne,\n",
      "Wt(A) =\n",
      "∑\n",
      "i∈St(A)\n",
      "Wt(i) Wt(B) =\n",
      "∑\n",
      "i∈St(B)\n",
      "Wt(i)\n",
      "and predict according to\n",
      "at =\n",
      "{\n",
      "A if Wt(A) ≥Wt(B)\n",
      "B otherwise.\n",
      "Next, update the weights Wt(i) as follows:\n",
      "Wt+1(i) =\n",
      "{\n",
      "Wt(i) if expert i was correct\n",
      "Wt(i)(1 −ε) if expert i was wrong ,\n",
      "where ε is a parameter of the algorithm that will aﬀect its performance.\n",
      "This concludes the description of the WM algorithm. We proceed to bound\n",
      "the number of mistakes it makes.\n",
      "Lemma 5.3. Denote by Mt the number of mistakes the algorithm makes\n",
      "until time t, and by Mt(i) the number of mistakes made by expert i until\n",
      "time t. Then, for any expert i∈[N] we have\n",
      "MT ≤2(1 + ε)MT(i) + 2 logN\n",
      "ε .\n",
      "We can optimize ε to minimize the above bound. The expression on the\n",
      "right hand side is of the form f(x) = ax+ b/x, that reaches its minimum\n",
      "at x =\n",
      "√\n",
      "b/a. Therefore the bound is minimized at ε⋆ =\n",
      "√\n",
      "log N/MT(i).\n",
      "Using this optimal value of ε, we get that for the best expert i⋆\n",
      "MT ≤2MT(i⋆) + O\n",
      "(√\n",
      "MT(i⋆) logN\n",
      ")\n",
      ".\n",
      "Of course, this value of ε⋆ cannot be used in advance since we do not know\n",
      "which expert is the best one ahead of time (and therefore we do not know the\n",
      "value of MT(i⋆)). However, we shall see later on that the same asymptotic\n",
      "bound can be obtained even without this prior knowledge.\n",
      "Let us now prove Lemma 5.3.50 CHAPTER 5. REGULARIZATION\n",
      "Proof. Let Φt = ∑N\n",
      "i=1 Wt(i) for all t∈[T], and note that Φ 1 = N.\n",
      "Notice that Φ t+1 ≤Φt. However, on iterations in which the WM algo-\n",
      "rithm erred, we have\n",
      "Φt+1 ≤Φt(1 −ε\n",
      "2),\n",
      "the reason being that experts with at least half of total weight were wrong\n",
      "(else WM would not have erred), and therefore\n",
      "Φt+1 ≤1\n",
      "2Φt(1 −ε) + 1\n",
      "2Φt = Φt(1 −ε\n",
      "2).\n",
      "From both observations,\n",
      "Φt ≤Φ1(1 −ε\n",
      "2)Mt = N(1 −ε\n",
      "2)Mt.\n",
      "On the other hand, by deﬁnition we have for any expert i that\n",
      "WT(i) = (1 −ε)MT(i).\n",
      "Since the value of WT(i) is always less than the sum of all weights Φ T, we\n",
      "conclude that\n",
      "(1 −ε)MT(i) = WT(i) ≤ΦT ≤N(1 −ε\n",
      "2)MT.\n",
      "Taking the logarithm of both sides we get\n",
      "MT(i) log(1−ε) ≤log N + MT log (1−ε\n",
      "2).\n",
      "Next, we use the approximations\n",
      "−x−x2 ≤log (1−x) ≤−x 0 <x< 1\n",
      "2,\n",
      "which follow from the Taylor series of the logarithm function, to obtain that\n",
      "−MT(i)(ε+ ε2) ≤log N −MT\n",
      "ε\n",
      "2,\n",
      "and the lemma follows.5.1. MOTIVATION: PREDICTION FROM EXPERT ADVICE 51\n",
      "5.1.2 Randomized weighted majority\n",
      "In the randomized version of the WM algorithm, denoted RWM, we choose\n",
      "expert i w.p. pt(i) = Wt(i)/∑N\n",
      "j=1 Wt(j) at time t.\n",
      "Lemma 5.4. Let Mt denote the number of mistakes made by RWM until\n",
      "iteration t. Then, for any expert i∈[N] we have\n",
      "E[MT] ≤(1 + ε)MT(i) + log N\n",
      "ε .\n",
      "The proof of this lemma is very similar to the previous one, where the factor\n",
      "of two is saved by the use of randomness:\n",
      "Proof. As before, let Φt = ∑N\n",
      "i=1 Wt(i) for all t∈[T], and note that Φ1 = N.\n",
      "Let ˜mt = Mt −Mt−1 be the indicator variable that equals one if the RWM\n",
      "algorithm makes a mistake on iteration t. Let mt(i) equal one if the i’th\n",
      "expert makes a mistake on iteration t and zero otherwise. Inspecting the\n",
      "sum of the weights:\n",
      "Φt+1 =\n",
      "∑\n",
      "i\n",
      "Wt(i)(1 −εmt(i))\n",
      "= Φt(1 −ε\n",
      "∑\n",
      "i\n",
      "pt(i)mt(i)) pt(i) = Wt(i)∑\n",
      "jWt(j)\n",
      "= Φt(1 −εE[ ˜mt])\n",
      "≤Φte−εE[ ˜mt]. 1 + x≤ex\n",
      "On the other hand, by deﬁnition we have for any expert i that\n",
      "WT(i) = (1 −ε)MT(i)\n",
      "Since the value of WT(i) is always less than the sum of all weights Φ T, we\n",
      "conclude that\n",
      "(1 −ε)MT(i) = WT(i) ≤ΦT ≤Ne−εE[MT].\n",
      "Taking the logarithm of both sides we get\n",
      "MT(i) log(1−ε) ≤log N −εE[MT]\n",
      "Next, we use the approximation\n",
      "−x−x2 ≤log (1−x) ≤−x , 0 <x< 1\n",
      "252 CHAPTER 5. REGULARIZATION\n",
      "to obtain\n",
      "−MT(i)(ε+ ε2) ≤log N −εE[MT],\n",
      "and the lemma follows.\n",
      "5.1.3 Hedge\n",
      "The RWM algorithm is in fact more general: instead of considering a dis-\n",
      "crete number of mistakes, we can consider measuring the performance of an\n",
      "expert by a non-negative real number ℓt(i), which we refer to as the loss\n",
      "of the expert i at iteration t. The randomized weighted majority algorithm\n",
      "guarantees that a decision maker following its advice will incur an average\n",
      "expected loss approaching that of the best expert in hindsight.\n",
      "Historically, this was observed by a diﬀerent and closely related algorithm\n",
      "called Hedge.\n",
      "Algorithm 6 Hedge\n",
      "1: Initialize: ∀i∈[N], W1(i) = 1\n",
      "2: for t= 1 to T do\n",
      "3: Pick it ∼R Wt, i.e., it = i with probability xt(i) = Wt(i)∑\n",
      "jWt(j)\n",
      "4: Incur loss ℓt(it).\n",
      "5: Update weights Wt+1(i) = Wt(i)e−εℓt(i)\n",
      "6: end for\n",
      "Henceforth, denote in vector notation the expected loss of the algorithm\n",
      "by\n",
      "E[ℓt(it)] =\n",
      "N∑\n",
      "i=1\n",
      "xt(i)ℓt(i) = x⊤\n",
      "t ℓt\n",
      "Theorem 5.5. Let ℓ2\n",
      "t denote the N-dimensional vector of square losses,\n",
      "i.e., ℓ2\n",
      "t(i) = ℓt(i)2, let ε> 0, and assume all losses to be non-negative. The\n",
      "Hedge algorithm satisﬁes for any expert i⋆ ∈[N]:\n",
      "T∑\n",
      "t=1\n",
      "x⊤\n",
      "t ℓt ≤\n",
      "T∑\n",
      "t=1\n",
      "ℓt(i⋆) + ε\n",
      "T∑\n",
      "t=1\n",
      "x⊤\n",
      "t ℓ2\n",
      "t + log N\n",
      "ε5.2. THE REGULARIZATION FRAMEWORK 53\n",
      "Proof. As before, let Φt = ∑N\n",
      "i=1 Wt(i) for all t∈[T], and note that Φ1 = N.\n",
      "Inspecting the sum of weights:\n",
      "Φt+1 = ∑\n",
      "iWt(i)e−εℓt(i)\n",
      "= Φt\n",
      "∑\n",
      "ixt(i)e−εℓt(i) xt(i) = Wt(i)∑\n",
      "jWt(j)\n",
      "≤Φt\n",
      "∑\n",
      "ixt(i)(1 −εℓt(i) + ε2ℓt(i)2)) for x≥0,\n",
      "e−x ≤1 −x+ x2\n",
      "= Φt(1 −εx⊤\n",
      "t ℓt + ε2x⊤\n",
      "t ℓ2\n",
      "t)\n",
      "≤Φte−εx⊤\n",
      "t ℓt+ε2x⊤\n",
      "t ℓ2\n",
      "t. 1 + x≤ex\n",
      "On the other hand, by deﬁnition, for expert i⋆ we have that\n",
      "WT(i⋆) = e−ε∑T\n",
      "t=1 ℓt(i⋆)\n",
      "Since the value of WT(i⋆) is always less than the sum of all weights Φ t, we\n",
      "conclude that\n",
      "WT(i⋆) ≤ΦT ≤Ne−ε∑\n",
      "tx⊤\n",
      "t ℓt+ε2 ∑\n",
      "tx⊤\n",
      "t ℓ2\n",
      "t.\n",
      "Taking the logarithm of both sides we get\n",
      "−ε\n",
      "T∑\n",
      "t=1\n",
      "ℓt(i⋆) ≤log N −ε\n",
      "T∑\n",
      "t=1\n",
      "x⊤\n",
      "t ℓt + ε2\n",
      "T∑\n",
      "t=1\n",
      "x⊤\n",
      "t ℓ2\n",
      "t\n",
      "and the theorem follows by simplifying.\n",
      "5.2 The Regularization framework\n",
      "In the previous section we studied the multiplicative weights update method\n",
      "for decision making. A natural question is: couldn’t we have used online\n",
      "gradient descent for the same exact purpose?\n",
      "Indeed, the setting of prediction from expert advice naturally follows\n",
      "into the framework of online convex optimization. To see this, consider the\n",
      "loss functions given by\n",
      "ft(x) = ℓ⊤\n",
      "t x = E\n",
      "i∼x\n",
      "[ℓt(i)],\n",
      "which capture the expected loss of choosing an expert from distribution\n",
      "x ∈∆n as a linear function.54 CHAPTER 5. REGULARIZATION\n",
      "The regret guarantees we have studied for OGD imply a regret of\n",
      "O(GD\n",
      "√\n",
      "T) = O(\n",
      "√\n",
      "nT).\n",
      "Here we have used the fact that the Eucliean diameter of the simplex is two,\n",
      "and that the losses are bounded by one, hence the Euclidean norm of the\n",
      "gradient vector ℓt is bounded by √n.\n",
      "In contrast, the Hedge algorithm attains regret of O(√Tlog n) for the\n",
      "same problem. How can we explain this discrepancy?!\n",
      "5.2.1 The RFTL algorithm\n",
      "Both OGD and Hedge are, in fact, instantiations of a more general meta-\n",
      "algorithm called RFTL (Regularized-Follow-The-Leader).\n",
      "In an OCO setting of regret minimization, the most straightforward\n",
      "approach for the online player is to use at any time the optimal decision\n",
      "(i.e., point in the convex set) in hindsight. Formally, let\n",
      "xt+1 = arg min\n",
      "x∈K\n",
      "t∑\n",
      "τ=1\n",
      "fτ(x).\n",
      "This ﬂavor of strategy is known as “ﬁctitious play” in economics, and has\n",
      "been named “Follow the Leader” (FTL) in machine learning. It is not hard\n",
      "to see that this simple strategy fails miserably in a worst-case sense. That\n",
      "is, this strategy’s regret can be linear in the number of iterations, as the\n",
      "following example shows: Consider K= [−1,1], let f1(x) = 1\n",
      "2 x, and let fτ\n",
      "for τ = 2,...,T alternate between −x or x. Thus,\n",
      "t∑\n",
      "τ=1\n",
      "fτ(x) =\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "2 x, t is odd\n",
      "−1\n",
      "2 x, otherwise\n",
      "The FTL strategy will keep shifting between xt = −1 and xt = 1, always\n",
      "making the wrong choice.\n",
      "The intuitive FTL strategy fails in the example above because it is un-\n",
      "stable. Can we modify the FTL strategy such that it won’t change decisions\n",
      "often, thereby causing it to attain low regret?\n",
      "This question motivates the need for a general means of stabilizing the\n",
      "FTL method. Such a means is referred to as “regularization”.5.2. THE REGULARIZATION FRAMEWORK 55\n",
      "Algorithm 7 Regularized Follow The Leader\n",
      "1: Input: η >0, regularization function R, and a convex compact set K.\n",
      "2: Let x1 = arg minx∈K{R(x)}.\n",
      "3: for t= 1 to T do\n",
      "4: Predict xt.\n",
      "5: Observe the payoﬀ function ft and let ∇t = ∇ft(xt).\n",
      "6: Update\n",
      "xt+1 = arg min\n",
      "x∈K\n",
      "{\n",
      "η\n",
      "t∑\n",
      "s=1\n",
      "∇⊤\n",
      "s x + R(x)\n",
      "}\n",
      "7: end for\n",
      "The generic RFTL meta-algorithm is deﬁned in Algorithm 7. The reg-\n",
      "ularization function Ris assumed to be strongly convex, smooth, and twice\n",
      "diﬀerentiable.\n",
      "5.2.2 Mirrored Descent\n",
      "An alternative view of this algorithm is in terms of iterative updates, which\n",
      "can be spelled out using the above deﬁnition directly. The resulting algo-\n",
      "rithm is called ”Mirrored Descent”.\n",
      "OMD is an iterative algorithm that computes the current decision using a\n",
      "simple gradient update rule and the previous decision, much like OGD. The\n",
      "generality of the method stems from the update being carried out in a “dual”\n",
      "space, where the duality notion is deﬁned by the choice of regularization:\n",
      "the gradient of the regularization function deﬁnes a mapping from Rn onto\n",
      "itself, which is a vector ﬁeld. The gradient updates are then carried out in\n",
      "this vector ﬁeld.\n",
      "For the RFTL algorithm the intuition was straightforward—the regular-\n",
      "ization was used to ensure stability of the decision. For OMD, regularization\n",
      "has an additional purpose: regularization transforms the space in which gra-\n",
      "dient updates are performed. This transformation enables better bounds in\n",
      "terms of the geometry of the space.\n",
      "The OMD algorithm comes in two ﬂavors: an agile and a lazy version.\n",
      "The lazy version keeps track of a point in Euclidean space and projects onto\n",
      "the convex decision setKonly at decision time. In contrast, the agile version\n",
      "maintains a feasible point at all times, much like OGD.56 CHAPTER 5. REGULARIZATION\n",
      "Algorithm 8 Online Mirrored Descent\n",
      "1: Input: parameter η >0, regularization function R(x).\n",
      "2: Let y1 be such that ∇R(y1) = 0 and x1 = arg minx∈KBR(x||y1).\n",
      "3: for t= 1 to T do\n",
      "4: Play xt.\n",
      "5: Observe the payoﬀ function ft and let ∇t = ∇ft(xt).\n",
      "6: Update yt according to the rule:\n",
      "[Lazy version] ∇R(yt+1) = ∇R(yt) −η∇t\n",
      "[Agile version] ∇R(yt+1) = ∇R(xt) −η∇t\n",
      "Project according to BR:\n",
      "xt+1 = arg min\n",
      "x∈K\n",
      "BR(x||yt+1)\n",
      "7: end for\n",
      "A myriad of questions arise, but ﬁrst, let us see how does this algorithm\n",
      "give rise to both OGD.\n",
      "We note that there are other important special cases of the RFTL meta-\n",
      "algorithm: those are derived with matrix-norm regularization—namely, the\n",
      "von Neumann entropy function, and the log-determinant function, as well\n",
      "as self-concordant barrier regularization. Perhaps most importantly for opti-\n",
      "mization, also the AdaGrad algorithm is obtained via changing regularization—\n",
      "which we shall explore in detail in the next chapter.\n",
      "5.2.3 Deriving online gradient descent\n",
      "To derive the online gradient descent algorithm, we take R(x) = 1\n",
      "2 ∥x −\n",
      "x0∥2\n",
      "2 for an arbitrary x0 ∈K. Projection with respect to this divergence\n",
      "is the standard Euclidean projection (left as an exercise), and in addition,\n",
      "∇R(x) = x−x0. Hence, the update rule for the OMD Algorithm 8 becomes:\n",
      "xt = Π\n",
      "K\n",
      "(yt), yt = yt−1 −η∇t−1 lazy version\n",
      "xt = Π\n",
      "K\n",
      "(yt), yt = xt−1 −η∇t−1 agile version\n",
      "The latter algorithm is exactly online gradient descent, as described in\n",
      "Algorithm 4 in Chapter 4. Furthermore, both variants are identical for the\n",
      "case in which Kis the unit ball.5.3. TECHNICAL BACKGROUND: REGULARIZATION FUNCTIONS57\n",
      "We later prove general regret bounds that will imply a O(GD\n",
      "√\n",
      "T) regret\n",
      "for OGD as a special case of mirrored descent.\n",
      "5.2.4 Deriving multiplicative updates\n",
      "Let R(x) = x log x = ∑\n",
      "ixilog xi be the negative entropy function, where\n",
      "log x is to be interpreted elementwise. Then ∇R(x) = 1 + logx, and hence\n",
      "the update rules for the OMD algorithm become:\n",
      "xt = arg min\n",
      "x∈K\n",
      "BR(x||yt), log yt = log yt−1 −η∇t−1 lazy version\n",
      "xt = arg min\n",
      "x∈K\n",
      "BR(x||yt), log yt = log xt−1 −η∇t−1 agile version\n",
      "With this choice of regularizer, a notable special case is the experts\n",
      "problem we encountered in §5.1, for which the decision set K is the n-\n",
      "dimensional simplex ∆ n = {x ∈Rn\n",
      "+ | ∑\n",
      "ixi = 1}. In this special case, the\n",
      "projection according to the negative entropy becomes scaling by theℓ1 norm\n",
      "(left as an exercise), which implies that both update rules amount to the\n",
      "same algorithm:\n",
      "xt+1(i) = xt(i) ·e−η∇t(i)\n",
      "∑n\n",
      "j=1 xt(j) ·e−η∇t(j) ,\n",
      "which is exactly the Hedge algorithm! The general theorem we shall prove\n",
      "henceforth recovers theO(√Tlog n) bound for prediction from expert advice\n",
      "for this algorithm.\n",
      "5.3 Technical background: regularization functions\n",
      "In the rest of this chapter we analyze the mirrored descent algorithm. For\n",
      "this purpose, consider regularization functions, denoted R : K↦→ R, which\n",
      "are strongly convex and smooth (recall deﬁnitions in §2.1).\n",
      "Although it is not strictly necessary, we assume that the regularization\n",
      "functions in this chapter are twice diﬀerentiable over Kand, for all points\n",
      "x ∈int(K) in the interior of the decision set, have a Hessian ∇2R(x) that\n",
      "is, by the strong convexity of R, positive deﬁnite.\n",
      "We denote the diameter of the set Krelative to the function R as\n",
      "DR =\n",
      "√\n",
      "max\n",
      "x,y∈K\n",
      "{R(x) −R(y)}58 CHAPTER 5. REGULARIZATION\n",
      "Henceforth we make use of general norms and their dual. The dual norm\n",
      "to a norm ∥·∥ is given by the following deﬁnition:\n",
      "∥y∥∗≜ max\n",
      "∥x∥≤1\n",
      "⟨x,y⟩\n",
      "A positive deﬁnite matrix Agives rise to the matrix norm ∥x∥A =\n",
      "√\n",
      "x⊤Ax.\n",
      "The dual norm of a matrix norm is ∥x∥∗\n",
      "A = ∥x∥A−1 .\n",
      "The generalized Cauchy-Schwarz theorem asserts ⟨x,y⟩≤∥ x∥∥y∥∗ and\n",
      "in particular for matrix norms, ⟨x,y⟩≤∥ x∥A∥y∥∗\n",
      "A.\n",
      "In our derivations, we usually consider matrix norms with respect to\n",
      "∇2R(x), the Hessian of the regularization function R(x). In such cases, we\n",
      "use the notation\n",
      "∥x∥y ≜ ∥x∥∇2R(y)\n",
      "and similarly\n",
      "∥x∥∗\n",
      "y ≜ ∥x∥∇−2R(y)\n",
      "A crucial quantity in the analysis with regularization is the remainder\n",
      "term of the Taylor approximation of the regularization function, and es-\n",
      "pecially the remainder term of the ﬁrst order Taylor approximation. The\n",
      "diﬀerence between the value of the regularization function atx and the value\n",
      "of the ﬁrst order Taylor approximation is known as the Bregman divergence,\n",
      "given by\n",
      "Deﬁnition 5.6. Denote by BR(x||y) the Bregman divergence with respect\n",
      "to the function R, deﬁned as\n",
      "BR(x||y) = R(x) −R(y) −∇R(y)⊤(x −y)\n",
      "For twice diﬀerentiable functions, Taylor expansion and the mean-value\n",
      "theorem assert that the Bregman divergence is equal to the second derivative\n",
      "at an intermediate point, i.e., (see exercises)\n",
      "BR(x||y) = 1\n",
      "2∥x −y∥2\n",
      "z,\n",
      "for some point z ∈[x,y], meaning there exists some α ∈[0,1] such that\n",
      "z = αx+ (1−α)y. Therefore, the Bregman divergence deﬁnes a local norm,\n",
      "which has a dual norm. We shall denote this dual norm by\n",
      "∥·∥∗\n",
      "x,y ≜ ∥·∥∗\n",
      "z.5.4. REGRET BOUNDS FOR MIRRORED DESCENT 59\n",
      "With this notation we have\n",
      "BR(x||y) = 1\n",
      "2∥x −y∥2\n",
      "x,y.\n",
      "In online convex optimization, we commonly refer to the Bregman divergence\n",
      "between two consecutive decision points xt and xt+1. In such cases, we\n",
      "shorthand notation for the norm deﬁned by the Bregman divergence with\n",
      "respect to R on the intermediate point in [ xt,xt+1] as ∥·∥ t ≜ ∥·∥ xt,xt+1 .\n",
      "The latter norm is called the local norm at iteration t. With this notation,\n",
      "we have BR(xt||xt+1) = 1\n",
      "2 ∥xt −xt+1∥2\n",
      "t.\n",
      "Finally, we consider below generalized projections that use the Bregman\n",
      "divergence as a distance instead of a norm. Formally, the projection of a\n",
      "point y according to the Bregman divergence with respect to function R is\n",
      "given by\n",
      "arg min\n",
      "x∈K\n",
      "BR(x||y)\n",
      "5.4 Regret bounds for Mirrored Descent\n",
      "In this subsection we prove regret bounds for the agile version of the RFTL\n",
      "algorithm. The analysis is quite diﬀerent than the one for the lazy version,\n",
      "and of independent interest.\n",
      "Theorem 5.7. The RFTL Algorithm 8 attains for every u ∈K the following\n",
      "bound on the regret:\n",
      "regretT ≤2η\n",
      "T∑\n",
      "t=1\n",
      "∥∇t∥∗2\n",
      "t + R(u) −R(x1)\n",
      "η .\n",
      "If an upper bound on the local norms is known, i.e. ∥∇t∥∗\n",
      "t ≤GR for all\n",
      "times t, then we can further optimize over the choice of η to obtain\n",
      "regretT ≤2DRGR\n",
      "√\n",
      "2T.\n",
      "Proof. Since the functions ft are convex, for any x∗∈K,\n",
      "ft(xt) −ft(x∗) ≤∇ft(xt)⊤(xt −x∗).60 CHAPTER 5. REGULARIZATION\n",
      "The following property of Bregman divergences follows easily from the def-\n",
      "inition: for any vectors x,y,z,\n",
      "(x −y)⊤(∇R(z) −∇R(y)) = BR(x,y) −BR(x,z) + BR(y,z).\n",
      "Combining both observations,\n",
      "2(ft(xt) −ft(x∗)) ≤2∇ft(xt)⊤(xt −x∗)\n",
      "= 1\n",
      "η(∇R(yt+1) −∇R(xt))⊤(x∗−xt)\n",
      "= 1\n",
      "η[BR(x∗,xt) −BR(x∗,yt+1) + BR(xt,yt+1)]\n",
      "≤1\n",
      "η[BR(x∗,xt) −BR(x∗,xt+1) + BR(xt,yt+1)]\n",
      "where the last inequality follows from the generalized Pythagorean inequality\n",
      "(see [15] Lemma 11.3), asxt+1 is the projection w.r.t the Bregman divergence\n",
      "of yt+1 and x∗∈K is in the convex set. Summing over all iterations,\n",
      "2regret ≤ 1\n",
      "η[BR(x∗,x1) −BR(x∗,xT)] +\n",
      "T∑\n",
      "t=1\n",
      "1\n",
      "ηBR(xt,yt+1)\n",
      "≤ 1\n",
      "ηD2 +\n",
      "T∑\n",
      "t=1\n",
      "1\n",
      "ηBR(xt,yt+1) (5.1)\n",
      "We proceed to boundBR(xt,yt+1). By deﬁnition of Bregman divergence,\n",
      "and the generalized Cauchy-Schwartz inequality,\n",
      "BR(xt,yt+1) + BR(yt+1,xt) = (∇R(xt) −∇R(yt+1))⊤(xt −yt+1)\n",
      "= η∇ft(xt)⊤(xt −yt+1)\n",
      "≤η∥∇ft(xt)∥∗∥xt −yt+1∥\n",
      "≤1\n",
      "2η2G2\n",
      "∗+ 1\n",
      "2∥xt −yt+1∥2.\n",
      "where in the last inequality follows from (a−b)2 ≥0. Thus, by our assump-\n",
      "tion BR(x,y) ≥1\n",
      "2 ∥x −y∥2, we have\n",
      "BR(xt,yt+1) ≤1\n",
      "2η2G2\n",
      "∗+ 1\n",
      "2∥xt −yt+1∥2 −BR(yt+1,xt) ≤1\n",
      "2η2G2\n",
      "∗.\n",
      "Plugging back into Equation (5.1), and by non-negativity of the Bregman\n",
      "divergence, we get\n",
      "regret ≤1\n",
      "2[1\n",
      "ηD2 + 1\n",
      "2ηTG2\n",
      "∗] ≤DG∗\n",
      "√\n",
      "T ,5.4. REGRET BOUNDS FOR MIRRORED DESCENT 61\n",
      "by taking η= D\n",
      "2\n",
      "√\n",
      "TG∗62 CHAPTER 5. REGULARIZATION\n",
      "5.5 Exercises\n",
      "1. (a) Show that the dual norm to a matrix norm given by A ≻ 0\n",
      "corresponds to the matrix norm of A−1.\n",
      "(b) Prove the generalized Cauchy-Schwarz inequality for any norm,\n",
      "i.e.,\n",
      "⟨x,y⟩≤∥ x∥∥y∥∗\n",
      "2. Prove that the Bregman divergence is equal to the local norm at an\n",
      "intermediate point, that is:\n",
      "BR(x||y) = 1\n",
      "2∥x −y∥2\n",
      "z,\n",
      "where z ∈[x,y] and the interval [x,y] is deﬁned as\n",
      "[x,y] = {v = αx + (1 −α)y , α∈[0,1]}\n",
      "3. Let R(x) = 1\n",
      "2 ∥x−x0∥2 be the (shifted) Euclidean regularization func-\n",
      "tion. Prove that the corresponding Bregman divergence is the Eu-\n",
      "clidean metric. Conclude that projections with respect to this diver-\n",
      "gence are standard Euclidean projections.\n",
      "4. Prove that both agile and lazy versions of the OMD meta-algorithm\n",
      "are equivalent in the case that the regularization is Euclidean and the\n",
      "decision set is the Euclidean ball.\n",
      "5. For this problem the decision set is the n-dimensional simplex. Let\n",
      "R(x) = x log x be the negative entropy regularization function. Prove\n",
      "that the corresponding Bregman divergence is the relative entropy,\n",
      "and prove that the diameter DR of the n-dimensional simplex with\n",
      "respect to this function is bounded by log n. Show that projections\n",
      "with respect to this divergence over the simplex amounts to scaling by\n",
      "the ℓ1 norm.\n",
      "6. ∗ A set K⊆ Rd is symmetric if x ∈K implies −x ∈K. Symmetric\n",
      "sets gives rise to a natural deﬁnition of a norm. Deﬁne the function\n",
      "∥·∥K: Rd ↦→R as\n",
      "∥x∥K= arg min\n",
      "α>0\n",
      "{1\n",
      "αx ∈K\n",
      "}\n",
      "Prove that ∥·∥Kis a norm if and only if Kis convex.5.6. BIBLIOGRAPHIC REMARKS 63\n",
      "5.6 Bibliographic Remarks\n",
      "Regularization in the context of online learning was ﬁrst studied in [26]\n",
      "and [48]. The inﬂuential paper of Kalai and Vempala [45] coined the term\n",
      "“follow-the-leader” and introduced many of the techniques that followed\n",
      "in OCO. The latter paper studies random perturbation as a regularization\n",
      "and analyzes the follow-the-perturbed-leader algorithm, following an early\n",
      "development by [29] that was overlooked in learning for many years.\n",
      "In the context of OCO, the term follow-the-regularized-leader was coined\n",
      "in [73, 71], and at roughly the same time an essentially identical algorithm\n",
      "was called “RFTL” in [1]. The equivalence of RFTL and Online Mirrored\n",
      "Descent was observed by [34].64 CHAPTER 5. REGULARIZATIONChapter 6\n",
      "Adaptive Regularization\n",
      "In the previous chapter we have studied a geometric extension of online /\n",
      "stochastic / determinisitic gradient descent. The technique to achieve it is\n",
      "called regularization, and we have seen how for the problem of prediction\n",
      "from expert advice, it can potentially given exponential improvements in\n",
      "the dependence on the dimension.\n",
      "A natural question that arises is whether we can automatically learn the\n",
      "optimal regularization, i.e. best algorithm from the mirrored-descent class,\n",
      "for the problem at hand?\n",
      "The answer is positive in a strong sense: it is theoretically possible to\n",
      "learn the optimal regularization online and in a data-speciﬁc way. Not only\n",
      "that, the resulting algorithms exhibit the most signiﬁcant speedups in train-\n",
      "ing deep neural networks from all accelerations studied thus far.\n",
      "6.1 Adaptive Learning Rates: Intuition\n",
      "The intuition for adaptive regularization is simple: consider an optimization\n",
      "problem which is axis-aligned, in which each coordinate is independent of\n",
      "the rest. It is reasonable to ﬁne tune the learning rate for each coordinate\n",
      "separately - to achieve optimal convergence in that particular subspace of\n",
      "the problem, independently of the rest.\n",
      "Thus, it is reasonable to change the SGD update rule from xt+1 ←\n",
      "xt −η∇t, to the more robust\n",
      "xt+1 ←xt −Dt∇t,\n",
      "where Dt is a diagonal matrix that contains in coordinate ( i,i) the learning\n",
      "rate for coordinate i in the gradient. Recall from the previous sections\n",
      "6566 CHAPTER 6. ADAPTIVE REGULARIZATION\n",
      "that the optimal learning rate for stochastic non-convex optimization is of\n",
      "the order O( 1√\n",
      "t). More precisely, in Theorem 3.4, we have seen that this\n",
      "learning rate should be on the order of O( 1√\n",
      "tσ2 ), where σ2 is the variance of\n",
      "the stochastic gradients. The empirical estimator of the latter is∑\n",
      "i<t∥∇i∥2.\n",
      "Thus, the robust version of stochastic gradient descent for smooth non-\n",
      "convex optimization should behave as the above equation, with\n",
      "Dt(i,i) = 1√∑\n",
      "i<t∇t(i)2 .\n",
      "This is exactly the diagonal version of the AdaGrad algorithm! We continue\n",
      "to rigorously derive it and prove its performance guarantee.\n",
      "6.2 A Regularization Viewpoint\n",
      "In the previous chapter we have introduced regularization as a general\n",
      "methodology for deriving online convex optimization algorithms. Theorem\n",
      "5.7 bounds the regret of the Mirrored Descent algorithm for any strongly\n",
      "convex regularizer as\n",
      "regretT ≤max\n",
      "u∈K\n",
      "√\n",
      "2\n",
      "∑\n",
      "t\n",
      "∥∇t∥∗2\n",
      "t BR(u||x1).\n",
      "In addition, we have seen how to derive the online gradient descent and the\n",
      "multiplicative weights algorithms as special cases of the RFTL methodology.\n",
      "We consider the following question: thus far we have thought of R as\n",
      "a strongly convex function. But which strongly convex function should we\n",
      "choose to minimize regret? This is a deep and diﬃcult question which has\n",
      "been considered in the optimization literature since its early developments.\n",
      "The ML approach is to learn the optimal regularization online. That is,\n",
      "a regularizer that adapts to the sequence of cost functions and is in a sense\n",
      "the “optimal” regularization to use in hindsight. We formalize this in the\n",
      "next section.\n",
      "6.3 Tools from Matrix Calculus\n",
      "Many of the inequalities that we are familiar with for positive real numbers\n",
      "hold for positive semi-deﬁnite matrices as well. We henceforth need the\n",
      "following inequality, which is left as an exercise,6.4. THE ADAGRAD ALGORITHM AND ITS ANALYSIS 67\n",
      "Proposition 6.1. For positive deﬁnite matrices A≽ B ≻0:\n",
      "2Tr((A−B)1/2) + Tr(A−1/2B) ≤2Tr(A1/2).\n",
      "Next, we require a structural result which explicitly gives the optimal\n",
      "regularization as a function of the gradients of the cost functions. For a\n",
      "proof see the exercises.\n",
      "Proposition 6.2. Let A≽ 0. The minimizer of the following minimization\n",
      "problem:\n",
      "min\n",
      "X\n",
      "Tr(X−1A)\n",
      "subject to X ≽ 0\n",
      "Tr(X) ≤1,\n",
      "is X = A1/2/Tr(A1/2), and the minimum objective value is Tr2(A1/2).\n",
      "6.4 The AdaGrad Algorithm and Its Analysis\n",
      "To be more formal, let us consider the set of all strongly convex regulariza-\n",
      "tion functions with a ﬁxed and bounded Hessian in the set\n",
      "∀x ∈K . ∇2R(x) = ∇2 ∈H ≜ {X ∈Rn×n ; Tr(X) ≤1 , X≽ 0}\n",
      "The set His a restricted class of regularization functions (which does not\n",
      "include the entropic regularization). However, it is a general enough class\n",
      "to capture online gradient descent along with any rotation of the Euclidean\n",
      "regularization.\n",
      "Algorithm 9 AdaGrad (Full Matrix version)\n",
      "1: Input: parameters η,x1 ∈K.\n",
      "2: Initialize: S0 = G0 = 0,\n",
      "3: for t= 1 to T do\n",
      "4: Predict xt, suﬀer loss ft(xt).\n",
      "5: Update:\n",
      "St = St−1 + ∇t∇⊤\n",
      "t , Gt = St1/2\n",
      "yt+1 = xt −ηG−1\n",
      "t ∇t\n",
      "xt+1 = arg min\n",
      "x∈K\n",
      "∥yt+1 −x∥2\n",
      "Gt\n",
      "6: end for68 CHAPTER 6. ADAPTIVE REGULARIZATION\n",
      "The problem of learning the optimal regularization has given rise to Algo-\n",
      "rithm 9, known as the AdaGrad (Adaptive subGradient method) algorithm.\n",
      "In the algorithm deﬁnition and throughout this chapter, the notation A−1\n",
      "refers to the Moore-Penrose pseudoinverse of the matrix A. Perhaps sur-\n",
      "prisingly, the regret of AdaGrad is at most a constant factor larger than the\n",
      "minimum regret of all RFTL algorithm with regularization functions whose\n",
      "Hessian is ﬁxed and belongs to the class H. The regret bound on AdaGrad\n",
      "is formally stated in the following theorem.\n",
      "Theorem 6.3. Let {xt}be deﬁned by Algorithm 9 with parameters η= D,\n",
      "where\n",
      "D= max\n",
      "u∈K\n",
      "∥u −x1∥2.\n",
      "Then for any x⋆ ∈K,\n",
      "regretT(AdaGrad) ≤2D\n",
      "√\n",
      "min\n",
      "H∈H\n",
      "∑\n",
      "t\n",
      "∥∇t∥∗2\n",
      "H.\n",
      "Before proving this theorem, notice that it delivers on one of the promised\n",
      "accounts: comparing to the bound of Theorem 5.7 and ignoring the diameter\n",
      "D and dimensionality, the regret bound is as good as the regret of RFTL\n",
      "for the class of regularization functions.\n",
      "We proceed to prove Theorem 6.3. First, a direct corollary of Proposition\n",
      "6.2 is that\n",
      "Corollary 6.4.\n",
      "√\n",
      "min\n",
      "H∈H\n",
      "∑\n",
      "t\n",
      "∥∇t∥∗2\n",
      "H =\n",
      "√\n",
      "minH∈HTr(H−1 ∑\n",
      "t∇t∇⊤\n",
      "t )\n",
      "= Tr\n",
      "√∑\n",
      "t∇t∇⊤\n",
      "t = Tr(GT)\n",
      "Hence, to prove Theorem 6.3, it suﬃces to prove the following lemma.\n",
      "Lemma 6.5.\n",
      "regretT(AdaGrad) ≤2DTr(GT) = 2D\n",
      "√\n",
      "min\n",
      "H∈H\n",
      "∑\n",
      "t\n",
      "∥∇t∥∗2\n",
      "H.\n",
      "Proof. By the deﬁnition of yt+1:\n",
      "yt+1 −x⋆ = xt −x⋆ −ηGt−1∇t, (6.1)6.4. THE ADAGRAD ALGORITHM AND ITS ANALYSIS 69\n",
      "and\n",
      "Gt(yt+1 −x⋆) = Gt(xt −x⋆) −η∇t. (6.2)\n",
      "Multiplying the transpose of (6.1) by (6.2) we get\n",
      "(yt+1 −x⋆)⊤Gt(yt+1 −x⋆) =\n",
      "(xt−x⋆)⊤Gt(xt−x⋆) −2η∇⊤\n",
      "t (xt−x⋆) + η2∇⊤\n",
      "t G−1\n",
      "t ∇t. (6.3)\n",
      "Since xt+1 is the projection of yt+1 in the norm induced by Gt, we have (see\n",
      "§2.1.1)\n",
      "(yt+1 −x⋆)⊤Gt(yt+1 −x⋆) = ∥yt+1 −x⋆∥2\n",
      "Gt ≥∥xt+1 −x⋆∥2\n",
      "Gt.\n",
      "This inequality is the reason for using generalized projections as opposed\n",
      "to standard projections, which were used in the analysis of online gradient\n",
      "descent (see §4.4 Equation (4.2)). This fact together with (6.3) gives\n",
      "∇⊤\n",
      "t (xt−x⋆) ≤ η\n",
      "2∇⊤\n",
      "t G−1\n",
      "t ∇t + 1\n",
      "2η\n",
      "(\n",
      "∥xt −x⋆∥2\n",
      "Gt −∥xt+1 −x⋆∥2\n",
      "Gt\n",
      ")\n",
      ".\n",
      "Now, summing up over t= 1 to T we get that\n",
      "T∑\n",
      "t=1\n",
      "∇⊤\n",
      "t (xt −x⋆) ≤η\n",
      "2\n",
      "T∑\n",
      "t=1\n",
      "∇⊤\n",
      "t G−1\n",
      "t ∇t + 1\n",
      "2η∥x1 −x⋆∥2\n",
      "G0 (6.4)\n",
      "+ 1\n",
      "2η\n",
      "T∑\n",
      "t=1\n",
      "(\n",
      "∥xt −x⋆∥2\n",
      "Gt −∥xt −x⋆∥2\n",
      "Gt−1\n",
      ")\n",
      "− 1\n",
      "2η∥xT+1 −x⋆∥2\n",
      "GT\n",
      "≤η\n",
      "2\n",
      "T∑\n",
      "t=1\n",
      "∇⊤\n",
      "t G−1\n",
      "t ∇t + 1\n",
      "2η\n",
      "T∑\n",
      "t=1\n",
      "(xt−x⋆)⊤(Gt −Gt−1)(xt−x⋆).\n",
      "In the last inequality we use the fact that G0 = 0. We proceed to bound\n",
      "each of the terms above separately.\n",
      "Lemma 6.6. With St,Gt as deﬁned in Algorithm 9,\n",
      "T∑\n",
      "t=1\n",
      "∇⊤\n",
      "t G−1\n",
      "t ∇t ≤2\n",
      "T∑\n",
      "t=1\n",
      "∇⊤\n",
      "t G−1\n",
      "T ∇t ≤2Tr(GT).\n",
      "Proof. We prove the lemma by induction. The base case follows since\n",
      "∇⊤\n",
      "1 G−1\n",
      "1 ∇1 = Tr(G−1\n",
      "1 ∇1∇⊤\n",
      "1 )\n",
      "= Tr(G−1\n",
      "1 G2\n",
      "1)\n",
      "= Tr(G1).70 CHAPTER 6. ADAPTIVE REGULARIZATION\n",
      "Assuming the lemma holds for T−1, we get by the inductive hypothesis\n",
      "T∑\n",
      "t=1\n",
      "∇⊤\n",
      "t G−1\n",
      "t ∇t ≤2Tr(GT−1) + ∇⊤\n",
      "TG−1\n",
      "T ∇T\n",
      "= 2Tr((G2\n",
      "T −∇T∇⊤\n",
      "T)1/2) + Tr(G−1\n",
      "T ∇T∇⊤\n",
      "T)\n",
      "≤2Tr(GT).\n",
      "Here, the last inequality is due to the matrix inequality 6.1.\n",
      "Lemma 6.7.\n",
      "T∑\n",
      "t=1\n",
      "(xt−x⋆)⊤(Gt −Gt−1)(xt−x⋆) ≤D2Tr(GT).\n",
      "Proof. By deﬁnition St ≽ St−1, and hence Gt ≽ Gt−1. Thus,\n",
      "T∑\n",
      "t=1\n",
      "(xt−x⋆)⊤(Gt −Gt−1)(xt−x⋆)\n",
      "≤\n",
      "T∑\n",
      "t=1\n",
      "D2λmax(Gt −Gt−1)\n",
      "≤D2\n",
      "T∑\n",
      "t=1\n",
      "Tr(Gt −Gt−1) A≽ 0 ⇒ λmax(A) ≤Tr(A)\n",
      "= D2\n",
      "T∑\n",
      "t=1\n",
      "(Tr(Gt) −Tr(Gt−1)) linearity of the trace\n",
      "≤D2Tr(GT).\n",
      "Plugging both lemmas into Equation (6.4), we obtain\n",
      "T∑\n",
      "t=1\n",
      "∇⊤\n",
      "t (xt −x⋆) ≤ ηTr(GT) + 1\n",
      "2ηD2Tr(GT) ≤2DTr(GT).6.5. DIAGONAL ADAGRAD 71\n",
      "6.5 Diagonal AdaGrad\n",
      "The AdaGrad algorithm maintains potentially dense matrices, and requires\n",
      "the computation of the square root of these matrices. This is usually pro-\n",
      "hibitive in machine learning applications in which the dimension is very\n",
      "large. Fortunately, the same ideas can be applied with almost no com-\n",
      "putational overhead on top of vanilla SGD, using the diagonal version of\n",
      "AdaGrad given by:\n",
      "Algorithm 10 AdaGrad (diagonal version)\n",
      "1: Input: parameters η,x1 ∈K.\n",
      "2: Initialize: S0 = G0 = 0,\n",
      "3: for t= 1 to T do\n",
      "4: Predict xt, suﬀer loss ft(xt).\n",
      "5: Update:\n",
      "St = St−1 + diag(∇t∇⊤\n",
      "t ), Gt = St1/2\n",
      "yt+1 = xt −ηG−1\n",
      "t ∇t\n",
      "xt+1 = arg min\n",
      "x∈K\n",
      "∥yt+1 −x∥2\n",
      "Gt\n",
      "6: end for\n",
      "In contrast to the full-matrix version, this version can be implemented in\n",
      "linear time and space, since diagonal matrices can be manipulated as vectors.\n",
      "Thus, memory overhead is only a single d-dimensional vector, which is used\n",
      "to represent the diagonal preconditioning (regularization) matrix, and the\n",
      "computational overhead is a few vector manipulations per iteration.\n",
      "Very similar to the full matrix case, the diagonal AdaGrad algorithm\n",
      "can be analyzed and the following performance bound obtained:\n",
      "Theorem 6.8. Let {xt}be deﬁned by Algorithm 10 with parameters η =\n",
      "D∞, where\n",
      "D∞= max\n",
      "u∈K\n",
      "∥u −x1∥∞,\n",
      "and let diag (H) be the set of all diagonal matrices in H. Then for any\n",
      "x⋆ ∈K,\n",
      "regretT(D-AdaGrad) ≤2D∞\n",
      "√\n",
      "min\n",
      "H∈diag(H)\n",
      "∑\n",
      "t\n",
      "∥∇t∥∗2\n",
      "H.72 CHAPTER 6. ADAPTIVE REGULARIZATION\n",
      "6.6 State-of-the-art: from Adam to Shampoo and\n",
      "beyond\n",
      "Since the introduction of the adaptive regularization technique in the con-\n",
      "text of regret minimization, several improvements were introduced that now\n",
      "compose state-of-the-art. A few notable advancements include:\n",
      "AdaDelta: The algorithm keeps an exponential average of past gradients and uses\n",
      "that in the update step.\n",
      "Adam: Adds a sliding window to AdaGrad, as well as adding a form of mo-\n",
      "mentum via estimating the second moments of past gradients and\n",
      "adjusting the update accordingly.\n",
      "Shampoo: Interpolates between full-matrix and diagonal adagrad in the context\n",
      "of deep neural networks: use of the special layer structure to reduce\n",
      "memory constraints.\n",
      "AdaFactor: Suggests a Shampoo-like approach to reduce memory footprint even\n",
      "further, to allow the training of huge models.\n",
      "GGT: While full-matrix AdaGrad is computationally slow due to the cost\n",
      "of manipulating matrices, this algorithm uses recent gradients (a thin\n",
      "matrix G), and via linear algebraic manipulations reduces computa-\n",
      "tion by never computing GG⊤, but rather only G⊤G, which is low\n",
      "dimensional.\n",
      "SM3 , ET: Diagonal AdaGrad requires an extra O(n) memory to store diag( Gt).\n",
      "These algorithms, inspired by AdaFactor, approximate Gt as a low\n",
      "rank tensor to save memory and computation.6.7. EXERCISES 73\n",
      "6.7 Exercises\n",
      "1. ∗Prove that for positive deﬁnite matrices A≽ B ≻0 it holds that\n",
      "(a) A1/2 ≽ B1/2\n",
      "(b) 2 Tr((A−B)1/2) + Tr(A−1/2B) ≤2Tr(A1/2).\n",
      "2. ∗Consider the following minimization problem where A≻0:\n",
      "min\n",
      "X\n",
      "Tr(X−1A)\n",
      "subject to X ≻0\n",
      "Tr(X) ≤1.\n",
      "Prove that its minimizer is given by X = A1/2/Tr(A1/2), and the\n",
      "minimum is obtained at Tr2(A1/2).74 CHAPTER 6. ADAPTIVE REGULARIZATION\n",
      "6.8 Bibliographic Remarks\n",
      "The AdaGrad algorithm was introduced in [19, 18], its diagonal version\n",
      "was also discovered in parallel in [52]. Adam [46] and RMSprop [39] are\n",
      "widely used methods based on adaptive regularization. A cleaner analysis\n",
      "was recently proposed in [27], see also [17].\n",
      "Adaptive regularization has received much attention recently, see e.g.,\n",
      "[60, 85]. Newer algorithmic developments on adaptive regularization include\n",
      "Shampoo [28], GGT [3], AdaFactor [77], Extreme Tensoring [16] and SM3\n",
      "[6].Chapter 7\n",
      "Variance Reduction\n",
      "In the previous chapter we have studied the ﬁrst of our three acceleration\n",
      "techniques over SGD, adaptive regularization, which is a geometric tool for\n",
      "acceleration. In this chapter we introduce the second ﬁrst-order accelera-\n",
      "tion technique, called variance reduction. This technique is probabilistic in\n",
      "nature, and applies to more restricted settings of mathematical optimiza-\n",
      "tion in which the objective function has a ﬁnite-sum structure. Namely, we\n",
      "consider optimization problems of the form\n",
      "min\n",
      "x∈K\n",
      "f(x) , f(x) = 1\n",
      "m\n",
      "m∑\n",
      "i=1\n",
      "fi(x) . (7.1)\n",
      "Such optimization problems are canonical in training of ML models, con-\n",
      "vex and non-convex. However, in the context of machine learning we should\n",
      "remember that the ultimate goal is generalization rather than training.\n",
      "7.1 Variance reduction: Intuition\n",
      "The intuition for variance reduction is simple, and comes from trying to\n",
      "improve the naive convergence bounds for SGD that we have covered in the\n",
      "ﬁrst lesson.\n",
      "Recall the SGD update rule xt+1 ←xt−ηˆ∇t, in which ˆ∇t is an unbiased\n",
      "estimator for the gradient such that\n",
      "E[ ˆ∇t] = ∇t , E[∥ˆ∇t∥2\n",
      "2] ≤σ2.\n",
      "We have seen in Theorem 3.4, that for this update rule,\n",
      "E\n",
      "[\n",
      "1\n",
      "T\n",
      "∑\n",
      "t\n",
      "∥∇t∥2\n",
      "]\n",
      "≤2\n",
      "√\n",
      "Mβσ2\n",
      "T .\n",
      "7576 CHAPTER 7. VARIANCE REDUCTION\n",
      "The convergence is proportional to the second moment of the gradient esti-\n",
      "mator, and thus it makes sense to try to reduce this second moment. The\n",
      "variance reduction technique attempts to do so by using the average of all\n",
      "previous gradients, as we show next.\n",
      "7.2 Setting and deﬁnitions\n",
      "We consider the ERM optimization problem over an average of loss func-\n",
      "tions. Before we begin, we need a few preliminaries and assumptions:\n",
      "1. We denote distance to optimality according to function value as\n",
      "ht = f(xt) −f(x∗),\n",
      "and in the k’th epoch of an algorithm, we denote hk\n",
      "t = f(xk\n",
      "t) −f(x∗).\n",
      "2. We denote ˜hk = max\n",
      "{\n",
      "4hk\n",
      "0 , 8αD2\n",
      "k\n",
      "}\n",
      "over an epoch.\n",
      "3. Assume all stochastic gradients have bounded second moments\n",
      "∥ˆ∇t∥2\n",
      "2 ≤σ2.\n",
      "4. We will assume that the individual functions fi in formulation (7.1)\n",
      "are also ˆβ-smooth and have ˆβ-Lipschitz gradient, namely\n",
      "∥∇fi(x) −∇fi(y)∥≤ ˆβ∥x −y∥.\n",
      "5. We will use, proved in Lemma 2.3, that for β-smooth and α-strongly\n",
      "convex f we have\n",
      "ht ≥ 1\n",
      "2β∥∇t∥2\n",
      "and\n",
      "α\n",
      "2 d2\n",
      "t = α\n",
      "2 ∥xt −x∗∥2 ≤ht ≤ 1\n",
      "2α∥∇t∥2.\n",
      "6. Recall that a function f is γ-well-conditioned if it is β-smooth, α-\n",
      "strongly convex and γ ≤α\n",
      "β.7.3. THE VARIANCE REDUCTION ADVANTAGE 77\n",
      "7.3 The variance reduction advantage\n",
      "Consider gradient descent for γ-well conditioned functions, and speciﬁcally\n",
      "used for ML training as in formulation (7.1) . It is well known that GD\n",
      "attains linear convergence rate as we now prove for completeness:\n",
      "Theorem 7.1. For unconstrained minimization of γ-well-conditioned func-\n",
      "tions and ηt = 1\n",
      "β, the Gradient Descent Algorithm 2 converges as\n",
      "ht+1 ≤h1e−γt.\n",
      "Proof.\n",
      "ht+1 −ht = f(xt+1) −f(xt)\n",
      "≤∇⊤\n",
      "t (xt+1 −xt) + β\n",
      "2 ∥xt+1 −xt∥2 β-smoothness\n",
      "= −ηt∥∇t∥2 + β\n",
      "2 η2\n",
      "t∥∇t∥2 algorithm defn.\n",
      "= −1\n",
      "2β∥∇t∥2 choice of ηt = 1\n",
      "β\n",
      "≤−α\n",
      "βht. by (2.1)\n",
      "Thus,\n",
      "ht+1 ≤ht(1 −α\n",
      "β) ≤···≤ h1(1 −γ)t ≤h1e−γt\n",
      "where the last inequality follows from 1 −x≤e−x for all x∈R.\n",
      "However, what is the overall computational cost? Assuming that we can\n",
      "compute the gradient of each loss function corresponding to the individual\n",
      "training examples in O(d) time, the overall running time to compute the\n",
      "gradient is O(md).\n",
      "In order to attain approximation ε to the objective, the algorithm re-\n",
      "quires O(1\n",
      "γ log 1\n",
      "ε) iterations, as per the Theorem above. Thus, the overall\n",
      "running time becomes O(md\n",
      "γ log 1\n",
      "ε). As we show below, variance reduction\n",
      "can reduce this running time to be O((m+ 1\n",
      "˜γ2 )dlog 1\n",
      "ε), where ˜γ is a diﬀerent\n",
      "condition number for the same problem, that is in general smaller than the\n",
      "original. Thus, in one line, the variance reduction advantage can be sum-\n",
      "marized as:\n",
      "md\n",
      "γ log 1\n",
      "ε ↦→(m+ 1\n",
      "˜γ2 )dlog 1\n",
      "ε .78 CHAPTER 7. VARIANCE REDUCTION\n",
      "7.4 A simple variance-reduced algorithm\n",
      "The following simple variance-reduced algorithm illustrates the main ideas\n",
      "of the technique. The algorithm is a stochastic gradient descent variant\n",
      "which proceeds in epochs. Strong convexity implies that the distance to the\n",
      "optimum shrinks with function value, so it is safe to decrease the distance\n",
      "upper bound every epoch.\n",
      "The main innovation is in line 7, which constructs the gradient estimator.\n",
      "Instead of the usual trick - which is to sample one example at random - here\n",
      "the estimator uses the entire gradient computed at the beginning of the\n",
      "current epoch.\n",
      "Algorithm 11 Epoch GD\n",
      "1: Input: f, T, x1\n",
      "0 ∈K, upper bound D1 ≥∥x1\n",
      "0 −x∗∥, step sizes {ηt}\n",
      "2: for k= 1 to log 1\n",
      "ε do\n",
      "3: Let BDk(xk\n",
      "0) be the ball of radius Dk around xk\n",
      "0.\n",
      "4: compute full gradient ∇k\n",
      "0 = ∇f(xk\n",
      "0)\n",
      "5: for t= 1 to T do\n",
      "6: Sample it ∈[m] uniformly at random, let ft = fit.\n",
      "7: construct stochastic gradient ˆ∇k\n",
      "t = ∇ft(xk\n",
      "t) −∇ft(xk\n",
      "0) + ∇k\n",
      "0\n",
      "8: Let yk\n",
      "t+1 = xk\n",
      "t −ηtˆ∇k\n",
      "t, xt+1 = ΠBDk(xk\n",
      "0 ) (yt+1)\n",
      "9: end for\n",
      "10: Set xk+1\n",
      "0 = 1\n",
      "T\n",
      "∑T\n",
      "t=1 xk\n",
      "t. Dk+1 ←Dk/2.\n",
      "11: end for\n",
      "12: return x 0\n",
      "T+1\n",
      "The main guarantee for this algorithm is the following theorem, which\n",
      "delivers upon the aforementioned improvement,\n",
      "Theorem 7.2. Algorithm 11 returns an ε-approximate solution to optimiza-\n",
      "tion problem (7.1) in total time\n",
      "O\n",
      "((\n",
      "m+ 1\n",
      "˜γ2\n",
      ")\n",
      "dlog 1\n",
      "ε\n",
      ")\n",
      ".\n",
      "Let ˜γ = α\n",
      "ˆβ <γ . Then the proof of this theorem follows from the following\n",
      "lemma.\n",
      "Lemma 7.3. For T = ˜O\n",
      "(\n",
      "1\n",
      "˜γ2\n",
      ")\n",
      ", we have\n",
      "E[˜hk+1] ≤1\n",
      "2\n",
      "˜hk.7.4. A SIMPLE VARIANCE-REDUCED ALGORITHM 79\n",
      "Proof. As a ﬁrst step, we bound the variance of the gradients. Due to the\n",
      "fact that xk\n",
      "t ∈BDk(xk\n",
      "0), we have that for k′>k, ∥xk\n",
      "t −xk′\n",
      "t ∥2 ≤4D2\n",
      "k. Thus,\n",
      "∥ˆ∇k\n",
      "t∥2 = ∥∇ft(xk\n",
      "t) −∇ft(xk\n",
      "0) + ∇f(xk\n",
      "0)∥2 deﬁnition\n",
      "≤2∥∇ft(xk\n",
      "t) −∇ft(xk\n",
      "0)∥2 + 2∥∇f(xk\n",
      "0)∥2 (a+ b)2 ≤2a2 + 2b2\n",
      "≤2 ˆβ2∥xk\n",
      "t −xk\n",
      "0∥2 + 4βhk\n",
      "0 smoothness\n",
      "≤8 ˆβ2D2\n",
      "k + 4βhk\n",
      "0 projection step\n",
      "≤ˆβ2 1\n",
      "α\n",
      "˜hk + 4βhk\n",
      "0 ≤˜hk(\n",
      "ˆβ2\n",
      "α + β)\n",
      "Next, using the regret bound for strongly convex functions, we have\n",
      "E[hk+1\n",
      "0 ] ≤E[ 1\n",
      "T\n",
      "∑\n",
      "thk\n",
      "t] Jensen\n",
      "≤ 1\n",
      "αT E[∑\n",
      "t\n",
      "1\n",
      "t∥ˆ∇k\n",
      "t∥2] Theorem 4.4\n",
      "≤ 1\n",
      "αT\n",
      "∑\n",
      "t\n",
      "1\n",
      "t\n",
      "˜hk(\n",
      "ˆβ2\n",
      "α + β) above\n",
      "≤log T\n",
      "T\n",
      "˜hk( 1\n",
      "˜γ2 + 1\n",
      "γ) ˜ γ = α\n",
      "ˆβ\n",
      "Which implies the Lemma by choice ofT, deﬁnition of ˜hk = max\n",
      "{\n",
      "4hk\n",
      "0 , 8αD2\n",
      "k\n",
      "}\n",
      ",\n",
      "and exponential decrease of Dk.\n",
      "The expectation is over the stochastic gradient deﬁnition, and is required\n",
      "for using Theorem 4.4.\n",
      "To obtain the theorem from the lemma above, we need to strengthen\n",
      "it to a high probability statement using a martingale argument. This is\n",
      "possible since the randomness in construction of the stochastic gradients is\n",
      "i.i.d.\n",
      "The lemma now implies the theorem by noting that O(log 1\n",
      "ε) epochs\n",
      "suﬃces to get ε-approximation. Each epoch requires the computation of one\n",
      "full gradient, in time O(md), and ˜O( 1\n",
      "˜γ2 ) iterations that require stochastic\n",
      "gradient computation, in time O(d).80 CHAPTER 7. VARIANCE REDUCTION\n",
      "7.5 Bibliographic Remarks\n",
      "The variance reduction technique was ﬁrst introduced as part of the SAG\n",
      "algorithm [70]. Since then a host of algorithms were developed using the\n",
      "technique. The simplest exposition of the technique was given in [44]. The\n",
      "exposition in this chapter is developed from the Epoch GD algorithm [37],\n",
      "which uses a related technique for stochastic strongly convex optimization,\n",
      "as developed in [86].Chapter 8\n",
      "Nesterov Acceleration\n",
      "In previous chapters we have studied our bread and butter technique, SGD,\n",
      "as well as two acceleration techniques of adaptive regularization and variance\n",
      "reduction. In this chapter we study the historically earliest acceleration\n",
      "technique, known as Nesterov acceleration, or simply “acceleration”.\n",
      "For smooth and convex functions, Nesterov acceleration improves the\n",
      "convergence rate to optimality to O( 1\n",
      "T2 ), a quadratic improvement over\n",
      "vanilla gradient descent. Similar accelerations are possible when the func-\n",
      "tion is also strongly convex: an accelerated rate of e−√γT, where γ is the\n",
      "condition number, vs. e−γT of vanilla gradient descent. This improvement\n",
      "is theoretically very signiﬁcant.\n",
      "However, in terms of applicability, Nesterov acceleration is theoretically\n",
      "the most restricted in the context of machine learning: it requires a smooth\n",
      "and convex objective. More importantly, the learning rates of this method\n",
      "are very brittle, and the method is not robust to noise. Since noise is pre-\n",
      "dominant in machine learning, the theoretical guarantees in stochastic op-\n",
      "timization environments are very restricted.\n",
      "However, the heuristic of momentum, which historically inspired acceler-\n",
      "ation, is extremely useful for non-convex stochastic optimization (although\n",
      "not known to yield signiﬁcant improvements in theory).\n",
      "8.1 Algorithm and implementation\n",
      "Nesterov acceleration applies to the general setting of constrained smooth\n",
      "convex optimization:\n",
      "min\n",
      "x∈Rd\n",
      "f(x). (8.1)\n",
      "8182 CHAPTER 8. NESTEROV ACCELERATION\n",
      "For simplicity of presentation, we restrict ourselves to the unconstrained\n",
      "convex and smooth case. Nevertheless, the method can be extended to\n",
      "constrained smooth convex, and potentially strongly convex, settings.\n",
      "The simple method presented in Algorithm 12 below is computationally\n",
      "equivalent to gradient descent. The only overhead is saving three state\n",
      "vectors (that can be reduced to two) instead of one for gradient descent.\n",
      "The following simple accelerated algorithm illustrates the main ideas of the\n",
      "technique.\n",
      "Algorithm 12 Simpliﬁed Nesterov Acceleration\n",
      "1: Input: f, T, initial point x0, parameters η,β,τ .\n",
      "2: for t= 1 to T do\n",
      "3: Set xt+1 = τzt + (1 −τ)yt, and denote ∇t+1 = ∇f(xt+1).\n",
      "4: Let yt+1 = xt+1 −1\n",
      "β∇t+1\n",
      "5: Let zt+1 = zt −η∇t+1\n",
      "6: end for\n",
      "7: return ¯x = 1\n",
      "T\n",
      "∑\n",
      "txt\n",
      "8.2 Analysis\n",
      "The main guarantee for this algorithm is the following theorem.\n",
      "Theorem 8.1. Algorithm 12 converges to an ε-approximate solution to op-\n",
      "timization problem (8.1) in O( 1√ε) iterations.\n",
      "The proof starts with the following lemma which follows from our earlier\n",
      "standard derivations.\n",
      "Lemma 8.2.\n",
      "η∇⊤\n",
      "t+1(zt −x∗) ≤2η2β(f(xt+1) −f(yt+1)) +\n",
      "[\n",
      "∥zt −x∗∥2 −∥zt+1 −x∗∥2]\n",
      ".\n",
      "Proof. The proof is very similar to that of Theorem 4.2. By deﬁnition of zt,\n",
      "1\n",
      "∥zt+1 −x∗∥2 = ∥zt −η∇t+1 −x∗∥2\n",
      "= ∥zt −x∗∥2 −η∇⊤\n",
      "t+1(zt −x∗) + η2∥∇t+1∥2\n",
      "≤∥zt −x∗∥2 −η∇⊤\n",
      "t+1(zt −x∗) + 2η2β(f(xt+1) −f(yt+1)) Lemma 2.3 part 3\n",
      "1Henceforth we use Lemma 2.3 part 3. This proof of this Lemma shows that for\n",
      "y = x −1\n",
      "β∇f(x), it holds that f(x) −f(y) ≥ 1\n",
      "2β∥∇f(x)∥2.8.2. ANALYSIS 83\n",
      "Lemma 8.3. For 2ηβ = 1−τ\n",
      "τ , we have that\n",
      "η∇⊤\n",
      "t+1(xt+1 −x∗) ≤2η2β(f(yt) −f(yt+1)) +\n",
      "[\n",
      "∥zt −x∗∥2 −∥zt+1 −x∗∥2]\n",
      ".\n",
      "Proof.\n",
      "η∇⊤\n",
      "t+1(xt+1 −x∗) −η∇⊤\n",
      "t+1(zt −x∗)\n",
      "= η∇⊤\n",
      "t+1(xt+1 −zt)\n",
      "= (1−τ)η\n",
      "τ ∇⊤\n",
      "t+1(yt −xt+1) τ(xt+1 −zt) = (1 −τ)(yt −xt+1)\n",
      "≤(1−τ)η\n",
      "τ (f(yt) −f(xt+1)). convexity\n",
      "Thus, in combination with Lemma 8.2, and the condition of the Lemma, we\n",
      "get the inequality.\n",
      "We can now sketch the proof of the main theorem.\n",
      "Proof. Telescope Lemma 8.3 for all iterations to obtain:\n",
      "ThT = T(f(¯x) −f(x∗))\n",
      "≤∑\n",
      "t∇⊤\n",
      "t (xt −x∗)\n",
      "≤2ηβ∑\n",
      "t(f(yt) −f(yt+1)) + 1\n",
      "η\n",
      "∑\n",
      "t\n",
      "[\n",
      "∥zt −x∗∥2 −∥zt+1 −x∗∥2]\n",
      "≤2ηβ(f(y1) −f(yT+1)) + 1\n",
      "η\n",
      "[\n",
      "∥z1 −x∗∥2 −∥zT+1 −x∗∥2]\n",
      "≤√2βh1D, optimizing η\n",
      "where h1 is an upper bound on the distance f(y1) −f(x∗), and D bounds\n",
      "the Euclidean distance of zt to the optimum. Thus, we get a recurrence of\n",
      "the form\n",
      "hT ≤\n",
      "√h1\n",
      "T .\n",
      "Restarting Algorithm 12 and adapting the learning rate according to hT\n",
      "gives a rate of convergence of O( 1\n",
      "T2 ) to optimality.84 CHAPTER 8. NESTEROV ACCELERATION\n",
      "8.3 Bibliographic Remarks\n",
      "Accelerated rates of order O( 1\n",
      "T2 ) were obtained by Nemirovski as early as\n",
      "the late seventies. The ﬁrst practically eﬃcient accelerated algorithm is due\n",
      "to Nesterov [56] , see also [57]. The simpliﬁed proof presented hereby is due\n",
      "to [5].Chapter 9\n",
      "The conditional gradient\n",
      "method\n",
      "In many computational and learning scenarios the main bottleneck of opti-\n",
      "mization, both online and oﬄine, is the computation of projections onto the\n",
      "underlying decision set (see§2.1.1). In this chapter we discuss projection-free\n",
      "methods in convex optimization, and some of their applications in machine\n",
      "learning.\n",
      "The motivating example throughout this chapter is the problem of ma-\n",
      "trix completion, which is a widely used and accepted model in the con-\n",
      "struction of recommendation systems. For matrix completion and related\n",
      "problems, projections amount to expensive linear algebraic operations and\n",
      "avoiding them is crucial in big data applications.\n",
      "Henceforth we describe the conditional gradient algorithm, also known as\n",
      "the Frank-Wolfe algorithm. Afterwards, we describe problems for which lin-\n",
      "ear optimization can be carried out much more eﬃciently than projections.\n",
      "We conclude with an application to exploration in reinforcement learning.\n",
      "9.1 Review: relevant concepts from linear algebra\n",
      "This chapter addresses rectangular matrices, which model applications such\n",
      "as recommendation systems naturally. Consider a matrix X ∈Rn×m. A\n",
      "non-negative number σ∈R+ is said to be a singular value for X if there are\n",
      "two vectors u ∈Rn,v ∈Rm such that\n",
      "X⊤u = σv, X v = σu.\n",
      "8586 CHAPTER 9. THE CONDITIONAL GRADIENT METHOD\n",
      "The vectors u,v are called the left and right singular vectors respectively.\n",
      "The non-zero singular values are the square roots of the eigenvalues of the\n",
      "matrix XX⊤(and X⊤X). The matrix X can be written as\n",
      "X = UΣV⊤, U∈Rn×ρ , V⊤∈Rρ×m,\n",
      "where ρ= min{n,m}, the matrix U is an orthogonal basis of the left singular\n",
      "vectors of X, the matrix V is an orthogonal basis of right singular vectors,\n",
      "and Σ is a diagonal matrix of singular values. This form is called the singular\n",
      "value decomposition for X.\n",
      "The number of non-zero singular values for X is called its rank, which\n",
      "we denote by k≤ρ. The nuclear norm of X is deﬁned as the ℓ1 norm of its\n",
      "singular values, and denoted by\n",
      "∥X∥∗=\n",
      "ρ∑\n",
      "i=1\n",
      "σi\n",
      "It can be shown (see exercises) that the nuclear norm is equal to the trace\n",
      "of the square root of the matrix times its transpose, i.e.,\n",
      "∥X∥∗= Tr(\n",
      "√\n",
      "X⊤X)\n",
      "We denote by A•B the inner product of two matrices as vectors in Rn×m,\n",
      "that is\n",
      "A•B =\n",
      "n∑\n",
      "i=1\n",
      "m∑\n",
      "j=1\n",
      "AijBij = Tr(AB⊤)\n",
      "9.2 Motivation: matrix completion and recommen-\n",
      "dation systems\n",
      "Media recommendations have changed signiﬁcantly with the advent of the\n",
      "Internet and rise of online media stores. The large amounts of data collected\n",
      "allow for eﬃcient clustering and accurate prediction of users’ preferences\n",
      "for a variety of media. A well-known example is the so called “Netﬂix\n",
      "challenge”—a competition of automated tools for recommendation from a\n",
      "large dataset of users’ motion picture preferences.\n",
      "One of the most successful approaches for automated recommendation\n",
      "systems, as proven in the Netﬂix competition, is matrix completion. Perhaps\n",
      "the simplest version of the problem can be described as follows.\n",
      "The entire dataset of user-media preference pairs is thought of as a\n",
      "partially-observed matrix. Thus, every person is represented by a row in9.2. MOTIVATION 87\n",
      "the matrix, and every column represents a media item (movie). For sim-\n",
      "plicity, let us think of the observations as binary—a person either likes or\n",
      "dislikes a particular movie. Thus, we have a matrix M ∈{0,1,∗}n×m where\n",
      "n is the number of persons considered, m is the number of movies at our\n",
      "library, and 0/1 and ∗signify “dislike”, “like” and “unknown” respectively:\n",
      "Mij =\n",
      "\n",
      "\n",
      "\n",
      "0, person i dislikes movie j\n",
      "1, person i likes movie j\n",
      "∗, preference unknown\n",
      ".\n",
      "The natural goal is to complete the matrix, i.e. correctly assign 0 or 1 to\n",
      "the unknown entries. As deﬁned so far, the problem is ill-posed, since any\n",
      "completion would be equally good (or bad), and no restrictions have been\n",
      "placed on the completions.\n",
      "The common restriction on completions is that the “true” matrix has\n",
      "low rank. Recall that a matrix X ∈Rn×m has rank k < ρ= min{n,m}if\n",
      "and only if it can be written as\n",
      "X = UV , U∈Rn×k,V ∈Rk×m.\n",
      "The intuitive interpretation of this property is that each entry in M\n",
      "can be explained by only k numbers. In matrix completion this means,\n",
      "intuitively, that there are only kfactors that determine a persons preference\n",
      "over movies, such as genre, director, actors and so on.\n",
      "Now the simplistic matrix completion problem can be well-formulated\n",
      "as in the following mathematical program. Denote by ∥·∥OB the Euclidean\n",
      "norm only on the observed (non starred) entries of M, i.e.,\n",
      "∥X∥2\n",
      "OB =\n",
      "∑\n",
      "Mij̸=∗\n",
      "X2\n",
      "ij.\n",
      "The mathematical program for matrix completion is given by\n",
      "min\n",
      "X∈Rn×m\n",
      "1\n",
      "2∥X−M∥2\n",
      "OB\n",
      "s.t. rank( X) ≤k.\n",
      "Since the constraint over the rank of a matrix is non-convex, it is stan-\n",
      "dard to consider a relaxation that replaces the rank constraint by the nuclear\n",
      "norm. It is known that the nuclear norm is a lower bound on the matrix88 CHAPTER 9. THE CONDITIONAL GRADIENT METHOD\n",
      "rank if the singular values are bounded by one (see exercises). Thus, we\n",
      "arrive at the following convex program for matrix completion:\n",
      "min\n",
      "X∈Rn×m\n",
      "1\n",
      "2∥X−M∥2\n",
      "OB (9.1)\n",
      "s.t. ∥X∥∗≤k.\n",
      "We consider algorithms to solve this convex optimization problem next.\n",
      "9.3 The Frank-Wolfe method\n",
      "In this section we consider minimization of a convex function over a convex\n",
      "domain.\n",
      "The conditional gradient (CG) method, or Frank-Wolfe algorithm, is a\n",
      "simple algorithm for minimizing a smooth convex function f over a convex\n",
      "set K⊆ Rn. The appeal of the method is that it is a ﬁrst order interior point\n",
      "method - the iterates always lie inside the convex set, and thus no projections\n",
      "are needed, and the update step on each iteration simply requires minimizing\n",
      "a linear objective over the set. The basic method is given in Algorithm 13.\n",
      "Algorithm 13 Conditional gradient\n",
      "1: Input: step sizes {ηt ∈(0,1], t∈[T]}, initial point x1 ∈K.\n",
      "2: for t= 1 to T do\n",
      "3: vt ←arg minx∈K\n",
      "{\n",
      "x⊤∇f(xt)\n",
      "}\n",
      ".\n",
      "4: xt+1 ←xt + ηt(vt −xt).\n",
      "5: end for\n",
      "Note that in the CG method, the update to the iterate xt may be not be\n",
      "in the direction of the gradient, as vt is the result of a linear optimization\n",
      "procedure in the direction of the negative gradient. This is depicted in\n",
      "Figure 9.1.\n",
      "The following theorem gives an essentially tight performance guarantee\n",
      "of this algorithm over smooth functions. Recall our notation from Chapter\n",
      "2: x⋆ denotes the global minimizer of f over K, D denotes the diameter of\n",
      "the set K, and ht = f(xt)−f(x⋆) denotes the suboptimality of the objective\n",
      "value in iteration t.\n",
      "Theorem 9.1. The CG algorithm applied to β-smooth functions with step\n",
      "sizes ηt = min{2H\n",
      "t ,1}, for H ≥max{1,h1}, attains the following conver-\n",
      "gence guarantee:\n",
      "ht ≤2βHD2\n",
      "t9.3. THE FRANK-WOLFE METHOD 89\n",
      "Figure 9.1: Direction of progression of the conditional gradient algorithm.\n",
      "Proof. As done before in this manuscript, we denote ∇t = ∇f(xt), and also\n",
      "denote H ≥max{h1,1}, such that ηt = min {2H\n",
      "t ,1}. For any set of step\n",
      "sizes, we have\n",
      "f(xt+1) −f(x⋆) = f(xt + ηt(vt −xt)) −f(x⋆)\n",
      "≤f(xt) −f(x⋆) + ηt(vt −xt)⊤∇t + η2\n",
      "t\n",
      "β\n",
      "2 ∥vt −xt∥2 β-smoothness\n",
      "≤f(xt) −f(x⋆) + ηt(x⋆ −xt)⊤∇t + η2\n",
      "t\n",
      "β\n",
      "2 ∥vt −xt∥2 vt optimality\n",
      "≤f(xt) −f(x⋆) + ηt(f(x⋆) −f(xt)) + η2\n",
      "t\n",
      "β\n",
      "2 ∥vt −xt∥2 convexity of f\n",
      "≤(1 −ηt)(f(xt) −f(x⋆)) + η2\n",
      "tβ\n",
      "2 D2. (9.2)90 CHAPTER 9. THE CONDITIONAL GRADIENT METHOD\n",
      "We reached the recursion ht+1 ≤(1 −ηt)ht + η2\n",
      "t\n",
      "βD2\n",
      "2 , and by induction,\n",
      "ht+1 ≤(1 −ηt)ht + η2\n",
      "t\n",
      "βD2\n",
      "2\n",
      "≤(1 −ηt)2βHD2\n",
      "t + η2\n",
      "t\n",
      "βD2\n",
      "2 induction hypothesis\n",
      "≤(1 −2H\n",
      "t )2βHD2\n",
      "t + 4H2\n",
      "t2\n",
      "βD2\n",
      "2 value of ηt\n",
      "= 2βHD2\n",
      "t −2H2βD2\n",
      "t2\n",
      "≤2βHD2\n",
      "t (1 −1\n",
      "t) since H ≥1\n",
      "≤2βHD2\n",
      "t+ 1 . t−1\n",
      "t ≤ t\n",
      "t+1\n",
      "9.4 Projections vs. linear optimization\n",
      "The conditional gradient (Frank-Wolfe) algorithm described before does not\n",
      "resort to projections, but rather computes a linear optimization problem of\n",
      "the form\n",
      "arg min\n",
      "x∈K\n",
      "{\n",
      "x⊤u\n",
      "}\n",
      ". (9.3)\n",
      "When is the CG method computationally preferable? The overall compu-\n",
      "tational complexity of an iterative optimization algorithm is the product\n",
      "of the number of iterations and the computational cost per iteration. The\n",
      "CG method does not converge as well as the most eﬃcient gradient descent\n",
      "algorithms, meaning it requires more iterations to produce a solution of a\n",
      "comparable level of accuracy. However, for many interesting scenarios the\n",
      "computational cost of a linear optimization step (9.3) is signiﬁcantly lower\n",
      "than that of a projection step.\n",
      "Let us point out several examples of problems for which we have very eﬃ-\n",
      "cient linear optimization algorithms, whereas our state-of-the-art algorithms\n",
      "for computing projections are signiﬁcantly slower.\n",
      "Recommendation systems and matrix prediction. In the example\n",
      "pointed out in the preceding section of matrix completion, known methods9.4. PROJECTIONS VS. LINEAR OPTIMIZATION 91\n",
      "for projection onto the spectahedron, or more generally the bounded nuclear-\n",
      "norm ball, require singular value decompositions, which take superlinear\n",
      "time via our best known methods. In contrast, the CG method requires\n",
      "maximal eigenvector computations which can be carried out in linear time\n",
      "via the power method (or the more sophisticated Lanczos algorithm).\n",
      "Network routing and convex graph problems. Various routing and\n",
      "graph problems can be modeled as convex optimization problems over a\n",
      "convex set called the ﬂow polytope.\n",
      "Consider a directed acyclic graph with m edges, a source node marked\n",
      "s and a target node marked t. Every path from s to t in the graph can be\n",
      "represented by its identifying vector, that is a vector in {0,1}m in which the\n",
      "entries that are set to 1 correspond to edges of the path. The ﬂow polytope\n",
      "of the graph is the convex hull of all such identifying vectors of the simple\n",
      "paths from s to t. This polytope is also exactly the set of all unit s–t ﬂows\n",
      "in the graph if we assume that each edge has a unit ﬂow capacity (a ﬂow\n",
      "is represented here as a vector in Rm in which each entry is the amount of\n",
      "ﬂow through the corresponding edge).\n",
      "Since the ﬂow polytope is just the convex hull of s–tpaths in the graph,\n",
      "minimizing a linear objective over it amounts to ﬁnding a minimum weight\n",
      "path given weights for the edges. For the shortest path problem we have\n",
      "very eﬃcient combinatorial optimization algorithms, namely Dijkstra’s al-\n",
      "gorithm.\n",
      "Thus, applying the CG algorithm to solveany convex optimization prob-\n",
      "lem over the ﬂow polytope will only require iterative shortest path compu-\n",
      "tations.\n",
      "Ranking and permutations. A common way to represent a permutation\n",
      "or ordering is by a permutation matrix. Such are square matrices over\n",
      "{0,1}n×n that contain exactly one 1 entry in each row and column.\n",
      "Doubly-stochastic matrices are square, real-valued matrices with non-\n",
      "negative entries, in which the sum of entries of each row and each column\n",
      "amounts to 1. The polytope that deﬁnes all doubly-stochastic matrices\n",
      "is called the Birkhoﬀ-von Neumann polytope. The Birkhoﬀ-von Neumann\n",
      "theorem states that this polytope is the convex hull of exactly all n×n\n",
      "permutation matrices.\n",
      "Since a permutation matrix corresponds to a perfect matching in a fully\n",
      "connected bipartite graph, linear minimization over this polytope corre-\n",
      "sponds to ﬁnding a minimum weight perfect matching in a bipartite graph.92 CHAPTER 9. THE CONDITIONAL GRADIENT METHOD\n",
      "Consider a convex optimization problem over the Birkhoﬀ-von Neumann\n",
      "polytope. The CG algorithm will iteratively solve a linear optimization\n",
      "problem over the BVN polytope, thus iteratively solving a minimum weight\n",
      "perfect matching in a bipartite graph problem, which is a well-studied com-\n",
      "binatorial optimization problem for which we know of eﬃcient algorithms.\n",
      "In contrast, other gradient based methods will require projections, which\n",
      "are quadratic optimization problems over the BVN polytope.\n",
      "Matroid polytopes. A matroid is pair (E,I) where Eis a set of elements\n",
      "and I is a set of subsets of E called the independent sets which satisfy vari-\n",
      "ous interesting proprieties that resemble the concept of linear independence\n",
      "in vector spaces. Matroids have been studied extensively in combinatorial\n",
      "optimization and a key example of a matroid is the graphical matroid in\n",
      "which the set E is the set of edges of a given graph and the set I is the set of\n",
      "all subsets of Ewhich are cycle-free. In this case, I contains all the spanning\n",
      "trees of the graph. A subset S ∈I could be represented by its identifying\n",
      "vector which lies in {0,1}|E| which also gives rise to the matroid polytope\n",
      "which is just the convex hull of all identifying vectors of sets in I. It can\n",
      "be shown that some matroid polytopes are deﬁned by exponentially many\n",
      "linear inequalities (exponential in |E|), which makes optimization over them\n",
      "diﬃcult.\n",
      "On the other hand, linear optimization over matroid polytopes is easy\n",
      "using a simple greedy procedure which runs in nearly linear time. Thus, the\n",
      "CG method serves as an eﬃcient algorithm to solve any convex optimization\n",
      "problem over matroids iteratively using only a simple greedy procedure.9.5. EXERCISES 93\n",
      "9.5 Exercises\n",
      "1. Prove that if the singular values are smaller than or equal to one, then\n",
      "the nuclear norm is a lower bound on the rank, i.e., show\n",
      "rank(X) ≥∥X∥∗.\n",
      "2. Prove that the trace is related to the nuclear norm via\n",
      "∥X∥∗= Tr(\n",
      "√\n",
      "XX⊤) = Tr(\n",
      "√\n",
      "X⊤X).\n",
      "3. Show that maximizing a linear function over the spectahedron is equiv-\n",
      "alent to a maximal eigenvector computation. That is, show that the\n",
      "following mathematical program:\n",
      "min X•C\n",
      "X ∈Sd = {X ∈Rd×d , X≽ 0 , Tr(X) ≤1},\n",
      "is equivalent to the following:\n",
      "min\n",
      "x∈Rd\n",
      "x⊤Cx\n",
      "s.t. ∥x∥2 ≤1.\n",
      "4. Download the MovieLens dataset from the web. Implement an online\n",
      "recommendation system based on the matrix completion model: im-\n",
      "plement the OCG and OGD algorithms for matrix completion. Bench-\n",
      "mark your results.94 CHAPTER 9. THE CONDITIONAL GRADIENT METHOD\n",
      "9.6 Bibliographic Remarks\n",
      "The matrix completion model has been extremely popular since its inception\n",
      "in the context of recommendation systems [80, 66, 69, 50, 14, 75].\n",
      "The conditional gradient algorithm was devised in the seminal paper\n",
      "by Frank and Wolfe [21]. Due to the applicability of the FW algorithm to\n",
      "large-scale constrained problems, it has been a method of choice in recent\n",
      "machine learning applications, to name a few: [42, 49, 41, 20, 30, 36, 72, 7,\n",
      "82, 22, 23, 8].\n",
      "The online conditional gradient algorithm is due to [36]. An optimal\n",
      "regret algorithm, attaining the O(\n",
      "√\n",
      "T) bound, for the special case of poly-\n",
      "hedral sets was devised in [23].Chapter 10\n",
      "Second order methods for\n",
      "machine learning\n",
      "At this point in our course, we have exhausted the main techniques in\n",
      "ﬁrst-order (or gradient-based) optimization. We have studied the main\n",
      "workhorse - stochastic gradient descent, the three acceleration techniques,\n",
      "and projection-free gradient methods. Have we exhausted optimization for\n",
      "ML?\n",
      "In this section we discuss using higher derivatives of the objective func-\n",
      "tion to accelerate optimization. The canonical method is Newton’s method,\n",
      "which involves the second derivative or Hessian in high dimensions. The\n",
      "vanilla approach is computationally expensive since it involves matrix inver-\n",
      "sion in high dimensions that machine learning problems usually require.\n",
      "However, recent progress in random estimators gives rise to linear-time\n",
      "second order methods, for which each iteration is as computationally cheap\n",
      "as gradient descent.\n",
      "10.1 Motivating example: linear regression\n",
      "In the problem of linear regression we are given a set of measurements {ai ∈\n",
      "Rd,bi ∈R}, and the goal is to ﬁnd a set of weights that explains them best\n",
      "in the mean squared error sense. As a mathematical program, the goal is to\n",
      "optimize:\n",
      "min\n",
      "x∈Rd\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "2\n",
      "∑\n",
      "i∈[m]\n",
      "(\n",
      "a⊤\n",
      "i x −bi\n",
      ")2\n",
      "\n",
      "\n",
      ",\n",
      "9596 CHAPTER 10. SECOND ORDER METHODS\n",
      "or in matrix form,\n",
      "min\n",
      "x\n",
      "f(x) =\n",
      "{1\n",
      "2∥Ax −b∥2\n",
      "}\n",
      ".\n",
      "Here A ∈Rm×d,b ∈Rm. Notice that the objective function f is smooth,\n",
      "but not necessarily strongly convex. Therefore, all algorithms that we have\n",
      "studied so far without exception, which are all ﬁrst order methods, attain\n",
      "rates which are poly( 1\n",
      "ε).\n",
      "However, the linear regression problem has a closed form solution that\n",
      "can be computed by taking the gradient to be zero, i.e. ( Ax −b)⊤A = 0,\n",
      "which gives\n",
      "x = (A⊤A)−1A⊤b.\n",
      "The Newton direction is given by the inverse Hessian multiplied by the\n",
      "gradient, ∇−2f(x)∇f(x). Observe that a single Newton step, i.e. moving in\n",
      "the Newton direction with step size one, from any direction gets us directly\n",
      "to the optimal solution in one iteration! (see exercises)\n",
      "More generally, Newton’s method yields O(log 1\n",
      "ε) convergence rates for\n",
      "a large class of functions without dependence on the condition number of\n",
      "the function! We study this property next.\n",
      "10.2 Self-Concordant Functions\n",
      "In this section we deﬁne and collect some of the properties of a special class\n",
      "of functions, called self-concordant functions. These functions allow New-\n",
      "ton’s method to run in time which is independent of the condition number.\n",
      "The class of self-concordant functions is expressive and includes quadratic\n",
      "functions, logarithms of inner products, a variety of barriers such as the log\n",
      "determinant, and many more.\n",
      "An excellent reference for this material is the lecture notes on this subject\n",
      "by Nemirovski [55]. We begin by deﬁning self-concordant functions.\n",
      "Deﬁnition 10.1 (Self-Concordant Functions). Let K⊆ Rn be a non-empty\n",
      "open convex set, and and let f : K↦→ R be a C3 convex function. Then, f\n",
      "is said to be self-concordant if\n",
      "|∇3f(x)[h,h,h]|≤ 2(h⊤∇2f(x)h)3/2,\n",
      "where we have\n",
      "∇kf(x)[h1,..., hk] ≜ ∂k\n",
      "∂t1 ...∂t k\n",
      "|t1=···=tkf(x + t1h1 + ··· + tkhk).10.2. NEWTON’S METHOD 97\n",
      "Another key object in the analysis of self concordant functions is the\n",
      "notion of a Dikin Ellipsoid, which is the unit ball around a point in the\n",
      "norm given by the Hessian ∥·∥∇2f at the point. We will refer to this norm\n",
      "as the local norm around a point and denote it as ∥·∥x. Formally,\n",
      "Deﬁnition 10.2 (Dikin ellipsoid). The Dikin ellipsoid of radius r centered\n",
      "at a point x is deﬁned as\n",
      "Er(x) ≜ {y |∥y −x∥∇2f(x) ≤r}\n",
      "One of the key properties of self-concordant functions that we use is that\n",
      "inside the Dikin ellipsoid, the function is well conditioned with respect to\n",
      "the local norm at the center. The next lemma makes this formal. The proof\n",
      "of this lemma can be found in [55].\n",
      "Lemma 10.3 (See [55]). For all h such that ∥h∥x <1 we have that\n",
      "(1 −∥h∥x)2∇2f(x) ⪯∇2f(x + h) ⪯ 1\n",
      "(1 −∥h∥x)2 ∇2f(x)\n",
      "Another key quantity, which is used both as a potential function as well\n",
      "as a dampening for the step size in the analysis of Newton’s method, is the\n",
      "Newton Decrement:\n",
      "λx ≜ ∥∇f(x)∥∗\n",
      "x =\n",
      "√\n",
      "∇f(x)⊤∇−2f(x)∇f(x).\n",
      "The following lemma quantiﬁes how λx behaves as a potential by showing\n",
      "that once it drops below 1, it ensures that the minimum of the function lies\n",
      "in the current Dikin ellipsoid. This is the property which we use crucially\n",
      "in our analysis. The proof can be found in [55].\n",
      "Lemma 10.4 (See [55]). If λx <1 then\n",
      "∥x −x∗∥x ≤ λx\n",
      "1 −λx\n",
      "10.3 Newton’s method for self-concordant func-\n",
      "tions\n",
      "Before introducing the linear time second order methods, we start by intro-\n",
      "ducing a robust Newton’s method and its properties. The pseudo-code is\n",
      "given in Algorithm 14.98 CHAPTER 10. SECOND ORDER METHODS\n",
      "The usual analysis of Newton’s method allows for quadratic convergence,\n",
      "i.e. error ε in O(log log 1\n",
      "ε) iterations for convex objectives. However, we\n",
      "prefer to present a version of Newton’s method which is robust to certain\n",
      "random estimators of the Newton direction. This yields a slower rate of\n",
      "O(log 1\n",
      "ε). The faster running time per iteration, which does not require\n",
      "matrix manipulations, more than makes up for this.\n",
      "Algorithm 14 Robust Newton’s method\n",
      "Input: T,x1\n",
      "for t= 1 to T do\n",
      "Set c= 1\n",
      "8 , η= min{c, c\n",
      "8λxt\n",
      "}. Let 1\n",
      "2 ∇−2f(xt) ⪯˜∇−2\n",
      "t ⪯2∇−2f(xt).\n",
      "xt+1 = xt −η˜∇−2\n",
      "t ∇f(xt)\n",
      "end for\n",
      "return xT+1\n",
      "It is important to notice that every two consecutive points are within\n",
      "the same Dikin ellipsoid of radius 1\n",
      "2 . Denote ∇t = ∇xt, and similarly for\n",
      "the Hessian. Then we have:\n",
      "∥xt −xt+1∥2\n",
      "xt = η2∇⊤\n",
      "t ˜∇−2\n",
      "t ∇2\n",
      "t ˜∇−2\n",
      "t ∇t ≤4η2λ2\n",
      "t ≤1\n",
      "2.\n",
      "The advantage of Newton’s method as applied to self-concordant func-\n",
      "tions is its linear convergence rate, as given in the following theorem.\n",
      "Theorem 10.5. Let f be self-concordant, and f(x1) ≤M, then\n",
      "ht = f(xt) −f(x∗) ≤O(M + log 1\n",
      "ε)\n",
      "The proof of this theorem is composed of two steps, according to the\n",
      "magnitude of the Newton decrement.\n",
      "Phase 1: damped Newton\n",
      "Lemma 10.6. As long as λx ≥1\n",
      "8 , we have that\n",
      "ht ≤−1\n",
      "4c10.3. NEWTON’S METHOD FOR SELF-CONCORDANT FUNCTIONS99\n",
      "Proof. Using similar analysis to the descent lemma we have that\n",
      "f(xt+1) −f(xt)\n",
      "≤∇⊤\n",
      "t (xt+1 −xt) + 1\n",
      "2 (xt −xt+1)⊤∇2(ζ)(xt −xt+1) Taylor\n",
      "≤∇⊤\n",
      "t (xt+1 −xt) + 1\n",
      "4 (xt −xt+1)⊤∇2(xt)(xt −xt+1) xt+1 ∈E1/2(xt)\n",
      "= −η∇⊤\n",
      "t ˜∇−2\n",
      "t ∇t + 1\n",
      "4 η2∇⊤\n",
      "t ˜∇−2\n",
      "t ∇2\n",
      "t ˜∇−2\n",
      "t ∇t\n",
      "= −ηλ2\n",
      "t + 1\n",
      "4 η2λ2\n",
      "t ≤− 1\n",
      "16 c\n",
      "The conclusion from this step is that after O(M) steps, Algorithm 14\n",
      "reaches a point for which λx ≤1\n",
      "8 . According to Lemma 10.4, we also have\n",
      "that ∥x −x∗∥x ≤1\n",
      "4 , that is, the optimum is in the same Dikin ellipsoid as\n",
      "the current point.\n",
      "Phase 2: pure Newton In the second phase our step size is changed to\n",
      "be larger. In this case, we are guaranteed that the Newton decrement is less\n",
      "than one, and thus we know that the global optimum is in the same Dikin\n",
      "ellipsoid as the current point. In this ellipsoid, all Hessians are equivalent\n",
      "up to a factor of two, and thus Mirrored-Descent with the inverse Hessian\n",
      "as preconditioner becomes gradient descent. We make this formal below.\n",
      "Algorithm 15 Preconditioned Gradient Descent\n",
      "Input: P,T\n",
      "for t= 1 to T do\n",
      "xt+1 = xt −ηP−1∇f(xt)\n",
      "end for\n",
      "return xT+1\n",
      "Lemma 10.7. Suppose that 1\n",
      "2 P ⪯∇2f(x) ⪯2P, and ∥x1 −x∗∥P ≤1\n",
      "2 , then\n",
      "Algorithm 15 converges as\n",
      "ht+1 ≤h1e−1\n",
      "8 t.\n",
      "This theorem follows from noticing that the function g(z) = f(P−1/2x)\n",
      "is 1\n",
      "2 -strongly convex and 2-smooth, and using Theorem 3.2. It can be shown\n",
      "that gradient descent on g is equivalent to Newton’s method in f. Details\n",
      "are left as an exercise.\n",
      "An immediate corollary is that Newton’s method converges at a rate of\n",
      "O(log 1\n",
      "ε) in this phase.100 CHAPTER 10. SECOND ORDER METHODS\n",
      "10.4 Linear-time second-order methods\n",
      "Newton’s algorithm is of foundational importance in the study of mathemat-\n",
      "ical programming in general. A major application are interior point methods\n",
      "for convex optimization, which are the most important polynomial-time al-\n",
      "gorithms for general constrained convex optimization.\n",
      "However, the main downside of this method is the need to maintain and\n",
      "manipulate matrices - namely the Hessians. This is completely impractical\n",
      "for machine learning applications in which the dimension is huge.\n",
      "Another signiﬁcant downside is the non-robust nature of the algorithm,\n",
      "which makes applying it in stochastic environments challenging.\n",
      "In this section we show how to apply Newton’s method to machine\n",
      "learning problems. This involves relatively new developments that allow\n",
      "for linear-time per-iteration complexity, similar to SGD, and theoretically\n",
      "superior running times. At the time of writing, however, these methods\n",
      "are practical only for convex optimization, and have not shown superior\n",
      "performance on optimization tasks involving deep neural networks.\n",
      "The ﬁrst step to developing a linear time Newton’s method is an eﬃcient\n",
      "stochastic estimator for the Newton direction, and the Hessian inverse.\n",
      "10.4.1 Estimators for the Hessian Inverse\n",
      "The key idea underlying the construction is the following well known fact\n",
      "about the Taylor series expansion of the matrix inverse.\n",
      "Lemma 10.8. For a matrix A ∈Rd×d such that A ⪰0 and ∥A∥≤ 1, we\n",
      "have that\n",
      "A−1 =\n",
      "∞∑\n",
      "i=0\n",
      "(I−A)i\n",
      "We propose two unbiased estimators based on the above series. To deﬁne\n",
      "the ﬁrst estimator pick a probability distribution over non-negative integers\n",
      "{pi}and sampleˆifrom the above distribution. Let X1,...X ˆi be independent\n",
      "samples of the Hessian ∇2f and deﬁne the estimator as\n",
      "Deﬁnition 10.9 (Estimator 1).\n",
      "˜∇−2f = 1\n",
      "pˆi\n",
      "ˆi∏\n",
      "j=1\n",
      "(I−Xj)10.4. LINEAR-TIME SECOND-ORDER METHODS 101\n",
      "Observe that our estimator of the Hessian inverse is unbiased, i.e.E[ ˆX] =\n",
      "∇−2f at any point. Estimator 1 has the disadvantage that in a single sample\n",
      "it incorporates only one term of the Taylor series.\n",
      "The second estimator below is based on the observation that the above\n",
      "series has the following succinct recursive deﬁnition, and is more eﬃcient.\n",
      "For a matrix A deﬁne\n",
      "A−1\n",
      "j =\n",
      "j∑\n",
      "i=0\n",
      "(I−A)i\n",
      "i.e. the ﬁrst j terms of the above Taylor expansion. It is easy to see that\n",
      "the following recursion holds for A−1\n",
      "j\n",
      "A−1\n",
      "j = I+ (I−A)A−1\n",
      "j−1\n",
      "Using the above recursive formulation, we now describe an unbiased\n",
      "estimator of ∇−2f by deriving an unbiased estimator ˜∇−2fj for ∇−2fj.\n",
      "Deﬁnition 10.10 (Estimator 2). Given jindependent and unbiased samples\n",
      "{X1 ...X j}of the hessian ∇2f. Deﬁne {˜∇−2f0 ... ˜∇−2fj}recursively as\n",
      "follows\n",
      "˜∇−2f0 = I\n",
      "˜∇−2ft = I+ (I−Xj) ˜∇−2ft−1\n",
      "It can be readily seen that E[ ˜∇−2fj] = ∇−2fj and therefore E[ ˜∇−2fj] →\n",
      "∇−2f as j →∞ giving us an unbiased estimator in the limit.\n",
      "10.4.2 Incorporating the estimator\n",
      "Both of the above estimators can be computed using only Hessian-vector\n",
      "products, rather than matrix manipulations. For many machine learning\n",
      "problems, Hessian-vector products can be computed in linear time. Exam-\n",
      "ples include:\n",
      "1. Convex regression and SVM objectives over training data have the\n",
      "form\n",
      "min\n",
      "w\n",
      "f(w) = E\n",
      "i\n",
      "[ℓ(w⊤xi)],\n",
      "where ℓ is a convex function. The Hessian can thus be written as\n",
      "∇2f(w) = E\n",
      "i\n",
      "[ℓ′′(w⊤xi)xix⊤\n",
      "i ]102 CHAPTER 10. SECOND ORDER METHODS\n",
      "Thus, the ﬁrst Newton direction estimator can now be written as\n",
      "˜∇2f(w)∇w = E\n",
      "j∼D\n",
      "[\n",
      "j∏\n",
      "i=1\n",
      "(I−ℓ′′(w⊤xi)xix⊤\n",
      "i )]∇w.\n",
      "Notice that this estimator can be computed usingjvector-vector prod-\n",
      "ucts if the ordinal j was randomly chosen.\n",
      "2. Non-convex optimization over neural networks: a similar derivation as\n",
      "above shows that the estimator can be computed only using Hessian-\n",
      "vector products. The special structure of neural networks allow this\n",
      "computation in a constant number of backpropagation steps, i.e. linear\n",
      "time in the network size, this is called the “Pearlmutter trick”, see [61].\n",
      "We note that non-convex optimization presents special challenges for\n",
      "second order methods, since the Hessian need not be positive semi-\n",
      "deﬁnite. Nevertheless, the techniques presented hereby can still be\n",
      "used to provide theoretical speedups for second order methods over\n",
      "ﬁrst order methods in terms of convergence to local minima. The\n",
      "details are beyond our scope, and can be found in [2].\n",
      "Putting everything together. These estimators we have studied can\n",
      "be used to create unbiased estimators to the Newton direction of the form\n",
      "˜∇−2\n",
      "x ∇x for ˜∇−2\n",
      "x which satisﬁes\n",
      "1\n",
      "2∇−2f(xt) ⪯˜∇−2\n",
      "t ⪯2∇−2f(xt).\n",
      "These can be incorporated into Algorithm 14, which we proved is capable\n",
      "of obtaining fast convergence with approximate Newton directions of this\n",
      "form.10.5. EXERCISES 103\n",
      "10.5 Exercises\n",
      "1. Prove that a single Newton step for linear regression yields the optimal\n",
      "solution.\n",
      "2. Let f : Rd ↦→R, and consider the aﬃne transformation y = Ax, for\n",
      "A∈Rd×d being a symmetric matrix. Prove that\n",
      "yt+1 ←yt −η∇f(yt)\n",
      "is equivalent to\n",
      "xt+1 ←xt −ηA−2∇f(xt).\n",
      "3. Prove that the function g(z) deﬁned in phase 2 of the robust Newton\n",
      "algorithm is 1\n",
      "2 -strongly convex and 2-smooth. Conclude with a proof\n",
      "of Theorem 10.7.104 CHAPTER 10. SECOND ORDER METHODS\n",
      "10.6 Bibliographic Remarks\n",
      "The modern application of Newton’s method to convex optimization was\n",
      "put forth in the seminal work of Nesterov and Nemirovski [58] on interior\n",
      "point methods. A wonderful exposition is Nemirovski’s lecture notes [55].\n",
      "The fact that Hessian-vector products can be computed in linear time\n",
      "for feed forward neural networks was described in [61]. Linear time second\n",
      "order methods for machine learning and the Hessian-vector product model\n",
      "in machine learning was introduced in [4]. This was extended to non-convex\n",
      "optimization for deep learning in [2].Chapter 11\n",
      "Hyperparameter\n",
      "Optimization\n",
      "Thus far in this class, we have been talking about continuous mathematical\n",
      "optimization, where the search space of our optimization problem is continu-\n",
      "ous and mostly convex. For example, we have learned about how to optimize\n",
      "the weights of a deep neural network, which take continuous real values, via\n",
      "various optimization algorithms (SGD, AdaGrad, Newton’s method, etc.).\n",
      "However, in the process of training a neural network, there are some meta\n",
      "parameters, which we call hyperparameters, that have a profound eﬀect on\n",
      "the ﬁnal outcome. These are global, mostly discrete, parameters that are\n",
      "treated diﬀerently by algorithm designers as well as by engineers. Examples\n",
      "include the architecture of the neural network (number of layers, width of\n",
      "each layer, type of activation function, ...), the optimization scheme for up-\n",
      "dating weights (SGD/AdaGrad, initial learning rate, decay rate of learning\n",
      "rate, momentum parameter, ...), and many more. Roughly speaking, these\n",
      "hyperparameters are chosen before the training starts.\n",
      "The purpose of this chapter is to formalize this problem as an optimiza-\n",
      "tion problem in machine learning, which requires a diﬀerent methodology\n",
      "than we have treated in the rest of this course. We remark that hyperpa-\n",
      "rameter optimization is still an active area of research and its theoretical\n",
      "properties are not well understood as of this time.\n",
      "11.1 Formalizing the problem\n",
      "What makes hyperparameters diﬀerent from “regular” parameters?\n",
      "105106 CHAPTER 11. HYPERPARAMETER OPTIMIZATION\n",
      "1. The search space is often discrete (for example, number of layers). As\n",
      "such, there is no natural notion of gradient or diﬀerentials and it is\n",
      "not clear how to apply the iterative methods we have studied thus far.\n",
      "2. Even evaluating the objective function is extremely expensive (think\n",
      "of evaluating the test error of the trained neural network). Thus it is\n",
      "crucial to minimize the number of function evaluations, whereas other\n",
      "computations are signiﬁcantly less expensive.\n",
      "3. Evaluating the function can be done in parallel. As an example, train-\n",
      "ing feedforward deep neural networks over diﬀerent architectures can\n",
      "be done in parallel.\n",
      "More formally, we consider the following optimization problem\n",
      "min\n",
      "xi∈GF(qi)\n",
      "f(x),\n",
      "where x is the representation of discrete hyperparameters, each taking value\n",
      "from qi ≥2 possible discrete values and thus in GF(q), the Galois ﬁeld of\n",
      "order q. The example to keep in mind is that the objective f(x) is the test\n",
      "error of the neural network trained with hyperparameters x. Note that x\n",
      "has a search space of size ∏\n",
      "iqi ≥2n, exponentially large in the number of\n",
      "diﬀerent hyperparameters.\n",
      "11.2 Hyperparameter optimization algorithms\n",
      "The properties of the problem mentioned before prohibits the use of the\n",
      "algorithms we have studied thus far, which are all suitable for continuous\n",
      "optimization. A naive method is to perform a grid search over all hyperpa-\n",
      "rameters, but this quickly becomes infeasible. An emerging ﬁeld of research\n",
      "in recent years, called AutoML, aims to choose hyperparameters automati-\n",
      "cally. The following techniques are in common use:\n",
      "•Grid search , try all possible assignments of hyperparameters and\n",
      "return the best. This becomes infeasible very quickly with n - the\n",
      "number of hyperparameters.\n",
      "•Random search, where one randomly picks some choices of hyper-\n",
      "parameters, evaluates their function objective, and chooses the one\n",
      "choice of hyperparameters giving best performance. An advantage of\n",
      "this method is that it is easy to implement in parallel.11.3. A SPECTRAL METHOD 107\n",
      "•Successive Halving and Hyperband , random search combined\n",
      "with early stopping using multi-armed bandit techniques. These gain\n",
      "a small constant factor improvement over random search.\n",
      "•Bayesian optimization, a statistical approach which has a prior over\n",
      "the objective and tries to iteratively pick an evaluation point which\n",
      "reduces the variance in objective value. Finally it picks the point\n",
      "that attains the lowest objective objective with highest conﬁdence.\n",
      "This approach is sequential in nature and thus diﬃcult to parallelize.\n",
      "Another important question is how to choose a good prior.\n",
      "The hyperparameter optimization problem is essentially a combinato-\n",
      "rial optimization problem with exponentially large search space. Without\n",
      "further assumptions, this optimization problem is information-theoretically\n",
      "hard. Such assumptions are explored in the next section with an accompa-\n",
      "nying algorithm.\n",
      "Finally, we note that a simple but hard-to-beat benchmark is random\n",
      "search with double budget. That is, compare the performance of a method\n",
      "to that of random search, but allow random search double the query budget\n",
      "of your own method.\n",
      "11.3 A Spectral Method\n",
      "For simplicity, in this section we consider the case in which hyperparam-\n",
      "eters are binary. This retains the diﬃculty of the setting, but makes the\n",
      "mathematical derivation simpler. The optimization problem now becomes\n",
      "min\n",
      "x∈{−1,1}n\n",
      "f(x). (11.1)\n",
      "The method we describe in this section is inspired by the following key\n",
      "observation: although the whole search space of hyperparameters is exponen-\n",
      "tially large, it is often the case in practice that only a few hyperparameters\n",
      "together play a signiﬁcant role in the performance of a deep neural network .\n",
      "To make this intuition more precise, we need some deﬁnitions and facts\n",
      "from Fourier analysis of Boolean functions.\n",
      "Fact 11.1. Any function f : {−1,1}n →[−1,1] can be uniquely represented\n",
      "in the Fourier basis\n",
      "f(x) =\n",
      "∑\n",
      "S⊆[n]\n",
      "αsˆχS(x),108 CHAPTER 11. HYPERPARAMETER OPTIMIZATION\n",
      "where each Fourier basis function\n",
      "ˆχS(x) =\n",
      "∏\n",
      "i∈S\n",
      "xi.\n",
      "is a monomial, and thus f(x) has a polynomial representation.\n",
      "Now we are ready to formalize our key observation in the following as-\n",
      "sumption:\n",
      "Assumption 11.2. The objective function f in the hyperparameter opti-\n",
      "mization problem (11.1) is low degree and sparse in the Fourier basis, i.e.\n",
      "f(x) ≈\n",
      "∑\n",
      "|S|≤d\n",
      "αSˆχS(x), ∥ααα∥1 ≤k, (11.2)\n",
      "where d is the upper bound of polynomial degree, and k is the sparsity of\n",
      "Fourier coeﬃcientααα(indexed by S) in ℓ1 sense (which is a convex relaxation\n",
      "of ∥ααα∥0, the true sparsity).\n",
      "Remark 11.3. Clearly this assumption does not always hold. For example,\n",
      "many deep reinforcement learning algorithms nowadays rely heavily on the\n",
      "choice of the random seed, which can also be seen as a hyperparameter. If\n",
      "x ∈{−1,1}32 is the bit representation of a int32 random seed, then there is\n",
      "no reason to assume that a few of these bits should play a more signiﬁcant\n",
      "role than the others.\n",
      "Under this assumption, all we need to do now is to ﬁnd out the few im-\n",
      "portant sets of variables S’s, as well as their coeﬃcients αS’s, in the approx-\n",
      "imation (11.2). Fortunately, there is already a whole area of research, called\n",
      "compressed sensing, that aims to recover a high-dimensional but sparse vec-\n",
      "tor, using only a few linear measurements. Next, we will brieﬂy introduce\n",
      "the problem of compressed sensing, and one useful result from the litera-\n",
      "ture. After that, we will introduce the Harmonica algorithm, which applies\n",
      "compressed sensing techniques to solve the hyperparameter optimization\n",
      "problem (11.1).\n",
      "11.3.1 Background: Compressed Sensing\n",
      "The problem of compressed sensing is as follows. Suppose there is a hidden\n",
      "signal x ∈Rn that we cannot observe. In order to recover x, we design a\n",
      "measurement matrix A ∈Rm×n, and obtain noisy linear measurements y =\n",
      "Ax + ηηη∈Rm, where ηηη is some random noise. The diﬃculty arises when we11.3. A SPECTRAL METHOD 109\n",
      "have a limited budget for measurements, i.e.m≪n. Note that even without\n",
      "noise, recovering x is non-trivial since y = Ax is an underdetermined linear\n",
      "system, therefore if there is one solution x that solves this linear system,\n",
      "there will be inﬁnitely many solutions. The key to this problem is to assume\n",
      "that x is k-sparse, that is, ∥x∥0 ≤k. This assumption has been justiﬁed\n",
      "in various real-world applications; for example, natural images tend to be\n",
      "sparse in the Fourier/wavelet domain, a property which forms the bases of\n",
      "many image compression algorithms.\n",
      "Under the assumption of sparsity, the natural way to recover x is to\n",
      "solve a least squares problem, subject to some sparsity constraint ∥x∥0 ≤k.\n",
      "However, ℓ0 norm is diﬃcult to handle, and it is often replaced by ℓ1 norm,\n",
      "its convex relaxation. One useful result from the literature of compressed\n",
      "sensing is the following.\n",
      "Proposition 11.4 (Informal statement of Theorem 4.4 in [63]) . Assume\n",
      "the ground-truth signal x ∈Rn is k-sparse. Then, with high probability,\n",
      "using a randomly designed A ∈Rm×n that is “near-orthogonal” (random\n",
      "Gaussian matrix, subsampled Fourier basis, etc.), with m = O(klog(n)/ε)\n",
      "and ∥ηηη∥2 = O(√m), x can be recovered by a convex program\n",
      "min\n",
      "z∈Rn\n",
      "∥y −Az∥2\n",
      "2 s.t. ∥z∥1 ≤k, (11.3)\n",
      "with accuracy ∥x −z∥2 ≤ε.\n",
      "This result is remarkable; in particular, it says that the number of mea-\n",
      "surements needed to recover a sparse signal is independent of the dimension\n",
      "n (up to a logarithm term), but only depends on the sparsity k and the\n",
      "desired accuracy ε. 1\n",
      "Remark 11.5. The convex program (11.3) is equivalent to the following\n",
      "LASSO problem\n",
      "min\n",
      "z∈Rn\n",
      "∥y −Az∥2\n",
      "2 + λ∥z∥1,\n",
      "with a proper choice of regularization parameter λ. The LASSO problem\n",
      "is an unconstrained convex program, and has eﬃcient solvers, as per the\n",
      "algorithms we have studied in this course.\n",
      "1It also depends on the desired high-probability bound, which is omitted in this informal\n",
      "statement.110 CHAPTER 11. HYPERPARAMETER OPTIMIZATION\n",
      "11.3.2 The Spectral Algorithm\n",
      "The main idea is that, under Assumption 11.2, we can view the problem\n",
      "of hyperparameter optimization as recovering the sparse signal ααα from lin-\n",
      "ear measurements. More speciﬁcally, we need to query T random samples,\n",
      "f(x1),...,f (xT), and then solve the LASSO problem\n",
      "min\n",
      "ααα\n",
      "T∑\n",
      "t=1\n",
      "(\n",
      "∑\n",
      "|S|≤d\n",
      "αSˆχS(xt) −f(xt))2 + λ∥ααα∥1, (11.4)\n",
      "where the regularization term λ∥ααα∥1 controls the sparsity of ααα. Also note\n",
      "that the constraint |S|≤ dnot only implies that the solution is a low-degree\n",
      "polynomial, but also helps to reduce the “eﬀective” dimension of αααfrom 2n\n",
      "to O(nd), which makes it feasible to solve this LASSO problem.\n",
      "Denote by S1,...,S s the indices of theslargest coeﬃcients of the LASSO\n",
      "solution, and deﬁne\n",
      "g(x) =\n",
      "∑\n",
      "i∈[s]\n",
      "αSiˆχSi(x),\n",
      "which involves only a few dimensions ofx since the LASSO solution is sparse\n",
      "and low-degree. The next step is to set the variables outside ∪i∈[s]Si to\n",
      "arbitrary values, and compute a minimizer x∗ ∈ arg ming(x). In other\n",
      "words, we have reduced the original problem of optimizing f(x) over n\n",
      "variables, to the problem of optimizing g(x) (an approximation of f(x))\n",
      "over only a few variables (which is now feasible to solve). One remarkable\n",
      "feature of this algorithm is that the returned solution x∗may not belong to\n",
      "the samples {x1,..., xT}, which is not the case for other existing methods\n",
      "(such as random search).\n",
      "Using theoretical results from compressed sensing (e.g. Proposition\n",
      "11.4), we can derive the following guarantee for the sparse recovery of ααα\n",
      "via LASSO.\n",
      "Theorem 11.6 (Informal statement of Lemma 7 in [38]) . Assume f is k-\n",
      "sparse in the Fourier expansion. Then, with T = O(k2 log(n)/ε) samples,\n",
      "the solution of the LASSO problem (11.4) achieves ε accuracy.\n",
      "Finally, the above derivation can be considered as only one stage in a\n",
      "multi-stage process, each iteratively setting the value of a few more variables\n",
      "that are the most signiﬁcant.11.4. BIBLIOGRAPHIC REMARKS 111\n",
      "11.4 Bibliographic Remarks\n",
      "For a nice exposition on hyperparameter optimization see [64, 65], in which\n",
      "the the benchmark of comparing to Random Search with double queries was\n",
      "proposed.\n",
      "Perhaps the simplest approach to HPO is random sampling of diﬀerent\n",
      "choices of parameters and picking the best amongst the chosen evaluations\n",
      "[9]. Successive Halving (SH) algorithm was introduced [43]. Hyperband\n",
      "further improves SH by automatically tuning the hyperparameters in SH\n",
      "[51].\n",
      "The Bayesian optimization (BO) methodology is currently the most stud-\n",
      "ied in HPO. For recent studies and algorithms of this ﬂavor see [10, 78, 81,\n",
      "79, 24, 84, 40].\n",
      "The spectral approach for hyperparameter optimization was introduced\n",
      "in [38]. For an in-depth treatment of compressed sensing see the survey of\n",
      "[63], and for Fourier analysis of Boolean functions see [59].112 CHAPTER 11. HYPERPARAMETER OPTIMIZATIONBibliography\n",
      "[1] Jacob Abernethy, Elad Hazan, and Alexander Rakhlin. Competing\n",
      "in the dark: An eﬃcient algorithm for bandit linear optimization. In\n",
      "Proceedings of the 21st Annual Conference on Learning Theory , pages\n",
      "263–274, 2008.\n",
      "[2] Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and\n",
      "Tengyu Ma. Finding approximate local minima faster than gradient\n",
      "descent. In Proceedings of the 49th Annual ACM SIGACT Symposium\n",
      "on Theory of Computing , pages 1195–1199. ACM, 2017.\n",
      "[3] Naman Agarwal, Brian Bullins, Xinyi Chen, Elad Hazan, Karan Singh,\n",
      "Cyril Zhang, and Yi Zhang. The case for full-matrix adaptive regular-\n",
      "ization. arXiv preprint arXiv:1806.02958 , 2018.\n",
      "[4] Naman Agarwal, Brian Bullins, and Elad Hazan. Second-order stochas-\n",
      "tic optimization for machine learning in linear time. The Journal of\n",
      "Machine Learning Research, 18(1):4148–4187, 2017.\n",
      "[5] Zeyuan Allen-Zhu and Lorenzo Orecchia. Linear coupling: An ul-\n",
      "timate uniﬁcation of gradient and mirror descent. arXiv preprint\n",
      "arXiv:1407.1537, 2014.\n",
      "[6] Rohan Anil, Vineet Gupta, Tomer Koren, and Yoram Singer. Memory-\n",
      "eﬃcient adaptive optimization for large-scale learning. arXiv preprint\n",
      "arXiv:1901.11150, 2019.\n",
      "[7] Francis Bach, Simon Lacoste-Julien, and Guillaume Obozinski. On\n",
      "the equivalence between herding and conditional gradient algorithms.\n",
      "In John Langford and Joelle Pineau, editors, Proceedings of the 29th\n",
      "International Conference on Machine Learning (ICML-12) , ICML ’12,\n",
      "pages 1359–1366, New York, NY, USA, July 2012. Omnipress.\n",
      "113114 BIBLIOGRAPHY\n",
      "[8] Aur´ elien Bellet, Yingyu Liang, Alireza Bagheri Garakani, Maria-\n",
      "Florina Balcan, and Fei Sha. Distributed frank-wolfe algorithm: A\n",
      "uniﬁed framework for communication-eﬃcient sparse learning. CoRR,\n",
      "abs/1404.2644, 2014.\n",
      "[9] James Bergstra and Yoshua Bengio. Random search for hyper-\n",
      "parameter optimization. J. Mach. Learn. Res. , 13:281–305, February\n",
      "2012.\n",
      "[10] James S. Bergstra, R´ emi Bardenet, Yoshua Bengio, and Bal´ azs K´ egl.\n",
      "Algorithms for hyper-parameter optimization. In J. Shawe-Taylor, R. S.\n",
      "Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger, editors, Ad-\n",
      "vances in Neural Information Processing Systems 24 , pages 2546–2554.\n",
      "Curran Associates, Inc., 2011.\n",
      "[11] J.M. Borwein and A.S. Lewis. Convex Analysis and Nonlinear Opti-\n",
      "mization: Theory and Examples . CMS Books in Mathematics. Springer,\n",
      "2006.\n",
      "[12] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge Uni-\n",
      "versity Press, March 2004.\n",
      "[13] S´ ebastien Bubeck. Convex optimization: Algorithms and complexity.\n",
      "Foundations and Trends in Machine Learning , 8(3–4):231–357, 2015.\n",
      "[14] E. Candes and B. Recht. Exact matrix completion via convex optimiza-\n",
      "tion. Foundations of Computational Mathematics , 9:717–772, 2009.\n",
      "[15] Nicol` o Cesa-Bianchi and G´ abor Lugosi. Prediction, Learning, and\n",
      "Games. Cambridge University Press, 2006.\n",
      "[16] Xinyi Chen, Naman Agarwal, Elad Hazan, Cyril Zhang, and Yi Zhang.\n",
      "Extreme tensoring for low-memory preconditioning. arXiv preprint\n",
      "arXiv:1902.04620, 2019.\n",
      "[17] Qi Deng, Yi Cheng, and Guanghui Lan. Optimal adaptive and accel-\n",
      "erated stochastic gradient descent. arXiv preprint arXiv:1810.00553 ,\n",
      "2018.\n",
      "[18] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient\n",
      "methods for online learning and stochastic optimization. The Journal\n",
      "of Machine Learning Research, 12:2121–2159, 2011.BIBLIOGRAPHY 115\n",
      "[19] John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient\n",
      "methods for online learning and stochastic optimization. In COLT 2010\n",
      "- The 23rd Conference on Learning Theory, Haifa, Israel, June 27-29,\n",
      "2010, pages 257–269, 2010.\n",
      "[20] Miroslav Dud´ ık, Za¨ ıd Harchaoui, and J´ erˆ ome Malick. Lifted coordinate\n",
      "descent for learning with trace-norm regularization.Journal of Machine\n",
      "Learning Research - Proceedings Track, 22:327–336, 2012.\n",
      "[21] M. Frank and P. Wolfe. An algorithm for quadratic programming.Naval\n",
      "Research Logistics Quarterly, 3:149–154, 1956.\n",
      "[22] Dan Garber and Elad Hazan. Approximating semideﬁnite programs in\n",
      "sublinear time. In NIPS, pages 1080–1088, 2011.\n",
      "[23] Dan Garber and Elad Hazan. Playing non-linear games with linear\n",
      "oracles. In FOCS, pages 420–428, 2013.\n",
      "[24] Jacob R. Gardner, Matt J. Kusner, Zhixiang Eddie Xu, Kilian Q. Wein-\n",
      "berger, and John P. Cunningham. Bayesian optimization with inequal-\n",
      "ity constraints. In Proceedings of the 31th International Conference\n",
      "on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014 ,\n",
      "pages 937–945, 2014.\n",
      "[25] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning.\n",
      "MIT Press, 2016. http://www.deeplearningbook.org.\n",
      "[26] A .J. Grove, N. Littlestone, and D. Schuurmans. General convergence\n",
      "results for linear discriminant updates. Machine Learning, 43(3):173–\n",
      "210, 2001.\n",
      "[27] Vineet Gupta, Tomer Koren, and Yoram Singer. A uniﬁed approach\n",
      "to adaptive regularization in online and stochastic optimization. arXiv\n",
      "preprint arXiv:1706.06569, 2017.\n",
      "[28] Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Precondi-\n",
      "tioned stochastic tensor optimization. arXiv preprint arXiv:1802.09568,\n",
      "2018.\n",
      "[29] James Hannan. Approximation to bayes risk in repeated play. In M.\n",
      "Dresher, A. W. Tucker, and P. Wolfe, editors, Contributions to the\n",
      "Theory of Games, volume 3 , pages 97–139, 1957.116 BIBLIOGRAPHY\n",
      "[30] Za¨ ıd Harchaoui, Matthijs Douze, Mattis Paulin, Miroslav Dud´ ık, and\n",
      "J´ erˆ ome Malick. Large-scale image classiﬁcation with trace-norm regu-\n",
      "larization. In CVPR, pages 3386–3393, 2012.\n",
      "[31] Elad Hazan. Introduction to online convex optimization. Foundations\n",
      "and Trends ˆA R⃝in Optimization, 2(3-4):157–325, 2016.\n",
      "[32] Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret al-\n",
      "gorithms for online convex optimization. In Machine Learning, volume\n",
      "69(2–3), pages 169–192, 2007.\n",
      "[33] Elad Hazan and Sham Kakade. Revisiting the polyak step size. arXiv\n",
      "preprint arXiv:1905.00313, 2019.\n",
      "[34] Elad Hazan and Satyen Kale. Extracting certainty from uncertainty:\n",
      "Regret bounded by variation in costs. In The 21st Annual Conference\n",
      "on Learning Theory (COLT), pages 57–68, 2008.\n",
      "[35] Elad Hazan and Satyen Kale. Beyond the regret minimization barrier:\n",
      "an optimal algorithm for stochastic strongly-convex optimization.Jour-\n",
      "nal of Machine Learning Research - Proceedings Track, pages 421–436,\n",
      "2011.\n",
      "[36] Elad Hazan and Satyen Kale. Projection-free online learning. In ICML,\n",
      "2012.\n",
      "[37] Elad Hazan and Satyen Kale. Beyond the regret minimization barrier:\n",
      "optimal algorithms for stochastic strongly-convex optimization. The\n",
      "Journal of Machine Learning Research , 15(1):2489–2512, 2014.\n",
      "[38] Elad Hazan, Adam Klivans, and Yang Yuan. Hyperparameter opti-\n",
      "mization: A spectral approach. ICLR, 2018.\n",
      "[39] Geoﬀrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural net-\n",
      "works for machine learning lecture 6a overview of mini-batch gradient\n",
      "descent. Cited on, 14, 2012.\n",
      "[40] Ilija Ilievski, Taimoor Akhtar, Jiashi Feng, and Christine Annette Shoe-\n",
      "maker. Eﬃcient hyperparameter optimization for deep learning al-\n",
      "gorithms using deterministic RBF surrogates. In Proceedings of the\n",
      "Thirty-First AAAI Conference on Artiﬁcial Intelligence, February 4-9,\n",
      "2017, San Francisco, California, USA. , pages 822–829, 2017.BIBLIOGRAPHY 117\n",
      "[41] Martin Jaggi. Revisiting frank-wolfe: Projection-free sparse convex\n",
      "optimization. In ICML, 2013.\n",
      "[42] Martin Jaggi and Marek Sulovsk´ y. A simple algorithm for nuclear norm\n",
      "regularized problems. In ICML, pages 471–478, 2010.\n",
      "[43] Kevin G. Jamieson and Ameet Talwalkar. Non-stochastic best arm\n",
      "identiﬁcation and hyperparameter optimization. In Proceedings of the\n",
      "19th International Conference on Artiﬁcial Intelligence and Statistics,\n",
      "AISTATS 2016, Cadiz, Spain, May 9-11, 2016 , pages 240–248, 2016.\n",
      "[44] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent\n",
      "using predictive variance reduction. In Advances in neural information\n",
      "processing systems, pages 315–323, 2013.\n",
      "[45] Adam Kalai and Santosh Vempala. Eﬃcient algorithms for online de-\n",
      "cision problems. Journal of Computer and System Sciences , 71(3):291–\n",
      "307, 2005.\n",
      "[46] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic\n",
      "optimization. arXiv preprint arXiv:1412.6980 , 2014.\n",
      "[47] Jyrki Kivinen and Manfred K. Warmuth. Exponentiated gradient ver-\n",
      "sus gradient descent for linear predictors. Inf. Comput. , 132(1):1–63,\n",
      "1997.\n",
      "[48] Jyrki Kivinen and Manfred K. Warmuth. Relative loss bounds for\n",
      "multidimensional regression problems. Machine Learning, 45(3):301–\n",
      "329, 2001.\n",
      "[49] Simon Lacoste-Julien, Martin Jaggi, Mark W. Schmidt, and Patrick\n",
      "Pletscher. Block-coordinate frank-wolfe optimization for structural\n",
      "svms. In Proceedings of the 30th International Conference on Machine\n",
      "Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013 , pages 53–\n",
      "61, 2013.\n",
      "[50] J. Lee, B. Recht, R. Salakhutdinov, N. Srebro, and J. A. Tropp. Prac-\n",
      "tical large-scale optimization for max-norm regularization. In NIPS,\n",
      "pages 1297–1305, 2010.\n",
      "[51] L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar.\n",
      "Hyperband: A Novel Bandit-Based Approach to Hyperparameter Op-\n",
      "timization. ArXiv e-prints, March 2016.118 BIBLIOGRAPHY\n",
      "[52] H. Brendan McMahan and Matthew J. Streeter. Adaptive bound op-\n",
      "timization for online convex optimization. In COLT 2010 - The 23rd\n",
      "Conference on Learning Theory, Haifa, Israel, June 27-29, 2010 , pages\n",
      "244–256, 2010.\n",
      "[53] Arkadi S. Nemirovski and David B. Yudin. Problem Complexity and\n",
      "Method Eﬃciency in Optimization . John Wiley UK/USA, 1983.\n",
      "[54] A.S. Nemirovskii. Interior point polynomial time methods in convex\n",
      "programming, 2004. Lecture Notes.\n",
      "[55] AS Nemirovskii. Interior point polynomial time methods in convex\n",
      "programming. Lecture Notes, 2004.\n",
      "[56] Y. Nesterov. A method of solving a convex programming problem with\n",
      "convergence rate O(1/k2). Soviet Mathematics Doklady, 27(2):372–376,\n",
      "1983.\n",
      "[57] Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic\n",
      "Course. Applied Optimization. Springer, 2004.\n",
      "[58] Y. E. Nesterov and A. S. Nemirovskii. Interior Point Polynomial Al-\n",
      "gorithms in Convex Programming . SIAM, Philadelphia, 1994.\n",
      "[59] Ryan O’Donnell. Analysis of Boolean Functions. Cambridge University\n",
      "Press, New York, NY, USA, 2014.\n",
      "[60] Francesco Orabona and Koby Crammer. New adaptive algorithms for\n",
      "online classiﬁcation. In Proceedings of the 24th Annual Conference on\n",
      "Neural Information Processing Systems 2010. , pages 1840–1848, 2010.\n",
      "[61] Barak A Pearlmutter. Fast exact multiplication by the hessian. Neural\n",
      "computation, 6(1):147–160, 1994.\n",
      "[62] Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making\n",
      "gradient descent optimal for strongly convex stochastic optimization.\n",
      "In ICML, 2012.\n",
      "[63] Holger Rauhut. Compressive sensing and structured random matrices.\n",
      "Theoretical foundations and numerical methods for sparse recovery, 9:1–\n",
      "92, 2010.\n",
      "[64] Benjamin Recht. Embracing the random. http://www.argmin.net/\n",
      "2016/06/23/hyperband/, 2016.BIBLIOGRAPHY 119\n",
      "[65] Benjamin Recht. The news on auto-tuning. http://www.argmin.net/\n",
      "2016/06/20/hypertuning/, 2016.\n",
      "[66] Jasson D. M. Rennie and Nathan Srebro. Fast maximum margin matrix\n",
      "factorization for collaborative prediction. In Proceedings of the 22Nd\n",
      "International Conference on Machine Learning , ICML ’05, pages 713–\n",
      "719, New York, NY, USA, 2005. ACM.\n",
      "[67] Herbert Robbins and Sutton Monro. A stochastic approximation\n",
      "method. The Annals of Mathematical Statistics, 22(3):400–407, 09 1951.\n",
      "[68] R.T. Rockafellar. Convex Analysis . Convex Analysis. Princeton Uni-\n",
      "versity Press, 1997.\n",
      "[69] R. Salakhutdinov and N. Srebro. Collaborative ﬁltering in a non-\n",
      "uniform world: Learning with the weighted trace norm. In NIPS, pages\n",
      "2056–2064, 2010.\n",
      "[70] Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing ﬁnite\n",
      "sums with the stochastic average gradient. Mathematical Programming,\n",
      "162(1-2):83–112, 2017.\n",
      "[71] Shai Shalev-Shwartz. Online Learning: Theory, Algorithms, and Ap-\n",
      "plications. PhD thesis, The Hebrew University of Jerusalem, 2007.\n",
      "[72] Shai Shalev-Shwartz, Alon Gonen, and Ohad Shamir. Large-scale con-\n",
      "vex minimization with a low-rank constraint. In ICML, pages 329–336,\n",
      "2011.\n",
      "[73] Shai Shalev-Shwartz and Yoram Singer. A primal-dual perspective of\n",
      "online learning algorithms. Machine Learning, 69(2-3):115–142, 2007.\n",
      "[74] Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro, and Andrew Cot-\n",
      "ter. Pegasos: primal estimated sub-gradient solver for svm. Math.\n",
      "Program., 127(1):3–30, 2011.\n",
      "[75] O. Shamir and S. Shalev-Shwartz. Collaborative ﬁltering with the\n",
      "trace norm: Learning, bounding, and transducing. JMLR - Proceedings\n",
      "Track, 19:661–678, 2011.\n",
      "[76] Ohad Shamir and Tong Zhang. Stochastic gradient descent for\n",
      "non-smooth optimization: Convergence results and optimal averaging\n",
      "schemes. In ICML, 2013.120 BIBLIOGRAPHY\n",
      "[77] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates\n",
      "with sublinear memory cost. arXiv preprint arXiv:1804.04235 , 2018.\n",
      "[78] Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian\n",
      "optimization of machine learning algorithms. In Advances in Neural\n",
      "Information Processing Systems 25: 26th Annual Conference on Neural\n",
      "Information Processing Systems 2012. Proceedings of a meeting held\n",
      "December 3-6, 2012, Lake Tahoe, Nevada, United States. , pages 2960–\n",
      "2968, 2012.\n",
      "[79] Jasper Snoek, Kevin Swersky, Richard S. Zemel, and Ryan P. Adams.\n",
      "Input warping for bayesian optimization of non-stationary functions. In\n",
      "Proceedings of the 31th International Conference on Machine Learning,\n",
      "ICML 2014, Beijing, China, 21-26 June 2014 , pages 1674–1682, 2014.\n",
      "[80] Nathan Srebro. Learning with Matrix Factorizations. PhD thesis, Mas-\n",
      "sachusetts Institute of Technology, 2004.\n",
      "[81] Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Multi-task\n",
      "bayesian optimization. In Advances in Neural Information Processing\n",
      "Systems 26: 27th Annual Conference on Neural Information Processing\n",
      "Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake\n",
      "Tahoe, Nevada, United States., pages 2004–2012, 2013.\n",
      "[82] Ambuj Tewari, Pradeep D. Ravikumar, and Inderjit S. Dhillon. Greedy\n",
      "algorithms for structurally constrained high dimensional problems. In\n",
      "NIPS, pages 882–890, 2011.\n",
      "[83] A. M. Turing. Computing machinery and intelligence. Mind,\n",
      "59(236):433–460, 1950.\n",
      "[84] Ziyu Wang, Masrour Zoghi, Frank Hutter, David Matheson, and Nando\n",
      "de Freitas. Bayesian optimization in high dimensions via random em-\n",
      "beddings. In IJCAI 2013, Proceedings of the 23rd International Joint\n",
      "Conference on Artiﬁcial Intelligence, Beijing, China, August 3-9, 2013 ,\n",
      "pages 1778–1784, 2013.\n",
      "[85] Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp\n",
      "convergence over nonconvex landscapes, from any initialization. arXiv\n",
      "preprint arXiv:1806.01811, 2018.\n",
      "[86] Lijun Zhang, Mehrdad Mahdavi, and Rong Jin. Linear convergence\n",
      "with condition number independent access of full gradients. InAdvances\n",
      "in Neural Information Processing Systems , pages 980–988, 2013.BIBLIOGRAPHY 121\n",
      "[87] Martin Zinkevich. Online convex programming and generalized in-\n",
      "ﬁnitesimal gradient ascent. In Proceedings of the 20th International\n",
      "Conference on Machine Learning, pages 928–936, 2003.\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "from pypdf import PdfReader\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    full_text = \"\"\n",
    "    for page in reader.pages:\n",
    "        full_text += page.extract_text()\n",
    "    return full_text\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "client = arxiv.Client()\n",
    "\n",
    "search = arxiv.Search(\n",
    "    query=\"machine learning\",\n",
    "    max_results=1,)\n",
    "\n",
    "results = client.results(search)\n",
    "for result in results:\n",
    "\n",
    "    pdf_file = result.download_pdf()  # Replace with your PDF file path\n",
    "extracted_text = extract_text_from_pdf(pdf_file)\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa9d446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
